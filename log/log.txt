Using device: cpu
Step 0, training loss: 10.928370475769043, total_norm: 30.599348068237305, lr: 5.9999999999999995e-05
Step 0, validation loss: 9.709343910217285
Step 1, training loss: 9.6537504196167, total_norm: 10.331649780273438, lr: 0.00011999999999999999
Step 2, training loss: 8.999080657958984, total_norm: 5.400961399078369, lr: 0.00017999999999999998
Step 3, training loss: 8.875970840454102, total_norm: 3.31951642036438, lr: 0.00023999999999999998
Step 4, training loss: 9.248791694641113, total_norm: 4.68358039855957, lr: 0.0003
Step 5, training loss: 8.656591415405273, total_norm: 2.5567822456359863, lr: 0.00035999999999999997
Step 6, training loss: 8.303866386413574, total_norm: 2.6235389709472656, lr: 0.00041999999999999996
Step 7, training loss: 8.327607154846191, total_norm: 2.205348253250122, lr: 0.00047999999999999996
Step 8, training loss: 8.387380599975586, total_norm: 2.163320779800415, lr: 0.0005399999999999999
Step 9, training loss: 8.094059944152832, total_norm: 1.9697264432907104, lr: 0.0006
Step 10, training loss: 7.544427394866943, total_norm: 2.465020179748535, lr: 0.0005999999999999998
Step 11, training loss: 6.8497209548950195, total_norm: 1.7199389934539795, lr: 0.0005991676801079444
Step 12, training loss: 7.164328098297119, total_norm: 2.0977907180786133, lr: 0.0005966758519606872
Step 13, training loss: 6.636142253875732, total_norm: 1.3986064195632935, lr: 0.0005925398785073725
Step 14, training loss: 6.692885875701904, total_norm: 1.9392787218093872, lr: 0.0005867852593996914
Step 15, training loss: 6.275835990905762, total_norm: 1.7515614032745361, lr: 0.0005794474737780473
Step 16, training loss: 6.546823024749756, total_norm: 2.213273525238037, lr: 0.0005705717615308592
Step 17, training loss: 6.7396440505981445, total_norm: 2.050086259841919, lr: 0.0005602128443756048
Step 18, training loss: 6.727998733520508, total_norm: 2.1169657707214355, lr: 0.0005484345884812357
Step 19, training loss: 6.313828468322754, total_norm: 1.8637112379074097, lr: 0.0005353096107120083
Step 20, training loss: 6.457025527954102, total_norm: 1.6126652956008911, lr: 0.0005209188309203677
Step 21, training loss: 6.259544372558594, total_norm: 1.9965083599090576, lr: 0.0005053509730491494
Step 22, training loss: 7.4231181144714355, total_norm: 2.533914566040039, lr: 0.0004887020181189676
Step 23, training loss: 6.726557731628418, total_norm: 1.9738283157348633, lr: 0.0004710746124733061
Step 24, training loss: 6.511180400848389, total_norm: 6.776599407196045, lr: 0.0004525774349296775
Step 25, training loss: 6.360112190246582, total_norm: 2.3788979053497314, lr: 0.00043332452673857416
Step 26, training loss: 7.110931873321533, total_norm: 1.8580224514007568, lr: 0.00041343458848123576
Step 27, training loss: 6.531113624572754, total_norm: 1.827165961265564, lr: 0.00039303024824109445
Step 28, training loss: 6.752917766571045, total_norm: 1.9833768606185913, lr: 0.0003722373055608623
Step 29, training loss: 6.314686298370361, total_norm: 1.7518492937088013, lr: 0.0003511839558465181
Step 30, training loss: 6.8870158195495605, total_norm: 1.7319632768630981, lr: 0.00032999999999999994
Step 31, training loss: 6.166318416595459, total_norm: 1.7236261367797852, lr: 0.0003088160441534818
Step 32, training loss: 6.310960292816162, total_norm: 1.6534864902496338, lr: 0.0002877626944391376
Step 33, training loss: 6.283860206604004, total_norm: 1.869521975517273, lr: 0.0002669697517589056
Step 34, training loss: 6.061735153198242, total_norm: 2.243509531021118, lr: 0.0002465654115187642
Step 35, training loss: 6.579897880554199, total_norm: 1.6962898969650269, lr: 0.00022667547326142573
Step 36, training loss: 5.598980903625488, total_norm: 1.9063999652862549, lr: 0.00020742256507032234
Step 37, training loss: 6.502202033996582, total_norm: 2.3530657291412354, lr: 0.00018892538752669378
Step 38, training loss: 6.600704193115234, total_norm: 2.080707311630249, lr: 0.00017129798188103226
Step 39, training loss: 5.756231784820557, total_norm: 1.8666925430297852, lr: 0.00015464902695085044
Step 40, training loss: 5.351025104522705, total_norm: 2.3488590717315674, lr: 0.00013908116907963218
Step 41, training loss: 5.894318580627441, total_norm: 2.1256744861602783, lr: 0.00012469038928799165
Step 42, training loss: 6.315521717071533, total_norm: 1.802610993385315, lr: 0.00011156541151876421
Step 43, training loss: 6.561228275299072, total_norm: 1.7851208448410034, lr: 9.97871556243951e-05
Step 44, training loss: 5.896499156951904, total_norm: 1.7647515535354614, lr: 8.942823846914069e-05
Step 45, training loss: 5.430598258972168, total_norm: 1.9561893939971924, lr: 8.055252622195258e-05
Step 46, training loss: 5.541513919830322, total_norm: 1.909793734550476, lr: 7.321474060030853e-05
Step 47, training loss: 5.408646583557129, total_norm: 2.054288148880005, lr: 6.746012149262733e-05
Step 48, training loss: 5.947915077209473, total_norm: 1.5721465349197388, lr: 6.332414803931283e-05
Step 49, training loss: 5.875054359436035, total_norm: 1.457417607307434, lr: 6.083231989205545e-05
Step 50, training loss: 5.448706150054932, total_norm: 1.6546834707260132, lr: 5.9999999999999995e-05
Step 50, validation loss: 6.9930877685546875
Step 51, training loss: 6.382951259613037, total_norm: 1.6507930755615234, lr: 5.9999999999999995e-05
Step 52, training loss: 6.175592422485352, total_norm: 1.8961645364761353, lr: 5.9999999999999995e-05
Step 53, training loss: 5.7774200439453125, total_norm: 1.49481201171875, lr: 5.9999999999999995e-05
Step 54, training loss: 5.6100969314575195, total_norm: 1.3763681650161743, lr: 5.9999999999999995e-05
Step 55, training loss: 5.746230125427246, total_norm: 1.3389697074890137, lr: 5.9999999999999995e-05
Step 56, training loss: 5.578695297241211, total_norm: 1.5886049270629883, lr: 5.9999999999999995e-05
Step 57, training loss: 6.342520236968994, total_norm: 1.6474566459655762, lr: 5.9999999999999995e-05
Step 58, training loss: 6.4860148429870605, total_norm: 1.7456912994384766, lr: 5.9999999999999995e-05
Step 59, training loss: 5.391048908233643, total_norm: 1.7758060693740845, lr: 5.9999999999999995e-05
Step 60, training loss: 5.435286521911621, total_norm: 1.6341850757598877, lr: 5.9999999999999995e-05
Step 61, training loss: 6.406947612762451, total_norm: 2.0351195335388184, lr: 5.9999999999999995e-05
Step 62, training loss: 6.174123764038086, total_norm: 1.5468027591705322, lr: 5.9999999999999995e-05
Step 63, training loss: 5.518178939819336, total_norm: 2.407710075378418, lr: 5.9999999999999995e-05
Step 64, training loss: 5.8205108642578125, total_norm: 2.1218254566192627, lr: 5.9999999999999995e-05
Step 65, training loss: 6.829455375671387, total_norm: 2.1601784229278564, lr: 5.9999999999999995e-05
Step 66, training loss: 6.618896961212158, total_norm: 1.7182115316390991, lr: 5.9999999999999995e-05
Step 67, training loss: 5.827852249145508, total_norm: 2.5779805183410645, lr: 5.9999999999999995e-05
Step 68, training loss: 6.374567985534668, total_norm: 1.8982831239700317, lr: 5.9999999999999995e-05
Step 69, training loss: 5.7038893699646, total_norm: 1.690022349357605, lr: 5.9999999999999995e-05
Step 70, training loss: 5.635043144226074, total_norm: 2.337489604949951, lr: 5.9999999999999995e-05
Step 71, training loss: 5.487753868103027, total_norm: 1.9628715515136719, lr: 5.9999999999999995e-05
Step 72, training loss: 5.683722019195557, total_norm: 1.820812463760376, lr: 5.9999999999999995e-05
Step 73, training loss: 5.494344234466553, total_norm: 1.618899941444397, lr: 5.9999999999999995e-05
Step 74, training loss: 6.1231160163879395, total_norm: 1.6189231872558594, lr: 5.9999999999999995e-05
Step 75, training loss: 6.015523910522461, total_norm: 2.2103946208953857, lr: 5.9999999999999995e-05
Step 76, training loss: 5.631886005401611, total_norm: 1.9335819482803345, lr: 5.9999999999999995e-05
Step 77, training loss: 5.277296543121338, total_norm: 1.807732343673706, lr: 5.9999999999999995e-05
Step 78, training loss: 6.063194751739502, total_norm: 2.1034417152404785, lr: 5.9999999999999995e-05
Step 79, training loss: 6.300579071044922, total_norm: 1.766161561012268, lr: 5.9999999999999995e-05
Step 80, training loss: 5.8366804122924805, total_norm: 1.4137511253356934, lr: 5.9999999999999995e-05
Step 81, training loss: 6.422974586486816, total_norm: 1.5243191719055176, lr: 5.9999999999999995e-05
Step 82, training loss: 6.070600986480713, total_norm: 1.9325454235076904, lr: 5.9999999999999995e-05
Step 83, training loss: 6.475867748260498, total_norm: 1.948969841003418, lr: 5.9999999999999995e-05
Step 84, training loss: 6.581255912780762, total_norm: 1.6923092603683472, lr: 5.9999999999999995e-05
Step 85, training loss: 5.6481523513793945, total_norm: 1.8145047426223755, lr: 5.9999999999999995e-05
Step 86, training loss: 6.046788215637207, total_norm: 1.9169790744781494, lr: 5.9999999999999995e-05
Step 87, training loss: 6.053215980529785, total_norm: 1.9010593891143799, lr: 5.9999999999999995e-05
Step 88, training loss: 6.13636589050293, total_norm: 1.8782068490982056, lr: 5.9999999999999995e-05
Step 89, training loss: 6.111469745635986, total_norm: 1.7506827116012573, lr: 5.9999999999999995e-05
Step 90, training loss: 5.886587619781494, total_norm: 2.342031478881836, lr: 5.9999999999999995e-05
Step 91, training loss: 6.157519817352295, total_norm: 1.8511439561843872, lr: 5.9999999999999995e-05
Step 92, training loss: 7.069211959838867, total_norm: 1.680670142173767, lr: 5.9999999999999995e-05
Step 93, training loss: 6.7967658042907715, total_norm: 1.6878701448440552, lr: 5.9999999999999995e-05
Step 94, training loss: 6.804399013519287, total_norm: 1.923231840133667, lr: 5.9999999999999995e-05
Step 95, training loss: 6.516629695892334, total_norm: 2.064481258392334, lr: 5.9999999999999995e-05
Step 96, training loss: 6.9346208572387695, total_norm: 1.9059576988220215, lr: 5.9999999999999995e-05
Step 97, training loss: 6.879948139190674, total_norm: 1.9236674308776855, lr: 5.9999999999999995e-05
Step 98, training loss: 6.8292951583862305, total_norm: 2.336555004119873, lr: 5.9999999999999995e-05
Step 99, training loss: 6.542308807373047, total_norm: 2.206082582473755, lr: 5.9999999999999995e-05
Step 100, training loss: 6.781918048858643, total_norm: 2.2578377723693848, lr: 5.9999999999999995e-05
Step 100, validation loss: 6.6910080909729
Step 101, training loss: 6.360531330108643, total_norm: 2.121417760848999, lr: 5.9999999999999995e-05
Step 102, training loss: 6.507232666015625, total_norm: 1.7911590337753296, lr: 5.9999999999999995e-05
Step 103, training loss: 6.6606926918029785, total_norm: 1.6709808111190796, lr: 5.9999999999999995e-05
Step 104, training loss: 6.702497959136963, total_norm: 1.8763667345046997, lr: 5.9999999999999995e-05
Step 105, training loss: 6.865561485290527, total_norm: 1.7778456211090088, lr: 5.9999999999999995e-05
Step 106, training loss: 6.582733154296875, total_norm: 1.7009249925613403, lr: 5.9999999999999995e-05
Step 107, training loss: 6.392313480377197, total_norm: 1.7507939338684082, lr: 5.9999999999999995e-05
Step 108, training loss: 6.558557987213135, total_norm: 1.699337124824524, lr: 5.9999999999999995e-05
Step 109, training loss: 6.734888553619385, total_norm: 2.1198344230651855, lr: 5.9999999999999995e-05
Step 110, training loss: 6.568641662597656, total_norm: 1.8957170248031616, lr: 5.9999999999999995e-05
Step 111, training loss: 6.3772077560424805, total_norm: 1.674682378768921, lr: 5.9999999999999995e-05
Step 112, training loss: 6.244932174682617, total_norm: 1.6857744455337524, lr: 5.9999999999999995e-05
Step 113, training loss: 6.368901252746582, total_norm: 1.5926172733306885, lr: 5.9999999999999995e-05
Step 114, training loss: 6.874480724334717, total_norm: 1.966224193572998, lr: 5.9999999999999995e-05
Step 115, training loss: 6.222782611846924, total_norm: 1.6153841018676758, lr: 5.9999999999999995e-05
Step 116, training loss: 6.356637001037598, total_norm: 2.655113697052002, lr: 5.9999999999999995e-05
Step 117, training loss: 5.833786964416504, total_norm: 2.285851001739502, lr: 5.9999999999999995e-05
Step 118, training loss: 6.339046478271484, total_norm: 1.8351291418075562, lr: 5.9999999999999995e-05
Step 119, training loss: 5.7843852043151855, total_norm: 2.084869384765625, lr: 5.9999999999999995e-05
Step 120, training loss: 5.952386856079102, total_norm: 1.861135482788086, lr: 5.9999999999999995e-05
Step 121, training loss: 6.3086981773376465, total_norm: 2.1616854667663574, lr: 5.9999999999999995e-05
Step 122, training loss: 6.267895221710205, total_norm: 1.9678438901901245, lr: 5.9999999999999995e-05
Step 123, training loss: 6.440422058105469, total_norm: 2.000807046890259, lr: 5.9999999999999995e-05
Step 124, training loss: 6.137362480163574, total_norm: 1.8677986860275269, lr: 5.9999999999999995e-05
Step 125, training loss: 6.325507640838623, total_norm: 1.7805317640304565, lr: 5.9999999999999995e-05
Step 126, training loss: 6.531624794006348, total_norm: 2.1023716926574707, lr: 5.9999999999999995e-05
Step 127, training loss: 6.284329891204834, total_norm: 1.7039874792099, lr: 5.9999999999999995e-05
Step 128, training loss: 5.87233304977417, total_norm: 3.250394105911255, lr: 5.9999999999999995e-05
Step 129, training loss: 6.1181793212890625, total_norm: 2.3606350421905518, lr: 5.9999999999999995e-05
Step 130, training loss: 6.372681140899658, total_norm: 2.4470431804656982, lr: 5.9999999999999995e-05
Step 131, training loss: 6.402728080749512, total_norm: 2.562203884124756, lr: 5.9999999999999995e-05
Step 132, training loss: 6.1829094886779785, total_norm: 1.7682301998138428, lr: 5.9999999999999995e-05
Step 133, training loss: 6.29425573348999, total_norm: 2.2667598724365234, lr: 5.9999999999999995e-05
Step 134, training loss: 5.8966288566589355, total_norm: 2.3635363578796387, lr: 5.9999999999999995e-05
Step 135, training loss: 5.632038116455078, total_norm: 2.5067827701568604, lr: 5.9999999999999995e-05
Step 136, training loss: 6.000830173492432, total_norm: 1.6126960515975952, lr: 5.9999999999999995e-05
Step 137, training loss: 5.8945465087890625, total_norm: 1.8146264553070068, lr: 5.9999999999999995e-05
Step 138, training loss: 6.100533962249756, total_norm: 1.8201640844345093, lr: 5.9999999999999995e-05
Step 139, training loss: 5.842684745788574, total_norm: 1.8449472188949585, lr: 5.9999999999999995e-05
Step 140, training loss: 5.712916851043701, total_norm: 1.8483710289001465, lr: 5.9999999999999995e-05
Step 141, training loss: 5.96818208694458, total_norm: 1.8049569129943848, lr: 5.9999999999999995e-05
Step 142, training loss: 5.7099456787109375, total_norm: 1.863778829574585, lr: 5.9999999999999995e-05
Step 143, training loss: 6.062418460845947, total_norm: 1.7622419595718384, lr: 5.9999999999999995e-05
Step 144, training loss: 6.188234329223633, total_norm: 1.7165509462356567, lr: 5.9999999999999995e-05
Step 145, training loss: 5.92563533782959, total_norm: 2.108950138092041, lr: 5.9999999999999995e-05
Step 146, training loss: 6.134703159332275, total_norm: 2.0190367698669434, lr: 5.9999999999999995e-05
Step 147, training loss: 6.265284061431885, total_norm: 1.793144941329956, lr: 5.9999999999999995e-05
Step 148, training loss: 6.164726734161377, total_norm: 1.7611174583435059, lr: 5.9999999999999995e-05
Step 149, training loss: 5.8042707443237305, total_norm: 2.0440030097961426, lr: 5.9999999999999995e-05
Step 150, training loss: 6.194312572479248, total_norm: 1.9535542726516724, lr: 5.9999999999999995e-05
Step 150, validation loss: 6.499777793884277
Step 151, training loss: 6.08475923538208, total_norm: 1.8254728317260742, lr: 5.9999999999999995e-05
Step 152, training loss: 6.050795078277588, total_norm: 1.703879475593567, lr: 5.9999999999999995e-05
Step 153, training loss: 5.865245819091797, total_norm: 2.290158748626709, lr: 5.9999999999999995e-05
Step 154, training loss: 5.98656702041626, total_norm: 2.1771023273468018, lr: 5.9999999999999995e-05
Step 155, training loss: 6.15355920791626, total_norm: 1.8466739654541016, lr: 5.9999999999999995e-05
Step 156, training loss: 6.193756580352783, total_norm: 2.539900064468384, lr: 5.9999999999999995e-05
Step 157, training loss: 6.309108257293701, total_norm: 2.7589058876037598, lr: 5.9999999999999995e-05
Step 158, training loss: 6.194482803344727, total_norm: 2.6170594692230225, lr: 5.9999999999999995e-05
Step 159, training loss: 5.876954078674316, total_norm: 3.1958868503570557, lr: 5.9999999999999995e-05
Step 160, training loss: 6.58133602142334, total_norm: 1.9523236751556396, lr: 5.9999999999999995e-05
Step 161, training loss: 6.546635627746582, total_norm: 2.193816900253296, lr: 5.9999999999999995e-05
Step 162, training loss: 6.411284923553467, total_norm: 2.225151777267456, lr: 5.9999999999999995e-05
Step 163, training loss: 6.327127933502197, total_norm: 2.142503261566162, lr: 5.9999999999999995e-05
Step 164, training loss: 6.363607406616211, total_norm: 1.9545310735702515, lr: 5.9999999999999995e-05
Step 165, training loss: 6.097361087799072, total_norm: 2.9810354709625244, lr: 5.9999999999999995e-05
Step 166, training loss: 6.125830173492432, total_norm: 2.1808526515960693, lr: 5.9999999999999995e-05
Step 167, training loss: 6.22459077835083, total_norm: 2.3532605171203613, lr: 5.9999999999999995e-05
Step 168, training loss: 5.799418926239014, total_norm: 2.384910821914673, lr: 5.9999999999999995e-05
Step 169, training loss: 5.94476318359375, total_norm: 2.7828075885772705, lr: 5.9999999999999995e-05
Step 170, training loss: 6.417841911315918, total_norm: 2.4498465061187744, lr: 5.9999999999999995e-05
Step 171, training loss: 5.74895715713501, total_norm: 3.7772457599639893, lr: 5.9999999999999995e-05
Step 172, training loss: 6.082788944244385, total_norm: 3.0412750244140625, lr: 5.9999999999999995e-05
Step 173, training loss: 5.934542179107666, total_norm: 2.2515645027160645, lr: 5.9999999999999995e-05
Step 174, training loss: 5.816658020019531, total_norm: 2.5617804527282715, lr: 5.9999999999999995e-05
Step 175, training loss: 5.859170436859131, total_norm: 2.3697144985198975, lr: 5.9999999999999995e-05
Step 176, training loss: 6.186806678771973, total_norm: 1.9067589044570923, lr: 5.9999999999999995e-05
Step 177, training loss: 6.293044090270996, total_norm: 1.8971142768859863, lr: 5.9999999999999995e-05
Step 178, training loss: 6.2889628410339355, total_norm: 2.120060443878174, lr: 5.9999999999999995e-05
Step 179, training loss: 6.2384514808654785, total_norm: 2.3409297466278076, lr: 5.9999999999999995e-05
Step 180, training loss: 5.922947406768799, total_norm: 2.770113229751587, lr: 5.9999999999999995e-05
Step 181, training loss: 6.291974067687988, total_norm: 2.129018545150757, lr: 5.9999999999999995e-05
Step 182, training loss: 6.215578556060791, total_norm: 2.836005687713623, lr: 5.9999999999999995e-05
Step 183, training loss: 6.120041370391846, total_norm: 1.907784342765808, lr: 5.9999999999999995e-05
Step 184, training loss: 6.011534214019775, total_norm: 2.259829521179199, lr: 5.9999999999999995e-05
Step 185, training loss: 6.0164008140563965, total_norm: 2.052194356918335, lr: 5.9999999999999995e-05
Step 186, training loss: 6.271049499511719, total_norm: 1.9776248931884766, lr: 5.9999999999999995e-05
Step 187, training loss: 6.328784465789795, total_norm: 2.1129684448242188, lr: 5.9999999999999995e-05
Step 188, training loss: 6.376135349273682, total_norm: 1.940393328666687, lr: 5.9999999999999995e-05
Step 189, training loss: 6.4440226554870605, total_norm: 1.9455509185791016, lr: 5.9999999999999995e-05
Step 190, training loss: 6.23347806930542, total_norm: 2.1278328895568848, lr: 5.9999999999999995e-05
Step 191, training loss: 6.452779293060303, total_norm: 1.8273893594741821, lr: 5.9999999999999995e-05
Step 192, training loss: 6.323281288146973, total_norm: 1.9813661575317383, lr: 5.9999999999999995e-05
Step 193, training loss: 6.456401824951172, total_norm: 2.130570888519287, lr: 5.9999999999999995e-05
Step 194, training loss: 6.0893778800964355, total_norm: 2.058871269226074, lr: 5.9999999999999995e-05
Step 195, training loss: 6.393607139587402, total_norm: 1.7508022785186768, lr: 5.9999999999999995e-05
Step 196, training loss: 6.354621410369873, total_norm: 1.9452186822891235, lr: 5.9999999999999995e-05
Step 197, training loss: 6.567707538604736, total_norm: 1.8574309349060059, lr: 5.9999999999999995e-05
Step 198, training loss: 6.267397403717041, total_norm: 1.984493374824524, lr: 5.9999999999999995e-05
Step 199, training loss: 6.143811225891113, total_norm: 1.8295822143554688, lr: 5.9999999999999995e-05
Step 200, training loss: 6.157985687255859, total_norm: 2.3481948375701904, lr: 5.9999999999999995e-05
Step 200, validation loss: 6.419406890869141
Step 201, training loss: 6.275787353515625, total_norm: 1.6755253076553345, lr: 5.9999999999999995e-05
Step 202, training loss: 6.252768039703369, total_norm: 1.6783456802368164, lr: 5.9999999999999995e-05
Step 203, training loss: 6.254484176635742, total_norm: 2.0580060482025146, lr: 5.9999999999999995e-05
Step 204, training loss: 6.646084308624268, total_norm: 2.25950026512146, lr: 5.9999999999999995e-05
Step 205, training loss: 6.0133562088012695, total_norm: 2.3567588329315186, lr: 5.9999999999999995e-05
Step 206, training loss: 5.998868465423584, total_norm: 2.016845464706421, lr: 5.9999999999999995e-05
Step 207, training loss: 6.109288215637207, total_norm: 2.139521837234497, lr: 5.9999999999999995e-05
Step 208, training loss: 6.0897345542907715, total_norm: 1.8557217121124268, lr: 5.9999999999999995e-05
Step 209, training loss: 6.249268054962158, total_norm: 2.5483081340789795, lr: 5.9999999999999995e-05
Step 210, training loss: 6.5980401039123535, total_norm: 2.282212972640991, lr: 5.9999999999999995e-05
Step 211, training loss: 6.2049994468688965, total_norm: 2.039640188217163, lr: 5.9999999999999995e-05
Step 212, training loss: 6.065828323364258, total_norm: 1.8385030031204224, lr: 5.9999999999999995e-05
Step 213, training loss: 5.822207927703857, total_norm: 1.9476957321166992, lr: 5.9999999999999995e-05
Step 214, training loss: 6.0637431144714355, total_norm: 2.0676729679107666, lr: 5.9999999999999995e-05
Step 215, training loss: 6.019453048706055, total_norm: 2.208224058151245, lr: 5.9999999999999995e-05
Step 216, training loss: 6.002498626708984, total_norm: 2.5313560962677, lr: 5.9999999999999995e-05
Step 217, training loss: 5.951335430145264, total_norm: 2.237056255340576, lr: 5.9999999999999995e-05
Step 218, training loss: 5.943198204040527, total_norm: 1.893354058265686, lr: 5.9999999999999995e-05
Step 219, training loss: 6.167470932006836, total_norm: 2.0531179904937744, lr: 5.9999999999999995e-05
Step 220, training loss: 6.12924861907959, total_norm: 2.2920093536376953, lr: 5.9999999999999995e-05
Step 221, training loss: 6.237090110778809, total_norm: 2.169987916946411, lr: 5.9999999999999995e-05
Step 222, training loss: 6.393745422363281, total_norm: 2.169347047805786, lr: 5.9999999999999995e-05
Step 223, training loss: 6.368893623352051, total_norm: 1.8584915399551392, lr: 5.9999999999999995e-05
Step 224, training loss: 6.3511786460876465, total_norm: 1.9366048574447632, lr: 5.9999999999999995e-05
Step 225, training loss: 6.263890743255615, total_norm: 1.8063385486602783, lr: 5.9999999999999995e-05
Step 226, training loss: 5.821198463439941, total_norm: 1.8078218698501587, lr: 5.9999999999999995e-05
Step 227, training loss: 6.192999839782715, total_norm: 1.9735894203186035, lr: 5.9999999999999995e-05
Step 228, training loss: 6.299842357635498, total_norm: 1.7882647514343262, lr: 5.9999999999999995e-05
Step 229, training loss: 5.995068550109863, total_norm: 1.6767423152923584, lr: 5.9999999999999995e-05
Step 230, training loss: 6.24462366104126, total_norm: 1.971447467803955, lr: 5.9999999999999995e-05
Step 231, training loss: 5.411448001861572, total_norm: 2.4491119384765625, lr: 5.9999999999999995e-05
Step 232, training loss: 6.008717060089111, total_norm: 2.170020580291748, lr: 5.9999999999999995e-05
Step 233, training loss: 6.274430274963379, total_norm: 2.0927939414978027, lr: 5.9999999999999995e-05
Step 234, training loss: 5.894476890563965, total_norm: 1.7893089056015015, lr: 5.9999999999999995e-05
Step 235, training loss: 5.8135600090026855, total_norm: 2.191418170928955, lr: 5.9999999999999995e-05
Step 236, training loss: 5.925241947174072, total_norm: 2.0288162231445312, lr: 5.9999999999999995e-05
Step 237, training loss: 6.1344170570373535, total_norm: 2.0397274494171143, lr: 5.9999999999999995e-05
Step 238, training loss: 5.91456413269043, total_norm: 2.397641658782959, lr: 5.9999999999999995e-05
Step 239, training loss: 5.583303928375244, total_norm: 2.1544458866119385, lr: 5.9999999999999995e-05
Step 240, training loss: 6.056760787963867, total_norm: 1.881993293762207, lr: 5.9999999999999995e-05
Step 241, training loss: 5.665711879730225, total_norm: 1.9687776565551758, lr: 5.9999999999999995e-05
Step 242, training loss: 5.276344299316406, total_norm: 2.7874503135681152, lr: 5.9999999999999995e-05
Step 243, training loss: 6.0154290199279785, total_norm: 1.7927062511444092, lr: 5.9999999999999995e-05
Step 244, training loss: 6.122443675994873, total_norm: 2.0106284618377686, lr: 5.9999999999999995e-05
Step 245, training loss: 5.574038028717041, total_norm: 2.260589838027954, lr: 5.9999999999999995e-05
Step 246, training loss: 6.038818359375, total_norm: 1.979156494140625, lr: 5.9999999999999995e-05
Step 247, training loss: 5.236830234527588, total_norm: 3.069859027862549, lr: 5.9999999999999995e-05
Step 248, training loss: 5.165914058685303, total_norm: 3.0244204998016357, lr: 5.9999999999999995e-05
Step 249, training loss: 5.529805660247803, total_norm: 2.2990562915802, lr: 5.9999999999999995e-05
Step 250, training loss: 5.235320568084717, total_norm: 2.78878116607666, lr: 5.9999999999999995e-05
Step 250, validation loss: 6.307806491851807
Step 251, training loss: 5.786770820617676, total_norm: 1.8897664546966553, lr: 5.9999999999999995e-05
Step 252, training loss: 5.566351890563965, total_norm: 2.1235697269439697, lr: 5.9999999999999995e-05
Step 253, training loss: 5.484491348266602, total_norm: 1.9359735250473022, lr: 5.9999999999999995e-05
Step 254, training loss: 6.062770366668701, total_norm: 1.9404098987579346, lr: 5.9999999999999995e-05
Step 255, training loss: 6.169434070587158, total_norm: 2.043219804763794, lr: 5.9999999999999995e-05
Step 256, training loss: 5.7086920738220215, total_norm: 2.051297187805176, lr: 5.9999999999999995e-05
Step 257, training loss: 5.875913619995117, total_norm: 1.9948019981384277, lr: 5.9999999999999995e-05
Step 258, training loss: 6.042059421539307, total_norm: 2.3577911853790283, lr: 5.9999999999999995e-05
Step 259, training loss: 5.950961112976074, total_norm: 3.4578583240509033, lr: 5.9999999999999995e-05
Step 260, training loss: 6.2514848709106445, total_norm: 2.2141551971435547, lr: 5.9999999999999995e-05
Step 261, training loss: 6.283284664154053, total_norm: 1.927863359451294, lr: 5.9999999999999995e-05
Step 262, training loss: 5.7869367599487305, total_norm: 2.7215468883514404, lr: 5.9999999999999995e-05
Step 263, training loss: 5.80025053024292, total_norm: 2.174731492996216, lr: 5.9999999999999995e-05
Step 264, training loss: 6.126290798187256, total_norm: 2.073867082595825, lr: 5.9999999999999995e-05
Step 265, training loss: 6.384591102600098, total_norm: 1.7591930627822876, lr: 5.9999999999999995e-05
Step 266, training loss: 6.360544681549072, total_norm: 2.457956075668335, lr: 5.9999999999999995e-05
Step 267, training loss: 6.010013103485107, total_norm: 2.0138490200042725, lr: 5.9999999999999995e-05
Step 268, training loss: 6.231297969818115, total_norm: 2.327817916870117, lr: 5.9999999999999995e-05
Step 269, training loss: 6.108551979064941, total_norm: 1.802682638168335, lr: 5.9999999999999995e-05
Step 270, training loss: 6.120953559875488, total_norm: 1.7510106563568115, lr: 5.9999999999999995e-05
Step 271, training loss: 5.9373321533203125, total_norm: 1.9227105379104614, lr: 5.9999999999999995e-05
Step 272, training loss: 6.963216304779053, total_norm: 2.477388381958008, lr: 5.9999999999999995e-05
Step 273, training loss: 6.298207759857178, total_norm: 2.6077988147735596, lr: 5.9999999999999995e-05
Step 274, training loss: 6.044888973236084, total_norm: 2.0443785190582275, lr: 5.9999999999999995e-05
Step 275, training loss: 5.870861053466797, total_norm: 2.1860439777374268, lr: 5.9999999999999995e-05
Step 276, training loss: 5.564894676208496, total_norm: 2.1950838565826416, lr: 5.9999999999999995e-05
Step 277, training loss: 5.792491912841797, total_norm: 2.1449177265167236, lr: 5.9999999999999995e-05
Step 278, training loss: 6.321321964263916, total_norm: 2.2873778343200684, lr: 5.9999999999999995e-05
Step 279, training loss: 5.997747421264648, total_norm: 2.076230049133301, lr: 5.9999999999999995e-05
Step 280, training loss: 5.757631778717041, total_norm: 2.2364211082458496, lr: 5.9999999999999995e-05
Step 281, training loss: 5.928417205810547, total_norm: 2.1285018920898438, lr: 5.9999999999999995e-05
Step 282, training loss: 5.505539894104004, total_norm: 2.1605312824249268, lr: 5.9999999999999995e-05
Step 283, training loss: 5.768487930297852, total_norm: 2.355848789215088, lr: 5.9999999999999995e-05
Step 284, training loss: 6.789752006530762, total_norm: 2.399592876434326, lr: 5.9999999999999995e-05
Step 285, training loss: 6.067865371704102, total_norm: 2.06880521774292, lr: 5.9999999999999995e-05
Step 286, training loss: 5.808725357055664, total_norm: 2.190880537033081, lr: 5.9999999999999995e-05
Step 287, training loss: 6.793057918548584, total_norm: 2.9348394870758057, lr: 5.9999999999999995e-05
Step 288, training loss: 5.931552410125732, total_norm: 3.09757137298584, lr: 5.9999999999999995e-05
Step 289, training loss: 5.543315887451172, total_norm: 2.8046436309814453, lr: 5.9999999999999995e-05
Step 290, training loss: 5.855637550354004, total_norm: 2.477588415145874, lr: 5.9999999999999995e-05
Step 291, training loss: 5.598538398742676, total_norm: 2.5109994411468506, lr: 5.9999999999999995e-05
Step 292, training loss: 6.020452976226807, total_norm: 2.0780749320983887, lr: 5.9999999999999995e-05
Step 293, training loss: 5.490357875823975, total_norm: 2.140803813934326, lr: 5.9999999999999995e-05
Step 294, training loss: 6.003288269042969, total_norm: 2.645500898361206, lr: 5.9999999999999995e-05
Step 295, training loss: 5.848155975341797, total_norm: 2.203218460083008, lr: 5.9999999999999995e-05
Step 296, training loss: 6.198807716369629, total_norm: 2.534095287322998, lr: 5.9999999999999995e-05
Step 297, training loss: 5.746373176574707, total_norm: 2.315742254257202, lr: 5.9999999999999995e-05
Step 298, training loss: 5.965660095214844, total_norm: 2.188904047012329, lr: 5.9999999999999995e-05
Step 299, training loss: 5.583475112915039, total_norm: 2.3666961193084717, lr: 5.9999999999999995e-05
Step 300, training loss: 6.477718353271484, total_norm: 2.2565062046051025, lr: 5.9999999999999995e-05
Step 300, validation loss: 6.219057559967041
Step 301, training loss: 5.990242004394531, total_norm: 2.224416494369507, lr: 5.9999999999999995e-05
Step 302, training loss: 5.794076919555664, total_norm: 2.266481637954712, lr: 5.9999999999999995e-05
Step 303, training loss: 6.104705333709717, total_norm: 2.0689890384674072, lr: 5.9999999999999995e-05
Step 304, training loss: 6.001014232635498, total_norm: 2.120255947113037, lr: 5.9999999999999995e-05
Step 305, training loss: 5.980412006378174, total_norm: 2.058671474456787, lr: 5.9999999999999995e-05
Step 306, training loss: 6.000302791595459, total_norm: 2.0836379528045654, lr: 5.9999999999999995e-05
Step 307, training loss: 5.646425724029541, total_norm: 2.0075345039367676, lr: 5.9999999999999995e-05
Step 308, training loss: 6.014038562774658, total_norm: 2.0669472217559814, lr: 5.9999999999999995e-05
Step 309, training loss: 5.473677635192871, total_norm: 2.0580203533172607, lr: 5.9999999999999995e-05
Step 310, training loss: 5.758210182189941, total_norm: 1.8562551736831665, lr: 5.9999999999999995e-05
Step 311, training loss: 5.42272424697876, total_norm: 2.169674873352051, lr: 5.9999999999999995e-05
Step 312, training loss: 5.250438213348389, total_norm: 2.696871757507324, lr: 5.9999999999999995e-05
Step 313, training loss: 5.6304192543029785, total_norm: 2.2579193115234375, lr: 5.9999999999999995e-05
Step 314, training loss: 5.796422481536865, total_norm: 1.9259285926818848, lr: 5.9999999999999995e-05
Step 315, training loss: 5.594860553741455, total_norm: 1.9566421508789062, lr: 5.9999999999999995e-05
Step 316, training loss: 5.3078083992004395, total_norm: 1.814509391784668, lr: 5.9999999999999995e-05
Step 317, training loss: 5.5030364990234375, total_norm: 2.5340397357940674, lr: 5.9999999999999995e-05
Step 318, training loss: 5.348382949829102, total_norm: 2.1569573879241943, lr: 5.9999999999999995e-05
Step 319, training loss: 6.190915107727051, total_norm: 2.004984140396118, lr: 5.9999999999999995e-05
Step 320, training loss: 5.721935272216797, total_norm: 1.9104269742965698, lr: 5.9999999999999995e-05
Step 321, training loss: 5.566033840179443, total_norm: 2.092580795288086, lr: 5.9999999999999995e-05
Step 322, training loss: 5.626053810119629, total_norm: 1.8980275392532349, lr: 5.9999999999999995e-05
Step 323, training loss: 5.683907508850098, total_norm: 2.2621870040893555, lr: 5.9999999999999995e-05
Step 324, training loss: 5.481263637542725, total_norm: 3.326270580291748, lr: 5.9999999999999995e-05
Step 325, training loss: 5.659255027770996, total_norm: 2.510413408279419, lr: 5.9999999999999995e-05
Step 326, training loss: 5.896923065185547, total_norm: 2.2195725440979004, lr: 5.9999999999999995e-05
Step 327, training loss: 6.230494499206543, total_norm: 5.480327606201172, lr: 5.9999999999999995e-05
Step 328, training loss: 5.7807841300964355, total_norm: 2.197303295135498, lr: 5.9999999999999995e-05
Step 329, training loss: 6.399019718170166, total_norm: 2.118405342102051, lr: 5.9999999999999995e-05
Step 330, training loss: 5.802630424499512, total_norm: 2.380321741104126, lr: 5.9999999999999995e-05
Step 331, training loss: 6.08802604675293, total_norm: 1.8126450777053833, lr: 5.9999999999999995e-05
Step 332, training loss: 5.550503253936768, total_norm: 1.7653381824493408, lr: 5.9999999999999995e-05
Step 333, training loss: 6.102684497833252, total_norm: 2.283698320388794, lr: 5.9999999999999995e-05
Step 334, training loss: 5.6089396476745605, total_norm: 2.5360195636749268, lr: 5.9999999999999995e-05
Step 335, training loss: 5.6817708015441895, total_norm: 2.256359577178955, lr: 5.9999999999999995e-05
Step 336, training loss: 5.705868721008301, total_norm: 2.318387269973755, lr: 5.9999999999999995e-05
Step 337, training loss: 5.931467533111572, total_norm: 2.0597116947174072, lr: 5.9999999999999995e-05
Step 338, training loss: 5.648942947387695, total_norm: 1.8725296258926392, lr: 5.9999999999999995e-05
Step 339, training loss: 5.861388206481934, total_norm: 2.1901133060455322, lr: 5.9999999999999995e-05
Step 340, training loss: 6.017776966094971, total_norm: 2.608877420425415, lr: 5.9999999999999995e-05
Step 341, training loss: 5.713984966278076, total_norm: 2.3622851371765137, lr: 5.9999999999999995e-05
Step 342, training loss: 5.727512836456299, total_norm: 2.235172986984253, lr: 5.9999999999999995e-05
Step 343, training loss: 5.739133358001709, total_norm: 2.7348668575286865, lr: 5.9999999999999995e-05
Step 344, training loss: 5.717920780181885, total_norm: 2.7334611415863037, lr: 5.9999999999999995e-05
Step 345, training loss: 5.7987165451049805, total_norm: 2.8531346321105957, lr: 5.9999999999999995e-05
Step 346, training loss: 5.587217807769775, total_norm: 2.574171781539917, lr: 5.9999999999999995e-05
Step 347, training loss: 5.583457946777344, total_norm: 2.4702341556549072, lr: 5.9999999999999995e-05
Step 348, training loss: 5.672730922698975, total_norm: 2.0362443923950195, lr: 5.9999999999999995e-05
Step 349, training loss: 5.608636379241943, total_norm: 1.964400053024292, lr: 5.9999999999999995e-05
Step 350, training loss: 6.244619846343994, total_norm: 2.228590965270996, lr: 5.9999999999999995e-05
Step 350, validation loss: 6.191658973693848
Step 351, training loss: 6.004638195037842, total_norm: 2.0102996826171875, lr: 5.9999999999999995e-05
Step 352, training loss: 5.743391513824463, total_norm: 2.224438428878784, lr: 5.9999999999999995e-05
Step 353, training loss: 6.239285469055176, total_norm: 1.8599587678909302, lr: 5.9999999999999995e-05
Step 354, training loss: 5.73307466506958, total_norm: 1.736365556716919, lr: 5.9999999999999995e-05
Step 355, training loss: 6.0804972648620605, total_norm: 1.930930256843567, lr: 5.9999999999999995e-05
Step 356, training loss: 5.980029582977295, total_norm: 1.821998119354248, lr: 5.9999999999999995e-05
Step 357, training loss: 6.060342311859131, total_norm: 1.8093914985656738, lr: 5.9999999999999995e-05
Step 358, training loss: 5.856289863586426, total_norm: 1.8917436599731445, lr: 5.9999999999999995e-05
Step 359, training loss: 5.9577956199646, total_norm: 1.7720204591751099, lr: 5.9999999999999995e-05
Step 360, training loss: 5.979990005493164, total_norm: 2.160278558731079, lr: 5.9999999999999995e-05
Step 361, training loss: 5.580495357513428, total_norm: 2.0666778087615967, lr: 5.9999999999999995e-05
Step 362, training loss: 5.398932456970215, total_norm: 2.701470136642456, lr: 5.9999999999999995e-05
Step 363, training loss: 5.525434494018555, total_norm: 2.230156898498535, lr: 5.9999999999999995e-05
Step 364, training loss: 5.863421440124512, total_norm: 2.3175384998321533, lr: 5.9999999999999995e-05
Step 365, training loss: 5.736886978149414, total_norm: 1.8760786056518555, lr: 5.9999999999999995e-05
Step 366, training loss: 5.780447959899902, total_norm: 2.1653780937194824, lr: 5.9999999999999995e-05
Step 367, training loss: 6.108839988708496, total_norm: 2.350527048110962, lr: 5.9999999999999995e-05
Step 368, training loss: 5.645057678222656, total_norm: 2.1059165000915527, lr: 5.9999999999999995e-05
Step 369, training loss: 5.622134208679199, total_norm: 2.280775308609009, lr: 5.9999999999999995e-05
Step 370, training loss: 6.025310039520264, total_norm: 1.995185136795044, lr: 5.9999999999999995e-05
Step 371, training loss: 5.686318397521973, total_norm: 2.2873852252960205, lr: 5.9999999999999995e-05
Step 372, training loss: 5.900197982788086, total_norm: 1.764522671699524, lr: 5.9999999999999995e-05
Step 373, training loss: 5.632194995880127, total_norm: 1.9577549695968628, lr: 5.9999999999999995e-05
Step 374, training loss: 5.186794757843018, total_norm: 2.903585195541382, lr: 5.9999999999999995e-05
Step 375, training loss: 5.333836078643799, total_norm: 2.238339424133301, lr: 5.9999999999999995e-05
Step 376, training loss: 5.022907257080078, total_norm: 4.249610900878906, lr: 5.9999999999999995e-05
Step 377, training loss: 5.196080684661865, total_norm: 3.31056809425354, lr: 5.9999999999999995e-05
Step 378, training loss: 5.179521560668945, total_norm: 2.5638716220855713, lr: 5.9999999999999995e-05
Step 379, training loss: 5.905203819274902, total_norm: 2.3320422172546387, lr: 5.9999999999999995e-05
Step 380, training loss: 6.141082286834717, total_norm: 2.355131149291992, lr: 5.9999999999999995e-05
Step 381, training loss: 5.729956150054932, total_norm: 2.866046667098999, lr: 5.9999999999999995e-05
Step 382, training loss: 6.018076419830322, total_norm: 2.119654893875122, lr: 5.9999999999999995e-05
Step 383, training loss: 5.749499797821045, total_norm: 2.504777669906616, lr: 5.9999999999999995e-05
Step 384, training loss: 5.709599494934082, total_norm: 2.222598075866699, lr: 5.9999999999999995e-05
Step 385, training loss: 5.800950050354004, total_norm: 2.734208822250366, lr: 5.9999999999999995e-05
Step 386, training loss: 5.546845436096191, total_norm: 2.7769577503204346, lr: 5.9999999999999995e-05
Step 387, training loss: 5.47249174118042, total_norm: 3.0388758182525635, lr: 5.9999999999999995e-05
Step 388, training loss: 5.733829021453857, total_norm: 2.647907018661499, lr: 5.9999999999999995e-05
Step 389, training loss: 5.409082889556885, total_norm: 2.0347704887390137, lr: 5.9999999999999995e-05
Step 390, training loss: 5.6483283042907715, total_norm: 1.995217204093933, lr: 5.9999999999999995e-05
Step 391, training loss: 5.682551860809326, total_norm: 2.147395133972168, lr: 5.9999999999999995e-05
Step 392, training loss: 5.499971389770508, total_norm: 2.609156608581543, lr: 5.9999999999999995e-05
Step 393, training loss: 5.388932704925537, total_norm: 2.134192943572998, lr: 5.9999999999999995e-05
Step 394, training loss: 5.693598747253418, total_norm: 2.4384331703186035, lr: 5.9999999999999995e-05
Step 395, training loss: 5.475893020629883, total_norm: 2.5096969604492188, lr: 5.9999999999999995e-05
Step 396, training loss: 5.332419395446777, total_norm: 2.155048370361328, lr: 5.9999999999999995e-05
Step 397, training loss: 5.608397006988525, total_norm: 1.893834114074707, lr: 5.9999999999999995e-05
Step 398, training loss: 5.330001354217529, total_norm: 2.0296409130096436, lr: 5.9999999999999995e-05
Step 399, training loss: 5.363747596740723, total_norm: 2.1832661628723145, lr: 5.9999999999999995e-05
Step 400, training loss: 5.640772819519043, total_norm: 1.9980642795562744, lr: 5.9999999999999995e-05
Step 400, validation loss: 6.193474292755127
Step 401, training loss: 5.758590221405029, total_norm: 1.8831992149353027, lr: 5.9999999999999995e-05
Step 402, training loss: 5.3513007164001465, total_norm: 2.2753021717071533, lr: 5.9999999999999995e-05
Step 403, training loss: 5.218266487121582, total_norm: 2.3057239055633545, lr: 5.9999999999999995e-05
Step 404, training loss: 5.435997486114502, total_norm: 1.851758360862732, lr: 5.9999999999999995e-05
Step 405, training loss: 5.619672775268555, total_norm: 2.171071767807007, lr: 5.9999999999999995e-05
Step 406, training loss: 5.581600666046143, total_norm: 1.7478976249694824, lr: 5.9999999999999995e-05
Step 407, training loss: 6.0510125160217285, total_norm: 2.318347930908203, lr: 5.9999999999999995e-05
Step 408, training loss: 5.602937698364258, total_norm: 2.128546714782715, lr: 5.9999999999999995e-05
Step 409, training loss: 5.336337089538574, total_norm: 1.8646162748336792, lr: 5.9999999999999995e-05
Step 410, training loss: 4.957282066345215, total_norm: 2.4568872451782227, lr: 5.9999999999999995e-05
Step 411, training loss: 4.9528117179870605, total_norm: 2.1253981590270996, lr: 5.9999999999999995e-05
Step 412, training loss: 5.5472493171691895, total_norm: 2.088824987411499, lr: 5.9999999999999995e-05
Step 413, training loss: 5.695679664611816, total_norm: 2.0785136222839355, lr: 5.9999999999999995e-05
Step 414, training loss: 5.978299617767334, total_norm: 2.1730051040649414, lr: 5.9999999999999995e-05
Step 415, training loss: 5.896862030029297, total_norm: 2.1741578578948975, lr: 5.9999999999999995e-05
Step 416, training loss: 6.632754802703857, total_norm: 2.2988972663879395, lr: 5.9999999999999995e-05
Step 417, training loss: 5.960436820983887, total_norm: 3.1525685787200928, lr: 5.9999999999999995e-05
Step 418, training loss: 6.093514919281006, total_norm: 2.997318744659424, lr: 5.9999999999999995e-05
Step 419, training loss: 6.031569480895996, total_norm: 2.8578391075134277, lr: 5.9999999999999995e-05
Step 420, training loss: 6.302027702331543, total_norm: 2.3325090408325195, lr: 5.9999999999999995e-05
Step 421, training loss: 5.85003662109375, total_norm: 2.515721321105957, lr: 5.9999999999999995e-05
Step 422, training loss: 6.2921142578125, total_norm: 2.4916536808013916, lr: 5.9999999999999995e-05
Step 423, training loss: 6.13851261138916, total_norm: 2.3606698513031006, lr: 5.9999999999999995e-05
Step 424, training loss: 6.4017767906188965, total_norm: 2.394315719604492, lr: 5.9999999999999995e-05
Step 425, training loss: 6.128811836242676, total_norm: 2.289592742919922, lr: 5.9999999999999995e-05
Step 426, training loss: 5.932632923126221, total_norm: 2.295989513397217, lr: 5.9999999999999995e-05
Step 427, training loss: 5.826560974121094, total_norm: 2.3683860301971436, lr: 5.9999999999999995e-05
Step 428, training loss: 6.078132629394531, total_norm: 2.4339582920074463, lr: 5.9999999999999995e-05
Step 429, training loss: 5.592100143432617, total_norm: 2.7693095207214355, lr: 5.9999999999999995e-05
Step 430, training loss: 5.821846961975098, total_norm: 2.385875701904297, lr: 5.9999999999999995e-05
Step 431, training loss: 6.07313346862793, total_norm: 2.4610912799835205, lr: 5.9999999999999995e-05
Step 432, training loss: 5.764290809631348, total_norm: 2.1090848445892334, lr: 5.9999999999999995e-05
Step 433, training loss: 5.789156436920166, total_norm: 2.251359701156616, lr: 5.9999999999999995e-05
Step 434, training loss: 5.8600873947143555, total_norm: 2.1693766117095947, lr: 5.9999999999999995e-05
Step 435, training loss: 6.106185436248779, total_norm: 2.153494119644165, lr: 5.9999999999999995e-05
Step 436, training loss: 5.702812671661377, total_norm: 2.056149959564209, lr: 5.9999999999999995e-05
Step 437, training loss: 5.62982702255249, total_norm: 2.156641721725464, lr: 5.9999999999999995e-05
Step 438, training loss: 5.6419267654418945, total_norm: 2.2284510135650635, lr: 5.9999999999999995e-05
Step 439, training loss: 5.766814708709717, total_norm: 2.53722882270813, lr: 5.9999999999999995e-05
Step 440, training loss: 5.5550103187561035, total_norm: 1.8679221868515015, lr: 5.9999999999999995e-05
Step 441, training loss: 5.989818572998047, total_norm: 2.1271471977233887, lr: 5.9999999999999995e-05
Step 442, training loss: 5.985012054443359, total_norm: 2.421722173690796, lr: 5.9999999999999995e-05
Step 443, training loss: 5.888766288757324, total_norm: 2.2883176803588867, lr: 5.9999999999999995e-05
Step 444, training loss: 5.832231044769287, total_norm: 1.9860748052597046, lr: 5.9999999999999995e-05
Step 445, training loss: 5.379695892333984, total_norm: 2.5015687942504883, lr: 5.9999999999999995e-05
Step 446, training loss: 6.365609645843506, total_norm: 2.112264394760132, lr: 5.9999999999999995e-05
Step 447, training loss: 5.8383402824401855, total_norm: 1.8639159202575684, lr: 5.9999999999999995e-05
Step 448, training loss: 5.645625114440918, total_norm: 1.8195616006851196, lr: 5.9999999999999995e-05
Step 449, training loss: 6.422189235687256, total_norm: 1.9580934047698975, lr: 5.9999999999999995e-05
Step 450, training loss: 6.52006721496582, total_norm: 2.616037130355835, lr: 5.9999999999999995e-05
Step 450, validation loss: 6.137326240539551
Step 451, training loss: 6.0089311599731445, total_norm: 2.558598279953003, lr: 5.9999999999999995e-05
Step 452, training loss: 6.1731367111206055, total_norm: 1.7807719707489014, lr: 5.9999999999999995e-05
Step 453, training loss: 6.38419246673584, total_norm: 2.0330772399902344, lr: 5.9999999999999995e-05
Step 454, training loss: 6.4780988693237305, total_norm: 2.1199729442596436, lr: 5.9999999999999995e-05
Step 455, training loss: 6.246047019958496, total_norm: 2.26501727104187, lr: 5.9999999999999995e-05
Step 456, training loss: 6.034745216369629, total_norm: 2.566237211227417, lr: 5.9999999999999995e-05
Step 457, training loss: 5.949599742889404, total_norm: 2.0778861045837402, lr: 5.9999999999999995e-05
Step 458, training loss: 6.09470796585083, total_norm: 2.2478978633880615, lr: 5.9999999999999995e-05
Step 459, training loss: 6.041534900665283, total_norm: 2.2375850677490234, lr: 5.9999999999999995e-05
Step 460, training loss: 6.236241817474365, total_norm: 1.8389933109283447, lr: 5.9999999999999995e-05
Step 461, training loss: 5.656368732452393, total_norm: 1.873649001121521, lr: 5.9999999999999995e-05
Step 462, training loss: 6.428335189819336, total_norm: 2.400085687637329, lr: 5.9999999999999995e-05
Step 463, training loss: 6.317948818206787, total_norm: 2.389225482940674, lr: 5.9999999999999995e-05
Step 464, training loss: 5.974784851074219, total_norm: 2.345288038253784, lr: 5.9999999999999995e-05
Step 465, training loss: 5.654777526855469, total_norm: 2.7202935218811035, lr: 5.9999999999999995e-05
Step 466, training loss: 6.068469047546387, total_norm: 2.341132164001465, lr: 5.9999999999999995e-05
Step 467, training loss: 5.324499607086182, total_norm: 2.430419921875, lr: 5.9999999999999995e-05
Step 468, training loss: 5.551654815673828, total_norm: 2.6602420806884766, lr: 5.9999999999999995e-05
Step 469, training loss: 5.873876094818115, total_norm: 1.812608003616333, lr: 5.9999999999999995e-05
Step 470, training loss: 5.689446449279785, total_norm: 2.0319602489471436, lr: 5.9999999999999995e-05
Step 471, training loss: 5.6595282554626465, total_norm: 2.0613012313842773, lr: 5.9999999999999995e-05
Step 472, training loss: 5.842780590057373, total_norm: 2.134683609008789, lr: 5.9999999999999995e-05
Step 473, training loss: 6.324936866760254, total_norm: 2.047708511352539, lr: 5.9999999999999995e-05
Step 474, training loss: 5.6864519119262695, total_norm: 2.5640528202056885, lr: 5.9999999999999995e-05
Step 475, training loss: 5.731269836425781, total_norm: 2.2011144161224365, lr: 5.9999999999999995e-05
Step 476, training loss: 5.490844249725342, total_norm: 2.6914944648742676, lr: 5.9999999999999995e-05
Step 477, training loss: 5.610626697540283, total_norm: 2.4777345657348633, lr: 5.9999999999999995e-05
Step 478, training loss: 6.278639793395996, total_norm: 2.1591200828552246, lr: 5.9999999999999995e-05
Step 479, training loss: 5.557872295379639, total_norm: 1.9906619787216187, lr: 5.9999999999999995e-05
Step 480, training loss: 5.62082052230835, total_norm: 2.2785916328430176, lr: 5.9999999999999995e-05
Step 481, training loss: 5.534598350524902, total_norm: 3.2605762481689453, lr: 5.9999999999999995e-05
Step 482, training loss: 5.5358567237854, total_norm: 2.6502280235290527, lr: 5.9999999999999995e-05
Step 483, training loss: 5.8768229484558105, total_norm: 1.9939448833465576, lr: 5.9999999999999995e-05
Step 484, training loss: 5.764273166656494, total_norm: 2.1214287281036377, lr: 5.9999999999999995e-05
Step 485, training loss: 5.4427080154418945, total_norm: 2.000044584274292, lr: 5.9999999999999995e-05
Step 486, training loss: 6.150955677032471, total_norm: 2.16699481010437, lr: 5.9999999999999995e-05
Step 487, training loss: 6.52225399017334, total_norm: 2.1758947372436523, lr: 5.9999999999999995e-05
Step 488, training loss: 6.359739780426025, total_norm: 2.0326128005981445, lr: 5.9999999999999995e-05
Step 489, training loss: 5.531406879425049, total_norm: 2.294987440109253, lr: 5.9999999999999995e-05
Step 490, training loss: 5.284899711608887, total_norm: 2.234266757965088, lr: 5.9999999999999995e-05
Step 491, training loss: 5.608029365539551, total_norm: 2.2295327186584473, lr: 5.9999999999999995e-05
Step 492, training loss: 5.429519176483154, total_norm: 2.127558469772339, lr: 5.9999999999999995e-05
Step 493, training loss: 5.555970668792725, total_norm: 2.0316436290740967, lr: 5.9999999999999995e-05
Step 494, training loss: 5.904472351074219, total_norm: 2.0180444717407227, lr: 5.9999999999999995e-05
Step 495, training loss: 6.079764366149902, total_norm: 2.1499674320220947, lr: 5.9999999999999995e-05
Step 496, training loss: 5.6778998374938965, total_norm: 2.40499210357666, lr: 5.9999999999999995e-05
Step 497, training loss: 5.729920864105225, total_norm: 3.1902432441711426, lr: 5.9999999999999995e-05
Step 498, training loss: 5.8737640380859375, total_norm: 2.8994593620300293, lr: 5.9999999999999995e-05
Step 499, training loss: 6.07372522354126, total_norm: 3.054241895675659, lr: 5.9999999999999995e-05
Step 500, training loss: 5.991367340087891, total_norm: 2.75203013420105, lr: 5.9999999999999995e-05
Step 500, validation loss: 6.020931720733643
Step 501, training loss: 6.090867519378662, total_norm: 2.801100969314575, lr: 5.9999999999999995e-05
Step 502, training loss: 6.15770959854126, total_norm: 2.9848034381866455, lr: 5.9999999999999995e-05
Step 503, training loss: 5.880659580230713, total_norm: 2.799558401107788, lr: 5.9999999999999995e-05
Step 504, training loss: 6.114885330200195, total_norm: 2.9252266883850098, lr: 5.9999999999999995e-05
Step 505, training loss: 5.688020706176758, total_norm: 2.121622085571289, lr: 5.9999999999999995e-05
Step 506, training loss: 5.668780326843262, total_norm: 2.3847808837890625, lr: 5.9999999999999995e-05
Step 507, training loss: 5.4616875648498535, total_norm: 2.9817185401916504, lr: 5.9999999999999995e-05
Step 508, training loss: 5.649816989898682, total_norm: 2.9439401626586914, lr: 5.9999999999999995e-05
Step 509, training loss: 5.209153175354004, total_norm: 2.6719439029693604, lr: 5.9999999999999995e-05
Step 510, training loss: 5.7751641273498535, total_norm: 2.5377485752105713, lr: 5.9999999999999995e-05
Step 511, training loss: 5.727818012237549, total_norm: 2.6745479106903076, lr: 5.9999999999999995e-05
Step 512, training loss: 5.446775913238525, total_norm: 2.2030601501464844, lr: 5.9999999999999995e-05
Step 513, training loss: 5.196249485015869, total_norm: 2.315561294555664, lr: 5.9999999999999995e-05
Step 514, training loss: 5.094560623168945, total_norm: 2.870374917984009, lr: 5.9999999999999995e-05
Step 515, training loss: 5.378594398498535, total_norm: 2.3018205165863037, lr: 5.9999999999999995e-05
Step 516, training loss: 5.805077075958252, total_norm: 2.714280128479004, lr: 5.9999999999999995e-05
Step 517, training loss: 5.743773460388184, total_norm: 2.4404585361480713, lr: 5.9999999999999995e-05
Step 518, training loss: 5.514545440673828, total_norm: 2.5838401317596436, lr: 5.9999999999999995e-05
Step 519, training loss: 5.76611328125, total_norm: 2.4890854358673096, lr: 5.9999999999999995e-05
Step 520, training loss: 5.450737476348877, total_norm: 3.329298257827759, lr: 5.9999999999999995e-05
Step 521, training loss: 5.295612812042236, total_norm: 2.7487096786499023, lr: 5.9999999999999995e-05
Step 522, training loss: 5.3977370262146, total_norm: 2.6712207794189453, lr: 5.9999999999999995e-05
Step 523, training loss: 5.631927967071533, total_norm: 2.165614604949951, lr: 5.9999999999999995e-05
Step 524, training loss: 6.136632919311523, total_norm: 2.4811503887176514, lr: 5.9999999999999995e-05
Step 525, training loss: 5.487000942230225, total_norm: 2.7687458992004395, lr: 5.9999999999999995e-05
Step 526, training loss: 5.738962173461914, total_norm: 2.617039918899536, lr: 5.9999999999999995e-05
Step 527, training loss: 5.755648136138916, total_norm: 2.395963430404663, lr: 5.9999999999999995e-05
Step 528, training loss: 5.80342435836792, total_norm: 2.8163156509399414, lr: 5.9999999999999995e-05
Step 529, training loss: 6.119328498840332, total_norm: 2.619802713394165, lr: 5.9999999999999995e-05
Step 530, training loss: 5.674917221069336, total_norm: 2.8674497604370117, lr: 5.9999999999999995e-05
Step 531, training loss: 5.634483337402344, total_norm: 2.4518256187438965, lr: 5.9999999999999995e-05
Step 532, training loss: 6.0496063232421875, total_norm: 2.187211513519287, lr: 5.9999999999999995e-05
Step 533, training loss: 5.7861199378967285, total_norm: 2.5527400970458984, lr: 5.9999999999999995e-05
Step 534, training loss: 5.618813514709473, total_norm: 3.333348035812378, lr: 5.9999999999999995e-05
Step 535, training loss: 5.811423301696777, total_norm: 2.5700793266296387, lr: 5.9999999999999995e-05
Step 536, training loss: 5.917969226837158, total_norm: 2.7679443359375, lr: 5.9999999999999995e-05
Step 537, training loss: 5.929166793823242, total_norm: 3.5424981117248535, lr: 5.9999999999999995e-05
Step 538, training loss: 5.671213626861572, total_norm: 3.003312587738037, lr: 5.9999999999999995e-05
Step 539, training loss: 5.358539581298828, total_norm: 3.158742904663086, lr: 5.9999999999999995e-05
Step 540, training loss: 5.748665809631348, total_norm: 2.588153123855591, lr: 5.9999999999999995e-05
Step 541, training loss: 5.3951544761657715, total_norm: 2.340683698654175, lr: 5.9999999999999995e-05
Step 542, training loss: 5.614758014678955, total_norm: 2.144402027130127, lr: 5.9999999999999995e-05
Step 543, training loss: 5.272951602935791, total_norm: 2.4708149433135986, lr: 5.9999999999999995e-05
Step 544, training loss: 5.489534378051758, total_norm: 1.802785873413086, lr: 5.9999999999999995e-05
Step 545, training loss: 5.602904319763184, total_norm: 2.1972901821136475, lr: 5.9999999999999995e-05
Step 546, training loss: 5.625123500823975, total_norm: 1.9574949741363525, lr: 5.9999999999999995e-05
Step 547, training loss: 5.204306125640869, total_norm: 2.2366175651550293, lr: 5.9999999999999995e-05
Step 548, training loss: 5.27097225189209, total_norm: 1.9063860177993774, lr: 5.9999999999999995e-05
Step 549, training loss: 5.069386005401611, total_norm: 2.3417491912841797, lr: 5.9999999999999995e-05
Step 550, training loss: 6.257960319519043, total_norm: 2.4426724910736084, lr: 5.9999999999999995e-05
Step 550, validation loss: 6.0067901611328125
Step 551, training loss: 5.781611919403076, total_norm: 2.3944430351257324, lr: 5.9999999999999995e-05
Step 552, training loss: 5.414179801940918, total_norm: 2.4326205253601074, lr: 5.9999999999999995e-05
Step 553, training loss: 5.352686405181885, total_norm: 2.491516351699829, lr: 5.9999999999999995e-05
Step 554, training loss: 5.814363479614258, total_norm: 2.126600742340088, lr: 5.9999999999999995e-05
Step 555, training loss: 5.4269490242004395, total_norm: 2.5391910076141357, lr: 5.9999999999999995e-05
Step 556, training loss: 5.811964511871338, total_norm: 2.3665359020233154, lr: 5.9999999999999995e-05
Step 557, training loss: 5.263860702514648, total_norm: 2.50301194190979, lr: 5.9999999999999995e-05
Step 558, training loss: 5.919392108917236, total_norm: 1.9905476570129395, lr: 5.9999999999999995e-05
Step 559, training loss: 5.139621734619141, total_norm: 2.0123982429504395, lr: 5.9999999999999995e-05
Step 560, training loss: 5.411736965179443, total_norm: 1.8930341005325317, lr: 5.9999999999999995e-05
Step 561, training loss: 5.266702175140381, total_norm: 1.8761917352676392, lr: 5.9999999999999995e-05
Step 562, training loss: 4.92365026473999, total_norm: 2.4866020679473877, lr: 5.9999999999999995e-05
Step 563, training loss: 5.685848712921143, total_norm: 2.4208784103393555, lr: 5.9999999999999995e-05
Step 564, training loss: 4.683485507965088, total_norm: 2.2887134552001953, lr: 5.9999999999999995e-05
Step 565, training loss: 5.525538444519043, total_norm: 2.330110788345337, lr: 5.9999999999999995e-05
Step 566, training loss: 5.703810691833496, total_norm: 2.0573577880859375, lr: 5.9999999999999995e-05
Step 567, training loss: 4.9155473709106445, total_norm: 2.3379313945770264, lr: 5.9999999999999995e-05
Step 568, training loss: 4.523995876312256, total_norm: 2.914369821548462, lr: 5.9999999999999995e-05
Step 569, training loss: 4.971004962921143, total_norm: 2.4626293182373047, lr: 5.9999999999999995e-05
Step 570, training loss: 5.382481575012207, total_norm: 2.0990476608276367, lr: 5.9999999999999995e-05
Step 571, training loss: 5.694517612457275, total_norm: 2.0118517875671387, lr: 5.9999999999999995e-05
Step 572, training loss: 5.037979602813721, total_norm: 2.073759078979492, lr: 5.9999999999999995e-05
Step 573, training loss: 4.640021324157715, total_norm: 2.103795051574707, lr: 5.9999999999999995e-05
Step 574, training loss: 4.655759334564209, total_norm: 1.867828607559204, lr: 5.9999999999999995e-05
Step 575, training loss: 4.52626895904541, total_norm: 2.103469133377075, lr: 5.9999999999999995e-05
Step 576, training loss: 5.127992630004883, total_norm: 2.140249729156494, lr: 5.9999999999999995e-05
Step 577, training loss: 5.036648273468018, total_norm: 1.8673529624938965, lr: 5.9999999999999995e-05
Step 578, training loss: 4.556402683258057, total_norm: 1.7942233085632324, lr: 5.9999999999999995e-05
Step 579, training loss: 5.461434364318848, total_norm: 1.7328035831451416, lr: 5.9999999999999995e-05
Step 580, training loss: 5.250588417053223, total_norm: 1.8549879789352417, lr: 5.9999999999999995e-05
Step 581, training loss: 4.90528678894043, total_norm: 1.854670763015747, lr: 5.9999999999999995e-05
Step 582, training loss: 4.747865200042725, total_norm: 1.9338691234588623, lr: 5.9999999999999995e-05
Step 583, training loss: 4.85036563873291, total_norm: 2.0423965454101562, lr: 5.9999999999999995e-05
Step 584, training loss: 4.614855766296387, total_norm: 2.0700700283050537, lr: 5.9999999999999995e-05
Step 585, training loss: 5.417217254638672, total_norm: 1.9749771356582642, lr: 5.9999999999999995e-05
Step 586, training loss: 5.602291107177734, total_norm: 2.1280627250671387, lr: 5.9999999999999995e-05
Step 587, training loss: 4.5348100662231445, total_norm: 2.586414098739624, lr: 5.9999999999999995e-05
Step 588, training loss: 4.579854488372803, total_norm: 2.1281681060791016, lr: 5.9999999999999995e-05
Step 589, training loss: 5.820475101470947, total_norm: 2.5443689823150635, lr: 5.9999999999999995e-05
Step 590, training loss: 5.29311990737915, total_norm: 1.9811030626296997, lr: 5.9999999999999995e-05
Step 591, training loss: 4.64603328704834, total_norm: 3.018934726715088, lr: 5.9999999999999995e-05
Step 592, training loss: 4.899719715118408, total_norm: 3.2219698429107666, lr: 5.9999999999999995e-05
Step 593, training loss: 5.927786827087402, total_norm: 2.4022457599639893, lr: 5.9999999999999995e-05
Step 594, training loss: 5.781167030334473, total_norm: 2.0299527645111084, lr: 5.9999999999999995e-05
Step 595, training loss: 5.095186233520508, total_norm: 2.6091370582580566, lr: 5.9999999999999995e-05
Step 596, training loss: 5.640977382659912, total_norm: 2.156435489654541, lr: 5.9999999999999995e-05
Step 597, training loss: 4.936501979827881, total_norm: 2.1245510578155518, lr: 5.9999999999999995e-05
Step 598, training loss: 4.9106574058532715, total_norm: 2.439736843109131, lr: 5.9999999999999995e-05
Step 599, training loss: 4.7603535652160645, total_norm: 2.0045857429504395, lr: 5.9999999999999995e-05
Step 600, training loss: 4.989839553833008, total_norm: 2.4162323474884033, lr: 5.9999999999999995e-05
Step 600, validation loss: 6.015751838684082
Step 601, training loss: 4.796289443969727, total_norm: 2.0466372966766357, lr: 5.9999999999999995e-05
Step 602, training loss: 5.3765082359313965, total_norm: 1.9015544652938843, lr: 5.9999999999999995e-05
Step 603, training loss: 5.3046875, total_norm: 2.434379816055298, lr: 5.9999999999999995e-05
Step 604, training loss: 4.897276401519775, total_norm: 2.2030205726623535, lr: 5.9999999999999995e-05
Step 605, training loss: 4.521523475646973, total_norm: 2.056417942047119, lr: 5.9999999999999995e-05
Step 606, training loss: 5.407407283782959, total_norm: 2.2213425636291504, lr: 5.9999999999999995e-05
Step 607, training loss: 5.592533588409424, total_norm: 2.3820207118988037, lr: 5.9999999999999995e-05
Step 608, training loss: 5.144501209259033, total_norm: 1.8191334009170532, lr: 5.9999999999999995e-05
Step 609, training loss: 5.645933151245117, total_norm: 1.8996376991271973, lr: 5.9999999999999995e-05
Step 610, training loss: 5.212827205657959, total_norm: 2.2044484615325928, lr: 5.9999999999999995e-05
Step 611, training loss: 5.6476359367370605, total_norm: 2.290334701538086, lr: 5.9999999999999995e-05
Step 612, training loss: 5.827672958374023, total_norm: 2.0652060508728027, lr: 5.9999999999999995e-05
Step 613, training loss: 4.925292491912842, total_norm: 1.8881810903549194, lr: 5.9999999999999995e-05
Step 614, training loss: 5.341444492340088, total_norm: 2.1251442432403564, lr: 5.9999999999999995e-05
Step 615, training loss: 5.3252081871032715, total_norm: 2.004962205886841, lr: 5.9999999999999995e-05
Step 616, training loss: 5.356487274169922, total_norm: 2.1170527935028076, lr: 5.9999999999999995e-05
Step 617, training loss: 5.403936386108398, total_norm: 2.1313304901123047, lr: 5.9999999999999995e-05
Step 618, training loss: 5.132671356201172, total_norm: 2.6867361068725586, lr: 5.9999999999999995e-05
Step 619, training loss: 5.355233192443848, total_norm: 2.3408803939819336, lr: 5.9999999999999995e-05
Step 620, training loss: 6.289147853851318, total_norm: 2.294984817504883, lr: 5.9999999999999995e-05
Step 621, training loss: 5.803994655609131, total_norm: 2.129239559173584, lr: 5.9999999999999995e-05
Step 622, training loss: 5.810826301574707, total_norm: 2.3658065795898438, lr: 5.9999999999999995e-05
Step 623, training loss: 5.55554723739624, total_norm: 2.215298652648926, lr: 5.9999999999999995e-05
Step 624, training loss: 6.048488140106201, total_norm: 2.3110852241516113, lr: 5.9999999999999995e-05
Step 625, training loss: 5.952354907989502, total_norm: 2.4376208782196045, lr: 5.9999999999999995e-05
Step 626, training loss: 5.683361530303955, total_norm: 3.0743112564086914, lr: 5.9999999999999995e-05
Step 627, training loss: 5.3318705558776855, total_norm: 3.036752223968506, lr: 5.9999999999999995e-05
Step 628, training loss: 5.544296741485596, total_norm: 2.9380667209625244, lr: 5.9999999999999995e-05
Step 629, training loss: 5.190547943115234, total_norm: 2.6965293884277344, lr: 5.9999999999999995e-05
Step 630, training loss: 5.513251781463623, total_norm: 2.404906749725342, lr: 5.9999999999999995e-05
Step 631, training loss: 5.626401424407959, total_norm: 1.957442283630371, lr: 5.9999999999999995e-05
Step 632, training loss: 5.64345121383667, total_norm: 2.2470078468322754, lr: 5.9999999999999995e-05
Step 633, training loss: 5.687551021575928, total_norm: 2.2935993671417236, lr: 5.9999999999999995e-05
Step 634, training loss: 5.318478107452393, total_norm: 2.1991565227508545, lr: 5.9999999999999995e-05
Step 635, training loss: 5.126168251037598, total_norm: 2.187741279602051, lr: 5.9999999999999995e-05
Step 636, training loss: 5.369877338409424, total_norm: 2.0851447582244873, lr: 5.9999999999999995e-05
Step 637, training loss: 5.520510196685791, total_norm: 2.388357400894165, lr: 5.9999999999999995e-05
Step 638, training loss: 5.229123592376709, total_norm: 2.256314277648926, lr: 5.9999999999999995e-05
Step 639, training loss: 5.265616416931152, total_norm: 2.1854968070983887, lr: 5.9999999999999995e-05
Step 640, training loss: 5.147701263427734, total_norm: 2.4421446323394775, lr: 5.9999999999999995e-05
Step 641, training loss: 5.425852298736572, total_norm: 2.217090129852295, lr: 5.9999999999999995e-05
Step 642, training loss: 6.001197338104248, total_norm: 2.405777931213379, lr: 5.9999999999999995e-05
Step 643, training loss: 5.423928260803223, total_norm: 2.6577916145324707, lr: 5.9999999999999995e-05
Step 644, training loss: 5.6238694190979, total_norm: 3.868576765060425, lr: 5.9999999999999995e-05
Step 645, training loss: 4.956848621368408, total_norm: 2.7481870651245117, lr: 5.9999999999999995e-05
Step 646, training loss: 5.355212211608887, total_norm: 1.9613529443740845, lr: 5.9999999999999995e-05
Step 647, training loss: 4.861570835113525, total_norm: 2.2390713691711426, lr: 5.9999999999999995e-05
Step 648, training loss: 5.142786502838135, total_norm: 2.073678970336914, lr: 5.9999999999999995e-05
Step 649, training loss: 5.307431221008301, total_norm: 2.4759528636932373, lr: 5.9999999999999995e-05
Step 650, training loss: 5.173152923583984, total_norm: 2.397618293762207, lr: 5.9999999999999995e-05
Step 650, validation loss: 5.94935417175293
Step 651, training loss: 5.418778419494629, total_norm: 2.629502058029175, lr: 5.9999999999999995e-05
Step 652, training loss: 5.151226997375488, total_norm: 2.1258938312530518, lr: 5.9999999999999995e-05
Step 653, training loss: 5.322871208190918, total_norm: 2.015692710876465, lr: 5.9999999999999995e-05
Step 654, training loss: 5.494370460510254, total_norm: 2.384298086166382, lr: 5.9999999999999995e-05
Step 655, training loss: 5.344785213470459, total_norm: 2.0173563957214355, lr: 5.9999999999999995e-05
Step 656, training loss: 4.902291297912598, total_norm: 2.0268800258636475, lr: 5.9999999999999995e-05
Step 657, training loss: 5.263363838195801, total_norm: 2.055671453475952, lr: 5.9999999999999995e-05
Step 658, training loss: 5.198359966278076, total_norm: 2.436892509460449, lr: 5.9999999999999995e-05
Step 659, training loss: 5.10220193862915, total_norm: 2.2874603271484375, lr: 5.9999999999999995e-05
Step 660, training loss: 5.120288848876953, total_norm: 2.5392627716064453, lr: 5.9999999999999995e-05
Step 661, training loss: 5.345101356506348, total_norm: 2.467928171157837, lr: 5.9999999999999995e-05
Step 662, training loss: 4.707941055297852, total_norm: 2.719045400619507, lr: 5.9999999999999995e-05
Step 663, training loss: 4.464430809020996, total_norm: 2.451878070831299, lr: 5.9999999999999995e-05
Step 664, training loss: 5.124261856079102, total_norm: 2.1511831283569336, lr: 5.9999999999999995e-05
Step 665, training loss: 4.933534145355225, total_norm: 2.5163261890411377, lr: 5.9999999999999995e-05
Step 666, training loss: 5.287808895111084, total_norm: 2.1623964309692383, lr: 5.9999999999999995e-05
Step 667, training loss: 5.10399055480957, total_norm: 2.6580870151519775, lr: 5.9999999999999995e-05
Step 668, training loss: 4.879759788513184, total_norm: 2.1635429859161377, lr: 5.9999999999999995e-05
Step 669, training loss: 5.161820888519287, total_norm: 2.784207344055176, lr: 5.9999999999999995e-05
Step 670, training loss: 4.724216461181641, total_norm: 2.4921743869781494, lr: 5.9999999999999995e-05
Step 671, training loss: 5.254152774810791, total_norm: 2.107069969177246, lr: 5.9999999999999995e-05
Step 672, training loss: 5.329849720001221, total_norm: 2.255920648574829, lr: 5.9999999999999995e-05
Step 673, training loss: 5.0213847160339355, total_norm: 2.2095539569854736, lr: 5.9999999999999995e-05
Step 674, training loss: 5.366848468780518, total_norm: 2.155073881149292, lr: 5.9999999999999995e-05
Step 675, training loss: 5.484675884246826, total_norm: 1.9382948875427246, lr: 5.9999999999999995e-05
Step 676, training loss: 5.384984493255615, total_norm: 2.123544216156006, lr: 5.9999999999999995e-05
Step 677, training loss: 5.0741987228393555, total_norm: 2.074713706970215, lr: 5.9999999999999995e-05
Step 678, training loss: 5.349759578704834, total_norm: 1.9430009126663208, lr: 5.9999999999999995e-05
Step 679, training loss: 5.413973331451416, total_norm: 2.122192621231079, lr: 5.9999999999999995e-05
Step 680, training loss: 5.25543737411499, total_norm: 2.0297980308532715, lr: 5.9999999999999995e-05
Step 681, training loss: 4.9075517654418945, total_norm: 2.1468093395233154, lr: 5.9999999999999995e-05
Step 682, training loss: 5.036836624145508, total_norm: 2.530688762664795, lr: 5.9999999999999995e-05
Step 683, training loss: 5.3874101638793945, total_norm: 2.262078285217285, lr: 5.9999999999999995e-05
Step 684, training loss: 5.064306735992432, total_norm: 2.73801326751709, lr: 5.9999999999999995e-05
Step 685, training loss: 5.389214515686035, total_norm: 2.726522445678711, lr: 5.9999999999999995e-05
Step 686, training loss: 5.175529479980469, total_norm: 2.7030396461486816, lr: 5.9999999999999995e-05
Step 687, training loss: 4.725535869598389, total_norm: 3.0018997192382812, lr: 5.9999999999999995e-05
Step 688, training loss: 5.721111297607422, total_norm: 2.335379123687744, lr: 5.9999999999999995e-05
Step 689, training loss: 5.6472296714782715, total_norm: 2.905294179916382, lr: 5.9999999999999995e-05
Step 690, training loss: 5.442448139190674, total_norm: 2.9261739253997803, lr: 5.9999999999999995e-05
Step 691, training loss: 5.551219940185547, total_norm: 2.357231378555298, lr: 5.9999999999999995e-05
Step 692, training loss: 5.5895609855651855, total_norm: 2.463447332382202, lr: 5.9999999999999995e-05
Step 693, training loss: 4.828329563140869, total_norm: 2.916504383087158, lr: 5.9999999999999995e-05
Step 694, training loss: 5.191263675689697, total_norm: 2.538832664489746, lr: 5.9999999999999995e-05
Step 695, training loss: 5.07500696182251, total_norm: 3.262812376022339, lr: 5.9999999999999995e-05
Step 696, training loss: 4.625929355621338, total_norm: 2.7425246238708496, lr: 5.9999999999999995e-05
Step 697, training loss: 4.834684371948242, total_norm: 2.562079668045044, lr: 5.9999999999999995e-05
Step 698, training loss: 5.661110877990723, total_norm: 2.189814329147339, lr: 5.9999999999999995e-05
Step 699, training loss: 4.199371337890625, total_norm: 3.651644468307495, lr: 5.9999999999999995e-05
Step 700, training loss: 4.932666778564453, total_norm: 2.747527599334717, lr: 5.9999999999999995e-05
Step 700, validation loss: 5.975422382354736
Step 701, training loss: 4.932963848114014, total_norm: 2.4852161407470703, lr: 5.9999999999999995e-05
Step 702, training loss: 4.871036529541016, total_norm: 2.361618757247925, lr: 5.9999999999999995e-05
Step 703, training loss: 4.9850358963012695, total_norm: 2.3397321701049805, lr: 5.9999999999999995e-05
Step 704, training loss: 5.442287921905518, total_norm: 2.013720750808716, lr: 5.9999999999999995e-05
Step 705, training loss: 5.696785926818848, total_norm: 2.073482036590576, lr: 5.9999999999999995e-05
Step 706, training loss: 5.5121636390686035, total_norm: 2.66074538230896, lr: 5.9999999999999995e-05
Step 707, training loss: 5.5457024574279785, total_norm: 2.6044046878814697, lr: 5.9999999999999995e-05
Step 708, training loss: 5.029449462890625, total_norm: 2.5761682987213135, lr: 5.9999999999999995e-05
Step 709, training loss: 5.612592697143555, total_norm: 2.6644411087036133, lr: 5.9999999999999995e-05
Step 710, training loss: 5.425153732299805, total_norm: 2.629770517349243, lr: 5.9999999999999995e-05
Step 711, training loss: 5.454946994781494, total_norm: 2.085663318634033, lr: 5.9999999999999995e-05
Step 712, training loss: 5.15421199798584, total_norm: 2.569744825363159, lr: 5.9999999999999995e-05
Step 713, training loss: 5.210275173187256, total_norm: 2.449723482131958, lr: 5.9999999999999995e-05
Step 714, training loss: 5.626003265380859, total_norm: 2.348463773727417, lr: 5.9999999999999995e-05
Step 715, training loss: 5.569565773010254, total_norm: 2.3059098720550537, lr: 5.9999999999999995e-05
Step 716, training loss: 5.730853080749512, total_norm: 2.089517831802368, lr: 5.9999999999999995e-05
Step 717, training loss: 5.709360122680664, total_norm: 2.2344207763671875, lr: 5.9999999999999995e-05
Step 718, training loss: 5.566477298736572, total_norm: 2.3796393871307373, lr: 5.9999999999999995e-05
Step 719, training loss: 5.769317626953125, total_norm: 2.1045970916748047, lr: 5.9999999999999995e-05
Step 720, training loss: 5.736771106719971, total_norm: 2.320265769958496, lr: 5.9999999999999995e-05
Step 721, training loss: 5.785348892211914, total_norm: 2.657122850418091, lr: 5.9999999999999995e-05
Step 722, training loss: 5.409064769744873, total_norm: 2.3528270721435547, lr: 5.9999999999999995e-05
Step 723, training loss: 5.689759731292725, total_norm: 2.287379026412964, lr: 5.9999999999999995e-05
Step 724, training loss: 5.650478839874268, total_norm: 2.2897119522094727, lr: 5.9999999999999995e-05
Step 725, training loss: 5.958772659301758, total_norm: 2.0279183387756348, lr: 5.9999999999999995e-05
Step 726, training loss: 5.549930095672607, total_norm: 2.1312177181243896, lr: 5.9999999999999995e-05
Step 727, training loss: 5.476954936981201, total_norm: 2.127892017364502, lr: 5.9999999999999995e-05
Step 728, training loss: 5.320989608764648, total_norm: 2.786288261413574, lr: 5.9999999999999995e-05
Step 729, training loss: 5.525083541870117, total_norm: 2.1762773990631104, lr: 5.9999999999999995e-05
Step 730, training loss: 5.675040245056152, total_norm: 2.0593974590301514, lr: 5.9999999999999995e-05
Step 731, training loss: 5.6041388511657715, total_norm: 2.05534291267395, lr: 5.9999999999999995e-05
Step 732, training loss: 6.064533233642578, total_norm: 2.751448631286621, lr: 5.9999999999999995e-05
Step 733, training loss: 5.340951442718506, total_norm: 2.326475143432617, lr: 5.9999999999999995e-05
Step 734, training loss: 5.38469934463501, total_norm: 2.0764944553375244, lr: 5.9999999999999995e-05
Step 735, training loss: 5.382020950317383, total_norm: 2.0815608501434326, lr: 5.9999999999999995e-05
Step 736, training loss: 5.416540145874023, total_norm: 2.2869486808776855, lr: 5.9999999999999995e-05
Step 737, training loss: 5.509557247161865, total_norm: 2.7345356941223145, lr: 5.9999999999999995e-05
Step 738, training loss: 5.810532093048096, total_norm: 2.2633895874023438, lr: 5.9999999999999995e-05
Step 739, training loss: 5.638023853302002, total_norm: 2.7120540142059326, lr: 5.9999999999999995e-05
Step 740, training loss: 5.360691547393799, total_norm: 2.7588584423065186, lr: 5.9999999999999995e-05
Step 741, training loss: 5.237636566162109, total_norm: 2.144970417022705, lr: 5.9999999999999995e-05
Step 742, training loss: 5.490860462188721, total_norm: 2.673516273498535, lr: 5.9999999999999995e-05
Step 743, training loss: 5.23571252822876, total_norm: 2.7669482231140137, lr: 5.9999999999999995e-05
Step 744, training loss: 5.164507865905762, total_norm: 2.8289449214935303, lr: 5.9999999999999995e-05
Step 745, training loss: 5.155534267425537, total_norm: 2.5499632358551025, lr: 5.9999999999999995e-05
Step 746, training loss: 5.310631275177002, total_norm: 2.2392680644989014, lr: 5.9999999999999995e-05
Step 747, training loss: 5.525097370147705, total_norm: 2.179716110229492, lr: 5.9999999999999995e-05
Step 748, training loss: 5.356468200683594, total_norm: 2.2620136737823486, lr: 5.9999999999999995e-05
Step 749, training loss: 5.55387544631958, total_norm: 2.4462084770202637, lr: 5.9999999999999995e-05
Step 750, training loss: 5.77511739730835, total_norm: 2.5465431213378906, lr: 5.9999999999999995e-05
Step 750, validation loss: 5.915713787078857
Step 751, training loss: 5.736665725708008, total_norm: 2.2240195274353027, lr: 5.9999999999999995e-05
Step 752, training loss: 5.715824604034424, total_norm: 2.5171921253204346, lr: 5.9999999999999995e-05
Step 753, training loss: 5.66880464553833, total_norm: 2.262273073196411, lr: 5.9999999999999995e-05
Step 754, training loss: 5.0790019035339355, total_norm: 2.1127560138702393, lr: 5.9999999999999995e-05
Step 755, training loss: 5.393219947814941, total_norm: 2.5875353813171387, lr: 5.9999999999999995e-05
Step 756, training loss: 5.705958366394043, total_norm: 2.2473950386047363, lr: 5.9999999999999995e-05
Step 757, training loss: 5.379850387573242, total_norm: 2.1787168979644775, lr: 5.9999999999999995e-05
Step 758, training loss: 5.59328031539917, total_norm: 2.2285654544830322, lr: 5.9999999999999995e-05
Step 759, training loss: 4.649265766143799, total_norm: 3.024980068206787, lr: 5.9999999999999995e-05
Step 760, training loss: 5.421638011932373, total_norm: 2.5760583877563477, lr: 5.9999999999999995e-05
Step 761, training loss: 5.7301764488220215, total_norm: 2.735811948776245, lr: 5.9999999999999995e-05
Step 762, training loss: 5.248825550079346, total_norm: 2.5358211994171143, lr: 5.9999999999999995e-05
Step 763, training loss: 5.055828094482422, total_norm: 2.750498056411743, lr: 5.9999999999999995e-05
Step 764, training loss: 5.308437824249268, total_norm: 2.346501588821411, lr: 5.9999999999999995e-05
Step 765, training loss: 5.407424449920654, total_norm: 2.426661729812622, lr: 5.9999999999999995e-05
Step 766, training loss: 5.276086330413818, total_norm: 2.3460612297058105, lr: 5.9999999999999995e-05
Step 767, training loss: 4.999178886413574, total_norm: 2.517141819000244, lr: 5.9999999999999995e-05
Step 768, training loss: 5.470700263977051, total_norm: 2.257415294647217, lr: 5.9999999999999995e-05
Step 769, training loss: 4.948456287384033, total_norm: 2.3359568119049072, lr: 5.9999999999999995e-05
Step 770, training loss: 4.482025623321533, total_norm: 2.807722568511963, lr: 5.9999999999999995e-05
Step 771, training loss: 5.439302921295166, total_norm: 2.0678226947784424, lr: 5.9999999999999995e-05
Step 772, training loss: 5.571560859680176, total_norm: 2.3608758449554443, lr: 5.9999999999999995e-05
Step 773, training loss: 5.004659175872803, total_norm: 2.5527682304382324, lr: 5.9999999999999995e-05
Step 774, training loss: 5.461589336395264, total_norm: 2.6148762702941895, lr: 5.9999999999999995e-05
Step 775, training loss: 4.47770357131958, total_norm: 3.8835980892181396, lr: 5.9999999999999995e-05
Step 776, training loss: 4.371270179748535, total_norm: 3.600440740585327, lr: 5.9999999999999995e-05
Step 777, training loss: 4.794908046722412, total_norm: 2.1112213134765625, lr: 5.9999999999999995e-05
Step 778, training loss: 4.288244724273682, total_norm: 3.1756503582000732, lr: 5.9999999999999995e-05
Step 779, training loss: 5.133278846740723, total_norm: 2.3886466026306152, lr: 5.9999999999999995e-05
Step 780, training loss: 4.984278678894043, total_norm: 2.956555128097534, lr: 5.9999999999999995e-05
Step 781, training loss: 4.818253040313721, total_norm: 2.1930088996887207, lr: 5.9999999999999995e-05
Step 782, training loss: 5.573647499084473, total_norm: 2.392432928085327, lr: 5.9999999999999995e-05
Step 783, training loss: 5.707310676574707, total_norm: 2.4041876792907715, lr: 5.9999999999999995e-05
Step 784, training loss: 5.237079620361328, total_norm: 2.6863536834716797, lr: 5.9999999999999995e-05
Step 785, training loss: 5.244353294372559, total_norm: 3.253748893737793, lr: 5.9999999999999995e-05
Step 786, training loss: 5.525020599365234, total_norm: 2.6809744834899902, lr: 5.9999999999999995e-05
Step 787, training loss: 5.345787525177002, total_norm: 3.6156532764434814, lr: 5.9999999999999995e-05
Step 788, training loss: 5.629373073577881, total_norm: 2.4714791774749756, lr: 5.9999999999999995e-05
Step 789, training loss: 5.829732418060303, total_norm: 2.786186933517456, lr: 5.9999999999999995e-05
Step 790, training loss: 5.2591552734375, total_norm: 2.883277654647827, lr: 5.9999999999999995e-05
Step 791, training loss: 5.372861385345459, total_norm: 2.7827792167663574, lr: 5.9999999999999995e-05
Step 792, training loss: 5.606453895568848, total_norm: 2.561048984527588, lr: 5.9999999999999995e-05
Step 793, training loss: 5.929195404052734, total_norm: 2.1237051486968994, lr: 5.9999999999999995e-05
Step 794, training loss: 5.688591957092285, total_norm: 2.7582008838653564, lr: 5.9999999999999995e-05
Step 795, training loss: 5.428943157196045, total_norm: 2.3286075592041016, lr: 5.9999999999999995e-05
Step 796, training loss: 5.721401691436768, total_norm: 2.5525450706481934, lr: 5.9999999999999995e-05
Step 797, training loss: 5.4856462478637695, total_norm: 2.241126537322998, lr: 5.9999999999999995e-05
Step 798, training loss: 5.491358757019043, total_norm: 2.2109262943267822, lr: 5.9999999999999995e-05
Step 799, training loss: 5.269423007965088, total_norm: 2.9327290058135986, lr: 5.9999999999999995e-05
Step 800, training loss: 6.373173236846924, total_norm: 2.419694662094116, lr: 5.9999999999999995e-05
Step 800, validation loss: 5.812080383300781
Step 801, training loss: 5.727481365203857, total_norm: 2.545415163040161, lr: 5.9999999999999995e-05
Step 802, training loss: 5.417703628540039, total_norm: 2.267634391784668, lr: 5.9999999999999995e-05
Step 803, training loss: 5.209301948547363, total_norm: 2.8647027015686035, lr: 5.9999999999999995e-05
Step 804, training loss: 4.881991386413574, total_norm: 2.535884380340576, lr: 5.9999999999999995e-05
Step 805, training loss: 5.182912349700928, total_norm: 2.516897678375244, lr: 5.9999999999999995e-05
Step 806, training loss: 5.761281967163086, total_norm: 2.528977632522583, lr: 5.9999999999999995e-05
Step 807, training loss: 5.325128555297852, total_norm: 2.2680470943450928, lr: 5.9999999999999995e-05
Step 808, training loss: 5.040722370147705, total_norm: 2.383575916290283, lr: 5.9999999999999995e-05
Step 809, training loss: 5.247650146484375, total_norm: 2.3616573810577393, lr: 5.9999999999999995e-05
Step 810, training loss: 4.83474063873291, total_norm: 2.5779807567596436, lr: 5.9999999999999995e-05
Step 811, training loss: 4.9535231590271, total_norm: 2.6408214569091797, lr: 5.9999999999999995e-05
Step 812, training loss: 6.236392974853516, total_norm: 2.1758651733398438, lr: 5.9999999999999995e-05
Step 813, training loss: 5.434017658233643, total_norm: 2.3372409343719482, lr: 5.9999999999999995e-05
Step 814, training loss: 5.081909656524658, total_norm: 2.627601385116577, lr: 5.9999999999999995e-05
Step 815, training loss: 6.165297508239746, total_norm: 3.3094522953033447, lr: 5.9999999999999995e-05
Step 816, training loss: 5.098740100860596, total_norm: 3.3572983741760254, lr: 5.9999999999999995e-05
Step 817, training loss: 4.868618965148926, total_norm: 2.891071319580078, lr: 5.9999999999999995e-05
Step 818, training loss: 5.298046588897705, total_norm: 2.5643551349639893, lr: 5.9999999999999995e-05
Step 819, training loss: 4.983583450317383, total_norm: 2.5558390617370605, lr: 5.9999999999999995e-05
Step 820, training loss: 5.457038402557373, total_norm: 2.432000160217285, lr: 5.9999999999999995e-05
Step 821, training loss: 4.95111083984375, total_norm: 2.2857167720794678, lr: 5.9999999999999995e-05
Step 822, training loss: 5.371454238891602, total_norm: 2.7095000743865967, lr: 5.9999999999999995e-05
Step 823, training loss: 5.144717693328857, total_norm: 2.4729530811309814, lr: 5.9999999999999995e-05
Step 824, training loss: 5.611940860748291, total_norm: 2.4481489658355713, lr: 5.9999999999999995e-05
Step 825, training loss: 5.118793964385986, total_norm: 2.29811429977417, lr: 5.9999999999999995e-05
Step 826, training loss: 5.29632568359375, total_norm: 2.1246771812438965, lr: 5.9999999999999995e-05
Step 827, training loss: 4.964370250701904, total_norm: 2.59120512008667, lr: 5.9999999999999995e-05
Step 828, training loss: 5.917255401611328, total_norm: 2.7158238887786865, lr: 5.9999999999999995e-05
Step 829, training loss: 5.493895053863525, total_norm: 2.386080741882324, lr: 5.9999999999999995e-05
Step 830, training loss: 5.181396007537842, total_norm: 2.566847324371338, lr: 5.9999999999999995e-05
Step 831, training loss: 5.572748184204102, total_norm: 2.337663173675537, lr: 5.9999999999999995e-05
Step 832, training loss: 5.386443138122559, total_norm: 2.1906821727752686, lr: 5.9999999999999995e-05
Step 833, training loss: 5.417158126831055, total_norm: 2.244192361831665, lr: 5.9999999999999995e-05
Step 834, training loss: 5.3239240646362305, total_norm: 2.253312349319458, lr: 5.9999999999999995e-05
Step 835, training loss: 5.00635290145874, total_norm: 2.2554776668548584, lr: 5.9999999999999995e-05
Step 836, training loss: 5.575444221496582, total_norm: 2.3842732906341553, lr: 5.9999999999999995e-05
Step 837, training loss: 4.963002681732178, total_norm: 2.20644474029541, lr: 5.9999999999999995e-05
Step 838, training loss: 5.1793107986450195, total_norm: 2.0947413444519043, lr: 5.9999999999999995e-05
Step 839, training loss: 4.822925090789795, total_norm: 2.117882013320923, lr: 5.9999999999999995e-05
Step 840, training loss: 4.506962776184082, total_norm: 2.963325262069702, lr: 5.9999999999999995e-05
Step 841, training loss: 4.9776082038879395, total_norm: 2.246124029159546, lr: 5.9999999999999995e-05
Step 842, training loss: 5.216901779174805, total_norm: 2.2561182975769043, lr: 5.9999999999999995e-05
Step 843, training loss: 5.072912216186523, total_norm: 2.313859462738037, lr: 5.9999999999999995e-05
Step 844, training loss: 4.699435710906982, total_norm: 1.9294054508209229, lr: 5.9999999999999995e-05
Step 845, training loss: 4.7242279052734375, total_norm: 2.690463066101074, lr: 5.9999999999999995e-05
Step 846, training loss: 4.671860694885254, total_norm: 2.1405975818634033, lr: 5.9999999999999995e-05
Step 847, training loss: 5.675251483917236, total_norm: 2.20041561126709, lr: 5.9999999999999995e-05
Step 848, training loss: 5.0950398445129395, total_norm: 2.304636240005493, lr: 5.9999999999999995e-05
Step 849, training loss: 4.891408443450928, total_norm: 2.409865379333496, lr: 5.9999999999999995e-05
Step 850, training loss: 5.104567050933838, total_norm: 2.3891353607177734, lr: 5.9999999999999995e-05
Step 850, validation loss: 5.881207466125488
Step 851, training loss: 5.074785232543945, total_norm: 2.633718252182007, lr: 5.9999999999999995e-05
Step 852, training loss: 4.910967826843262, total_norm: 3.424988269805908, lr: 5.9999999999999995e-05
Step 853, training loss: 5.090353488922119, total_norm: 2.8313190937042236, lr: 5.9999999999999995e-05
Step 854, training loss: 5.408755302429199, total_norm: 2.414717674255371, lr: 5.9999999999999995e-05
Step 855, training loss: 5.7032904624938965, total_norm: 4.548685073852539, lr: 5.9999999999999995e-05
Step 856, training loss: 5.338989734649658, total_norm: 2.5813488960266113, lr: 5.9999999999999995e-05
Step 857, training loss: 5.9379353523254395, total_norm: 2.616445541381836, lr: 5.9999999999999995e-05
Step 858, training loss: 5.1958208084106445, total_norm: 2.8148534297943115, lr: 5.9999999999999995e-05
Step 859, training loss: 5.604601860046387, total_norm: 2.002089023590088, lr: 5.9999999999999995e-05
Step 860, training loss: 5.063028335571289, total_norm: 2.128905773162842, lr: 5.9999999999999995e-05
Step 861, training loss: 5.6536102294921875, total_norm: 2.222170352935791, lr: 5.9999999999999995e-05
Step 862, training loss: 4.99567985534668, total_norm: 4.2098588943481445, lr: 5.9999999999999995e-05
Step 863, training loss: 5.132552146911621, total_norm: 2.619535446166992, lr: 5.9999999999999995e-05
Step 864, training loss: 5.10488748550415, total_norm: 2.1740150451660156, lr: 5.9999999999999995e-05
Step 865, training loss: 5.4613938331604, total_norm: 2.356729745864868, lr: 5.9999999999999995e-05
Step 866, training loss: 5.131538391113281, total_norm: 2.271099805831909, lr: 5.9999999999999995e-05
Step 867, training loss: 5.351828575134277, total_norm: 2.335538148880005, lr: 5.9999999999999995e-05
Step 868, training loss: 5.499215126037598, total_norm: 2.416975259780884, lr: 5.9999999999999995e-05
Step 869, training loss: 5.157934188842773, total_norm: 2.450136184692383, lr: 5.9999999999999995e-05
Step 870, training loss: 5.160872459411621, total_norm: 2.5215911865234375, lr: 5.9999999999999995e-05
Step 871, training loss: 5.156238555908203, total_norm: 2.3563923835754395, lr: 5.9999999999999995e-05
Step 872, training loss: 5.10889196395874, total_norm: 2.8036603927612305, lr: 5.9999999999999995e-05
Step 873, training loss: 5.167721271514893, total_norm: 2.383021354675293, lr: 5.9999999999999995e-05
Step 874, training loss: 4.953311920166016, total_norm: 2.744649887084961, lr: 5.9999999999999995e-05
Step 875, training loss: 5.088537693023682, total_norm: 2.2348949909210205, lr: 5.9999999999999995e-05
Step 876, training loss: 5.1789374351501465, total_norm: 2.233630895614624, lr: 5.9999999999999995e-05
Step 877, training loss: 5.129497528076172, total_norm: 2.334203004837036, lr: 5.9999999999999995e-05
Step 878, training loss: 5.740761756896973, total_norm: 2.432842969894409, lr: 5.9999999999999995e-05
Step 879, training loss: 5.479639053344727, total_norm: 2.3193397521972656, lr: 5.9999999999999995e-05
Step 880, training loss: 5.29313850402832, total_norm: 2.624033212661743, lr: 5.9999999999999995e-05
Step 881, training loss: 5.8010945320129395, total_norm: 2.432438611984253, lr: 5.9999999999999995e-05
Step 882, training loss: 5.241069316864014, total_norm: 2.1211509704589844, lr: 5.9999999999999995e-05
Step 883, training loss: 5.670579433441162, total_norm: 2.3167269229888916, lr: 5.9999999999999995e-05
Step 884, training loss: 5.533117294311523, total_norm: 2.2957329750061035, lr: 5.9999999999999995e-05
Step 885, training loss: 5.536929130554199, total_norm: 2.068466901779175, lr: 5.9999999999999995e-05
Step 886, training loss: 5.355282306671143, total_norm: 2.2345588207244873, lr: 5.9999999999999995e-05
Step 887, training loss: 5.469639778137207, total_norm: 2.367738723754883, lr: 5.9999999999999995e-05
Step 888, training loss: 5.546558380126953, total_norm: 2.281604051589966, lr: 5.9999999999999995e-05
Step 889, training loss: 5.061915397644043, total_norm: 2.4600558280944824, lr: 5.9999999999999995e-05
Step 890, training loss: 4.787039279937744, total_norm: 2.6570799350738525, lr: 5.9999999999999995e-05
Step 891, training loss: 4.996304512023926, total_norm: 2.5261127948760986, lr: 5.9999999999999995e-05
Step 892, training loss: 5.362203121185303, total_norm: 2.5572428703308105, lr: 5.9999999999999995e-05
Step 893, training loss: 5.27918004989624, total_norm: 2.265629529953003, lr: 5.9999999999999995e-05
Step 894, training loss: 5.331497669219971, total_norm: 2.384711742401123, lr: 5.9999999999999995e-05
Step 895, training loss: 5.759434223175049, total_norm: 2.3841519355773926, lr: 5.9999999999999995e-05
Step 896, training loss: 5.249889373779297, total_norm: 2.590695381164551, lr: 5.9999999999999995e-05
Step 897, training loss: 5.104290008544922, total_norm: 2.307804584503174, lr: 5.9999999999999995e-05
Step 898, training loss: 5.540279865264893, total_norm: 2.2849323749542236, lr: 5.9999999999999995e-05
Step 899, training loss: 5.1575703620910645, total_norm: 2.410043239593506, lr: 5.9999999999999995e-05
Step 900, training loss: 5.3704915046691895, total_norm: 2.2220370769500732, lr: 5.9999999999999995e-05
Step 900, validation loss: 5.923032760620117
Step 901, training loss: 5.224095344543457, total_norm: 2.435960292816162, lr: 5.9999999999999995e-05
Step 902, training loss: 4.68540620803833, total_norm: 3.136813163757324, lr: 5.9999999999999995e-05
Step 903, training loss: 4.784246921539307, total_norm: 2.5625064373016357, lr: 5.9999999999999995e-05
Step 904, training loss: 4.366450786590576, total_norm: 5.080138206481934, lr: 5.9999999999999995e-05
Step 905, training loss: 4.630460262298584, total_norm: 3.8160855770111084, lr: 5.9999999999999995e-05
Step 906, training loss: 4.634922981262207, total_norm: 2.8642587661743164, lr: 5.9999999999999995e-05
Step 907, training loss: 5.498475551605225, total_norm: 2.4887449741363525, lr: 5.9999999999999995e-05
Step 908, training loss: 5.650684833526611, total_norm: 2.6487841606140137, lr: 5.9999999999999995e-05
Step 909, training loss: 5.2352614402771, total_norm: 2.6533212661743164, lr: 5.9999999999999995e-05
Step 910, training loss: 5.530461311340332, total_norm: 2.565382719039917, lr: 5.9999999999999995e-05
Step 911, training loss: 5.258484840393066, total_norm: 2.785557270050049, lr: 5.9999999999999995e-05
Step 912, training loss: 5.249701976776123, total_norm: 3.004960775375366, lr: 5.9999999999999995e-05
Step 913, training loss: 5.317936897277832, total_norm: 2.588428020477295, lr: 5.9999999999999995e-05
Step 914, training loss: 5.013613224029541, total_norm: 2.4466686248779297, lr: 5.9999999999999995e-05
Step 915, training loss: 4.933758735656738, total_norm: 2.2163257598876953, lr: 5.9999999999999995e-05
Step 916, training loss: 5.171801567077637, total_norm: 2.168032646179199, lr: 5.9999999999999995e-05
Step 917, training loss: 4.961944103240967, total_norm: 2.4432449340820312, lr: 5.9999999999999995e-05
Step 918, training loss: 5.185025691986084, total_norm: 2.378499746322632, lr: 5.9999999999999995e-05
Step 919, training loss: 5.183403491973877, total_norm: 2.6886239051818848, lr: 5.9999999999999995e-05
Step 920, training loss: 4.991095066070557, total_norm: 2.9654104709625244, lr: 5.9999999999999995e-05
Step 921, training loss: 4.858638763427734, total_norm: 2.1320691108703613, lr: 5.9999999999999995e-05
Step 922, training loss: 5.193051815032959, total_norm: 2.2502145767211914, lr: 5.9999999999999995e-05
Step 923, training loss: 4.923030853271484, total_norm: 2.109813928604126, lr: 5.9999999999999995e-05
Step 924, training loss: 4.857978343963623, total_norm: 2.2463736534118652, lr: 5.9999999999999995e-05
Step 925, training loss: 5.149343013763428, total_norm: 2.470127582550049, lr: 5.9999999999999995e-05
Step 926, training loss: 4.836056709289551, total_norm: 2.3439760208129883, lr: 5.9999999999999995e-05
Step 927, training loss: 4.8314032554626465, total_norm: 2.4641246795654297, lr: 5.9999999999999995e-05
Step 928, training loss: 5.179638862609863, total_norm: 2.2709436416625977, lr: 5.9999999999999995e-05
Step 929, training loss: 5.33499813079834, total_norm: 2.4612131118774414, lr: 5.9999999999999995e-05
Step 930, training loss: 4.7494730949401855, total_norm: 2.3363428115844727, lr: 5.9999999999999995e-05
Step 931, training loss: 4.678815841674805, total_norm: 2.3847203254699707, lr: 5.9999999999999995e-05
Step 932, training loss: 4.926328659057617, total_norm: 2.249863386154175, lr: 5.9999999999999995e-05
Step 933, training loss: 5.133212089538574, total_norm: 2.3590407371520996, lr: 5.9999999999999995e-05
Step 934, training loss: 5.1487860679626465, total_norm: 2.247800350189209, lr: 5.9999999999999995e-05
Step 935, training loss: 5.660524368286133, total_norm: 2.526247978210449, lr: 5.9999999999999995e-05
Step 936, training loss: 5.18142557144165, total_norm: 2.4141602516174316, lr: 5.9999999999999995e-05
Step 937, training loss: 4.844549179077148, total_norm: 2.189791202545166, lr: 5.9999999999999995e-05
Step 938, training loss: 4.379538059234619, total_norm: 2.582305669784546, lr: 5.9999999999999995e-05
Step 939, training loss: 4.504585266113281, total_norm: 2.4399654865264893, lr: 5.9999999999999995e-05
Step 940, training loss: 5.001185417175293, total_norm: 2.391658067703247, lr: 5.9999999999999995e-05
Step 941, training loss: 5.234708309173584, total_norm: 2.3697926998138428, lr: 5.9999999999999995e-05
Step 942, training loss: 5.528182506561279, total_norm: 2.453845739364624, lr: 5.9999999999999995e-05
Step 943, training loss: 5.405885219573975, total_norm: 2.57242751121521, lr: 5.9999999999999995e-05
Step 944, training loss: 6.215928077697754, total_norm: 2.8346292972564697, lr: 5.9999999999999995e-05
Step 945, training loss: 5.496278762817383, total_norm: 3.520203113555908, lr: 5.9999999999999995e-05
Step 946, training loss: 5.667821884155273, total_norm: 3.5822060108184814, lr: 5.9999999999999995e-05
Step 947, training loss: 5.476513385772705, total_norm: 3.0968592166900635, lr: 5.9999999999999995e-05
Step 948, training loss: 5.836111068725586, total_norm: 2.8112542629241943, lr: 5.9999999999999995e-05
Step 949, training loss: 5.406774044036865, total_norm: 2.9641904830932617, lr: 5.9999999999999995e-05
Step 950, training loss: 5.700892925262451, total_norm: 3.1636390686035156, lr: 5.9999999999999995e-05
Step 950, validation loss: 5.894983291625977
Step 951, training loss: 5.616369724273682, total_norm: 3.372831344604492, lr: 5.9999999999999995e-05
Step 952, training loss: 5.867412090301514, total_norm: 2.8520193099975586, lr: 5.9999999999999995e-05
Step 953, training loss: 5.583611488342285, total_norm: 2.4721736907958984, lr: 5.9999999999999995e-05
Step 954, training loss: 5.300780296325684, total_norm: 2.518008232116699, lr: 5.9999999999999995e-05
Step 955, training loss: 5.245595932006836, total_norm: 2.5158002376556396, lr: 5.9999999999999995e-05
Step 956, training loss: 5.565202236175537, total_norm: 2.64583683013916, lr: 5.9999999999999995e-05
Step 957, training loss: 5.0402703285217285, total_norm: 4.283438682556152, lr: 5.9999999999999995e-05
Step 958, training loss: 5.255244731903076, total_norm: 2.743173837661743, lr: 5.9999999999999995e-05
Step 959, training loss: 5.556128025054932, total_norm: 2.346856117248535, lr: 5.9999999999999995e-05
Step 960, training loss: 5.252444267272949, total_norm: 2.1690196990966797, lr: 5.9999999999999995e-05
Step 961, training loss: 5.307962417602539, total_norm: 2.127575397491455, lr: 5.9999999999999995e-05
Step 962, training loss: 5.382327079772949, total_norm: 2.5252344608306885, lr: 5.9999999999999995e-05
Step 963, training loss: 5.61591911315918, total_norm: 2.425731897354126, lr: 5.9999999999999995e-05
Step 964, training loss: 5.24791145324707, total_norm: 2.4106509685516357, lr: 5.9999999999999995e-05
Step 965, training loss: 5.040873050689697, total_norm: 2.413409948348999, lr: 5.9999999999999995e-05
Step 966, training loss: 5.043776512145996, total_norm: 2.346240758895874, lr: 5.9999999999999995e-05
Step 967, training loss: 5.240947723388672, total_norm: 2.4767749309539795, lr: 5.9999999999999995e-05
Step 968, training loss: 5.083345413208008, total_norm: 2.253147840499878, lr: 5.9999999999999995e-05
Step 969, training loss: 5.4651103019714355, total_norm: 2.4343435764312744, lr: 5.9999999999999995e-05
Step 970, training loss: 5.521106243133545, total_norm: 2.80574631690979, lr: 5.9999999999999995e-05
Step 971, training loss: 5.4313063621521, total_norm: 2.3087472915649414, lr: 5.9999999999999995e-05
Step 972, training loss: 5.405388832092285, total_norm: 2.2322964668273926, lr: 5.9999999999999995e-05
Step 973, training loss: 4.885929584503174, total_norm: 3.0737907886505127, lr: 5.9999999999999995e-05
Step 974, training loss: 5.825074672698975, total_norm: 2.1236963272094727, lr: 5.9999999999999995e-05
Step 975, training loss: 5.408079624176025, total_norm: 2.1562209129333496, lr: 5.9999999999999995e-05
Step 976, training loss: 5.186900615692139, total_norm: 2.411616802215576, lr: 5.9999999999999995e-05
Step 977, training loss: 6.036839962005615, total_norm: 2.2408483028411865, lr: 5.9999999999999995e-05
Step 978, training loss: 6.148055553436279, total_norm: 3.186119794845581, lr: 5.9999999999999995e-05
Step 979, training loss: 5.633400917053223, total_norm: 3.198848009109497, lr: 5.9999999999999995e-05
Step 980, training loss: 5.7867536544799805, total_norm: 2.1645445823669434, lr: 5.9999999999999995e-05
Step 981, training loss: 5.92655086517334, total_norm: 2.456962823867798, lr: 5.9999999999999995e-05
Step 982, training loss: 6.064868450164795, total_norm: 2.3089442253112793, lr: 5.9999999999999995e-05
Step 983, training loss: 5.788013935089111, total_norm: 2.484639883041382, lr: 5.9999999999999995e-05
Step 984, training loss: 5.463958263397217, total_norm: 2.612558126449585, lr: 5.9999999999999995e-05
Step 985, training loss: 5.461063385009766, total_norm: 2.3567755222320557, lr: 5.9999999999999995e-05
Step 986, training loss: 5.712343215942383, total_norm: 2.7054293155670166, lr: 5.9999999999999995e-05
Step 987, training loss: 5.636579513549805, total_norm: 2.785849094390869, lr: 5.9999999999999995e-05
Step 988, training loss: 5.831903457641602, total_norm: 2.3142447471618652, lr: 5.9999999999999995e-05
Step 989, training loss: 5.208724498748779, total_norm: 2.150402307510376, lr: 5.9999999999999995e-05
Step 990, training loss: 5.97727632522583, total_norm: 2.5437803268432617, lr: 5.9999999999999995e-05
Step 991, training loss: 5.8586039543151855, total_norm: 2.7580912113189697, lr: 5.9999999999999995e-05
Step 992, training loss: 5.507258415222168, total_norm: 3.6531336307525635, lr: 5.9999999999999995e-05
Step 993, training loss: 5.193689346313477, total_norm: 3.857884645462036, lr: 5.9999999999999995e-05
Step 994, training loss: 5.651124954223633, total_norm: 2.355395555496216, lr: 5.9999999999999995e-05
Step 995, training loss: 4.917555332183838, total_norm: 2.4548401832580566, lr: 5.9999999999999995e-05
Step 996, training loss: 5.023106575012207, total_norm: 2.485041856765747, lr: 5.9999999999999995e-05
Step 997, training loss: 5.513603687286377, total_norm: 2.138453245162964, lr: 5.9999999999999995e-05
Step 998, training loss: 5.196551322937012, total_norm: 2.395740032196045, lr: 5.9999999999999995e-05
Step 999, training loss: 5.178347110748291, total_norm: 2.6709625720977783, lr: 5.9999999999999995e-05
Step 1000, training loss: 5.334466934204102, total_norm: 2.6577513217926025, lr: 5.9999999999999995e-05
Step 1000, validation loss: 5.775570869445801
Step 1001, training loss: 5.81712007522583, total_norm: 2.1660916805267334, lr: 5.9999999999999995e-05
Step 1002, training loss: 5.0298171043396, total_norm: 2.626448392868042, lr: 5.9999999999999995e-05
Step 1003, training loss: 5.1735358238220215, total_norm: 2.3682546615600586, lr: 5.9999999999999995e-05
Step 1004, training loss: 5.042874336242676, total_norm: 2.979698896408081, lr: 5.9999999999999995e-05
Step 1005, training loss: 5.121325969696045, total_norm: 3.0220372676849365, lr: 5.9999999999999995e-05
Step 1006, training loss: 5.85236930847168, total_norm: 2.2848551273345947, lr: 5.9999999999999995e-05
Step 1007, training loss: 5.081160545349121, total_norm: 2.1425368785858154, lr: 5.9999999999999995e-05
Step 1008, training loss: 5.239867210388184, total_norm: 2.5334887504577637, lr: 5.9999999999999995e-05
Step 1009, training loss: 5.035062789916992, total_norm: 3.1760590076446533, lr: 5.9999999999999995e-05
Step 1010, training loss: 5.154266834259033, total_norm: 2.5364413261413574, lr: 5.9999999999999995e-05
Step 1011, training loss: 5.4226393699646, total_norm: 2.2373859882354736, lr: 5.9999999999999995e-05
Step 1012, training loss: 5.308027267456055, total_norm: 2.193267345428467, lr: 5.9999999999999995e-05
Step 1013, training loss: 4.943583011627197, total_norm: 2.167741060256958, lr: 5.9999999999999995e-05
Step 1014, training loss: 5.779433727264404, total_norm: 2.5362436771392822, lr: 5.9999999999999995e-05
Step 1015, training loss: 6.102434158325195, total_norm: 2.462289571762085, lr: 5.9999999999999995e-05
Step 1016, training loss: 5.943795204162598, total_norm: 2.2579457759857178, lr: 5.9999999999999995e-05
Step 1017, training loss: 5.136030197143555, total_norm: 2.416684865951538, lr: 5.9999999999999995e-05
Step 1018, training loss: 4.83158540725708, total_norm: 2.437779188156128, lr: 5.9999999999999995e-05
Step 1019, training loss: 5.229735851287842, total_norm: 2.6434550285339355, lr: 5.9999999999999995e-05
Step 1020, training loss: 4.92547082901001, total_norm: 2.6513147354125977, lr: 5.9999999999999995e-05
Step 1021, training loss: 5.1148834228515625, total_norm: 2.46073842048645, lr: 5.9999999999999995e-05
Step 1022, training loss: 5.535198211669922, total_norm: 2.6421732902526855, lr: 5.9999999999999995e-05
Step 1023, training loss: 5.709934711456299, total_norm: 2.802823781967163, lr: 5.9999999999999995e-05
Step 1024, training loss: 5.287152290344238, total_norm: 2.754981517791748, lr: 5.9999999999999995e-05
Step 1025, training loss: 5.2491350173950195, total_norm: 2.5258917808532715, lr: 5.9999999999999995e-05
Step 1026, training loss: 5.514612197875977, total_norm: 4.880014896392822, lr: 5.9999999999999995e-05
Step 1027, training loss: 5.687207221984863, total_norm: 4.17016077041626, lr: 5.9999999999999995e-05
Step 1028, training loss: 5.586787223815918, total_norm: 2.749816656112671, lr: 5.9999999999999995e-05
Step 1029, training loss: 5.675389289855957, total_norm: 2.676456928253174, lr: 5.9999999999999995e-05
Step 1030, training loss: 5.7958903312683105, total_norm: 2.8626251220703125, lr: 5.9999999999999995e-05
Step 1031, training loss: 5.415548801422119, total_norm: 3.794557571411133, lr: 5.9999999999999995e-05
Step 1032, training loss: 5.626293182373047, total_norm: 3.3009207248687744, lr: 5.9999999999999995e-05
Step 1033, training loss: 5.267764568328857, total_norm: 2.8246028423309326, lr: 5.9999999999999995e-05
Step 1034, training loss: 5.362548351287842, total_norm: 4.253593444824219, lr: 5.9999999999999995e-05
Step 1035, training loss: 5.160009860992432, total_norm: 4.656223297119141, lr: 5.9999999999999995e-05
Step 1036, training loss: 5.362450122833252, total_norm: 3.866407632827759, lr: 5.9999999999999995e-05
Step 1037, training loss: 4.855654239654541, total_norm: 3.597066879272461, lr: 5.9999999999999995e-05
Step 1038, training loss: 5.36663818359375, total_norm: 3.4659759998321533, lr: 5.9999999999999995e-05
Step 1039, training loss: 5.305138111114502, total_norm: 2.9698472023010254, lr: 5.9999999999999995e-05
Step 1040, training loss: 5.077253818511963, total_norm: 2.7670774459838867, lr: 5.9999999999999995e-05
Step 1041, training loss: 4.879515171051025, total_norm: 3.1669793128967285, lr: 5.9999999999999995e-05
Step 1042, training loss: 4.6681694984436035, total_norm: 3.551589250564575, lr: 5.9999999999999995e-05
Step 1043, training loss: 5.038232803344727, total_norm: 2.6974356174468994, lr: 5.9999999999999995e-05
Step 1044, training loss: 5.418155193328857, total_norm: 3.395218849182129, lr: 5.9999999999999995e-05
Step 1045, training loss: 5.346363544464111, total_norm: 3.1635336875915527, lr: 5.9999999999999995e-05
Step 1046, training loss: 5.018116474151611, total_norm: 3.4156317710876465, lr: 5.9999999999999995e-05
Step 1047, training loss: 5.373630046844482, total_norm: 2.875899076461792, lr: 5.9999999999999995e-05
Step 1048, training loss: 4.989987373352051, total_norm: 2.521580219268799, lr: 5.9999999999999995e-05
Step 1049, training loss: 4.907413482666016, total_norm: 2.384540557861328, lr: 5.9999999999999995e-05
Step 1050, training loss: 4.986383438110352, total_norm: 2.4432458877563477, lr: 5.9999999999999995e-05
Step 1050, validation loss: 5.547300815582275
Step 1051, training loss: 5.218847751617432, total_norm: 2.159820795059204, lr: 5.9999999999999995e-05
Step 1052, training loss: 5.730731964111328, total_norm: 2.7745466232299805, lr: 5.9999999999999995e-05
Step 1053, training loss: 4.98513650894165, total_norm: 3.979038953781128, lr: 5.9999999999999995e-05
Step 1054, training loss: 5.230537414550781, total_norm: 3.632716655731201, lr: 5.9999999999999995e-05
Step 1055, training loss: 5.293812274932861, total_norm: 2.688563108444214, lr: 5.9999999999999995e-05
Step 1056, training loss: 5.543757438659668, total_norm: 3.227372407913208, lr: 5.9999999999999995e-05
Step 1057, training loss: 5.783049583435059, total_norm: 3.0904293060302734, lr: 5.9999999999999995e-05
Step 1058, training loss: 5.324615955352783, total_norm: 3.0518314838409424, lr: 5.9999999999999995e-05
Step 1059, training loss: 5.299652099609375, total_norm: 2.4952924251556396, lr: 5.9999999999999995e-05
Step 1060, training loss: 5.767030239105225, total_norm: 2.6656923294067383, lr: 5.9999999999999995e-05
Step 1061, training loss: 5.494399547576904, total_norm: 2.764927625656128, lr: 5.9999999999999995e-05
Step 1062, training loss: 5.345425605773926, total_norm: 3.375507354736328, lr: 5.9999999999999995e-05
Step 1063, training loss: 5.517316818237305, total_norm: 2.834756374359131, lr: 5.9999999999999995e-05
Step 1064, training loss: 5.6603217124938965, total_norm: 2.957521677017212, lr: 5.9999999999999995e-05
Step 1065, training loss: 5.602782249450684, total_norm: 3.3313190937042236, lr: 5.9999999999999995e-05
Step 1066, training loss: 5.31955099105835, total_norm: 2.846717596054077, lr: 5.9999999999999995e-05
Step 1067, training loss: 5.0015082359313965, total_norm: 2.769151449203491, lr: 5.9999999999999995e-05
Step 1068, training loss: 5.4248270988464355, total_norm: 2.521768808364868, lr: 5.9999999999999995e-05
Step 1069, training loss: 5.058337211608887, total_norm: 2.4949424266815186, lr: 5.9999999999999995e-05
Step 1070, training loss: 5.230184078216553, total_norm: 2.3731791973114014, lr: 5.9999999999999995e-05
Step 1071, training loss: 4.902928352355957, total_norm: 2.4967710971832275, lr: 5.9999999999999995e-05
Step 1072, training loss: 5.165599822998047, total_norm: 2.0432114601135254, lr: 5.9999999999999995e-05
Step 1073, training loss: 5.183512210845947, total_norm: 2.203543186187744, lr: 5.9999999999999995e-05
Step 1074, training loss: 5.315184116363525, total_norm: 2.094541549682617, lr: 5.9999999999999995e-05
Step 1075, training loss: 4.734649658203125, total_norm: 2.4992928504943848, lr: 5.9999999999999995e-05
Step 1076, training loss: 4.891749382019043, total_norm: 2.055691719055176, lr: 5.9999999999999995e-05
Step 1077, training loss: 4.6760969161987305, total_norm: 2.555367946624756, lr: 5.9999999999999995e-05
Step 1078, training loss: 5.889461994171143, total_norm: 2.863642454147339, lr: 5.9999999999999995e-05
Step 1079, training loss: 5.355256080627441, total_norm: 2.5606467723846436, lr: 5.9999999999999995e-05
Step 1080, training loss: 4.947702407836914, total_norm: 2.5826289653778076, lr: 5.9999999999999995e-05
Step 1081, training loss: 4.920714855194092, total_norm: 2.701364278793335, lr: 5.9999999999999995e-05
Step 1082, training loss: 5.39414119720459, total_norm: 2.538013219833374, lr: 5.9999999999999995e-05
Step 1083, training loss: 5.051019668579102, total_norm: 2.7790279388427734, lr: 5.9999999999999995e-05
Step 1084, training loss: 5.468380451202393, total_norm: 2.779078483581543, lr: 5.9999999999999995e-05
Step 1085, training loss: 4.85884952545166, total_norm: 2.6646385192871094, lr: 5.9999999999999995e-05
Step 1086, training loss: 5.529237747192383, total_norm: 2.2166035175323486, lr: 5.9999999999999995e-05
Step 1087, training loss: 4.686505317687988, total_norm: 2.2398085594177246, lr: 5.9999999999999995e-05
Step 1088, training loss: 5.086277961730957, total_norm: 2.6838200092315674, lr: 5.9999999999999995e-05
Step 1089, training loss: 4.93084192276001, total_norm: 2.5122110843658447, lr: 5.9999999999999995e-05
Step 1090, training loss: 4.40354585647583, total_norm: 2.9939827919006348, lr: 5.9999999999999995e-05
Step 1091, training loss: 5.316025733947754, total_norm: 2.3959848880767822, lr: 5.9999999999999995e-05
Step 1092, training loss: 4.181278228759766, total_norm: 2.3270301818847656, lr: 5.9999999999999995e-05
Step 1093, training loss: 5.177731990814209, total_norm: 2.606604814529419, lr: 5.9999999999999995e-05
Step 1094, training loss: 5.374352931976318, total_norm: 2.4810404777526855, lr: 5.9999999999999995e-05
Step 1095, training loss: 4.428498268127441, total_norm: 2.68068790435791, lr: 5.9999999999999995e-05
Step 1096, training loss: 4.009951591491699, total_norm: 2.542421579360962, lr: 5.9999999999999995e-05
Step 1097, training loss: 4.51103401184082, total_norm: 2.191777229309082, lr: 5.9999999999999995e-05
Step 1098, training loss: 4.9196929931640625, total_norm: 2.411371946334839, lr: 5.9999999999999995e-05
Step 1099, training loss: 5.358619689941406, total_norm: 2.2900426387786865, lr: 5.9999999999999995e-05
Step 1100, training loss: 4.5968732833862305, total_norm: 2.385063886642456, lr: 5.9999999999999995e-05
Step 1100, validation loss: 5.873547554016113
Step 1101, training loss: 4.242791652679443, total_norm: 2.594419002532959, lr: 5.9999999999999995e-05
Step 1102, training loss: 4.229915142059326, total_norm: 2.12091326713562, lr: 5.9999999999999995e-05
Step 1103, training loss: 4.135131359100342, total_norm: 2.489842176437378, lr: 5.9999999999999995e-05
Step 1104, training loss: 4.786071300506592, total_norm: 2.2199511528015137, lr: 5.9999999999999995e-05
Step 1105, training loss: 4.639560222625732, total_norm: 2.2050585746765137, lr: 5.9999999999999995e-05
Step 1106, training loss: 4.147427558898926, total_norm: 2.2420482635498047, lr: 5.9999999999999995e-05
Step 1107, training loss: 5.156199932098389, total_norm: 2.160170078277588, lr: 5.9999999999999995e-05
Step 1108, training loss: 4.895791530609131, total_norm: 2.3380508422851562, lr: 5.9999999999999995e-05
Step 1109, training loss: 4.500410556793213, total_norm: 2.0533649921417236, lr: 5.9999999999999995e-05
Step 1110, training loss: 4.3420610427856445, total_norm: 2.078050374984741, lr: 5.9999999999999995e-05
Step 1111, training loss: 4.43238639831543, total_norm: 2.380035877227783, lr: 5.9999999999999995e-05
Step 1112, training loss: 4.215646743774414, total_norm: 2.209228515625, lr: 5.9999999999999995e-05
Step 1113, training loss: 5.033882141113281, total_norm: 2.219647169113159, lr: 5.9999999999999995e-05
Step 1114, training loss: 5.163960933685303, total_norm: 2.198763370513916, lr: 5.9999999999999995e-05
Step 1115, training loss: 4.10640811920166, total_norm: 2.6714484691619873, lr: 5.9999999999999995e-05
Step 1116, training loss: 4.233127117156982, total_norm: 2.712510108947754, lr: 5.9999999999999995e-05
Step 1117, training loss: 5.528456211090088, total_norm: 3.5191004276275635, lr: 5.9999999999999995e-05
Step 1118, training loss: 4.997591018676758, total_norm: 2.5598111152648926, lr: 5.9999999999999995e-05
Step 1119, training loss: 4.256138324737549, total_norm: 3.310724973678589, lr: 5.9999999999999995e-05
Step 1120, training loss: 4.45670223236084, total_norm: 2.8883352279663086, lr: 5.9999999999999995e-05
Step 1121, training loss: 5.57855749130249, total_norm: 2.704373836517334, lr: 5.9999999999999995e-05
Step 1122, training loss: 5.4160003662109375, total_norm: 2.332473039627075, lr: 5.9999999999999995e-05
Step 1123, training loss: 4.738567352294922, total_norm: 3.7341420650482178, lr: 5.9999999999999995e-05
Step 1124, training loss: 5.311771392822266, total_norm: 2.5252578258514404, lr: 5.9999999999999995e-05
Step 1125, training loss: 4.607873916625977, total_norm: 2.4087817668914795, lr: 5.9999999999999995e-05
Step 1126, training loss: 4.520753860473633, total_norm: 2.766658306121826, lr: 5.9999999999999995e-05
Step 1127, training loss: 4.418035984039307, total_norm: 2.4329404830932617, lr: 5.9999999999999995e-05
Step 1128, training loss: 4.702526569366455, total_norm: 2.287653684616089, lr: 5.9999999999999995e-05
Step 1129, training loss: 4.469335556030273, total_norm: 2.2403929233551025, lr: 5.9999999999999995e-05
Step 1130, training loss: 5.080674171447754, total_norm: 2.226649045944214, lr: 5.9999999999999995e-05
Step 1131, training loss: 5.019986152648926, total_norm: 2.2711753845214844, lr: 5.9999999999999995e-05
Step 1132, training loss: 4.584809303283691, total_norm: 2.146439790725708, lr: 5.9999999999999995e-05
Step 1133, training loss: 4.156010627746582, total_norm: 2.2054784297943115, lr: 5.9999999999999995e-05
Step 1134, training loss: 5.092278957366943, total_norm: 2.4335532188415527, lr: 5.9999999999999995e-05
Step 1135, training loss: 5.250086784362793, total_norm: 2.561983823776245, lr: 5.9999999999999995e-05
Step 1136, training loss: 4.796111583709717, total_norm: 2.1107616424560547, lr: 5.9999999999999995e-05
Step 1137, training loss: 5.33650541305542, total_norm: 2.3685142993927, lr: 5.9999999999999995e-05
Step 1138, training loss: 4.846685886383057, total_norm: 2.6977217197418213, lr: 5.9999999999999995e-05
Step 1139, training loss: 5.346836090087891, total_norm: 2.4717044830322266, lr: 5.9999999999999995e-05
Step 1140, training loss: 5.523653984069824, total_norm: 2.4485044479370117, lr: 5.9999999999999995e-05
Step 1141, training loss: 4.552428245544434, total_norm: 2.36966609954834, lr: 5.9999999999999995e-05
Step 1142, training loss: 5.078778266906738, total_norm: 2.4563448429107666, lr: 5.9999999999999995e-05
Step 1143, training loss: 5.0363264083862305, total_norm: 2.4352595806121826, lr: 5.9999999999999995e-05
Step 1144, training loss: 4.999090194702148, total_norm: 2.3495032787323, lr: 5.9999999999999995e-05
Step 1145, training loss: 4.986170291900635, total_norm: 2.2673749923706055, lr: 5.9999999999999995e-05
Step 1146, training loss: 4.665499687194824, total_norm: 2.806154489517212, lr: 5.9999999999999995e-05
Step 1147, training loss: 4.996321678161621, total_norm: 2.95892596244812, lr: 5.9999999999999995e-05
Step 1148, training loss: 5.977836608886719, total_norm: 2.873337984085083, lr: 5.9999999999999995e-05
Step 1149, training loss: 5.41050910949707, total_norm: 3.0270168781280518, lr: 5.9999999999999995e-05
Step 1150, training loss: 5.3686089515686035, total_norm: 3.3407254219055176, lr: 5.9999999999999995e-05
Step 1150, validation loss: 5.7008056640625
Step 1151, training loss: 5.128322124481201, total_norm: 2.869480609893799, lr: 5.9999999999999995e-05
Step 1152, training loss: 5.674276828765869, total_norm: 2.7580244541168213, lr: 5.9999999999999995e-05
Step 1153, training loss: 5.488248825073242, total_norm: 2.574077844619751, lr: 5.9999999999999995e-05
Step 1154, training loss: 5.0090179443359375, total_norm: 3.214071035385132, lr: 5.9999999999999995e-05
Step 1155, training loss: 4.682098865509033, total_norm: 3.828270673751831, lr: 5.9999999999999995e-05
Step 1156, training loss: 4.878641128540039, total_norm: 3.526503324508667, lr: 5.9999999999999995e-05
Step 1157, training loss: 4.584090709686279, total_norm: 2.895655393600464, lr: 5.9999999999999995e-05
Step 1158, training loss: 5.043386936187744, total_norm: 2.4897615909576416, lr: 5.9999999999999995e-05
Step 1159, training loss: 5.249775409698486, total_norm: 2.337340831756592, lr: 5.9999999999999995e-05
Step 1160, training loss: 5.147483825683594, total_norm: 2.6209468841552734, lr: 5.9999999999999995e-05
Step 1161, training loss: 5.280007362365723, total_norm: 2.429013252258301, lr: 5.9999999999999995e-05
Step 1162, training loss: 4.847562789916992, total_norm: 2.335062026977539, lr: 5.9999999999999995e-05
Step 1163, training loss: 4.660222053527832, total_norm: 2.412961006164551, lr: 5.9999999999999995e-05
Step 1164, training loss: 4.941339492797852, total_norm: 2.2156822681427, lr: 5.9999999999999995e-05
Step 1165, training loss: 5.038236618041992, total_norm: 2.7198173999786377, lr: 5.9999999999999995e-05
Step 1166, training loss: 4.766000270843506, total_norm: 2.6858108043670654, lr: 5.9999999999999995e-05
Step 1167, training loss: 4.847326278686523, total_norm: 2.2405216693878174, lr: 5.9999999999999995e-05
Step 1168, training loss: 4.721329689025879, total_norm: 2.3796610832214355, lr: 5.9999999999999995e-05
Step 1169, training loss: 5.052364826202393, total_norm: 2.2380096912384033, lr: 5.9999999999999995e-05
Step 1170, training loss: 5.651692867279053, total_norm: 2.7211861610412598, lr: 5.9999999999999995e-05
Step 1171, training loss: 5.0199360847473145, total_norm: 2.6820123195648193, lr: 5.9999999999999995e-05
Step 1172, training loss: 5.247290134429932, total_norm: 3.2593417167663574, lr: 5.9999999999999995e-05
Step 1173, training loss: 4.660026550292969, total_norm: 3.073784351348877, lr: 5.9999999999999995e-05
Step 1174, training loss: 4.98674201965332, total_norm: 2.155665159225464, lr: 5.9999999999999995e-05
Step 1175, training loss: 4.4957990646362305, total_norm: 2.4174211025238037, lr: 5.9999999999999995e-05
Step 1176, training loss: 4.805813789367676, total_norm: 2.3058536052703857, lr: 5.9999999999999995e-05
Step 1177, training loss: 4.899624824523926, total_norm: 2.8688204288482666, lr: 5.9999999999999995e-05
Step 1178, training loss: 4.785115718841553, total_norm: 2.468174934387207, lr: 5.9999999999999995e-05
Step 1179, training loss: 5.097418785095215, total_norm: 3.0971102714538574, lr: 5.9999999999999995e-05
Step 1180, training loss: 4.808037281036377, total_norm: 2.3128061294555664, lr: 5.9999999999999995e-05
Step 1181, training loss: 4.975602626800537, total_norm: 2.392169952392578, lr: 5.9999999999999995e-05
Step 1182, training loss: 5.113272190093994, total_norm: 2.844750165939331, lr: 5.9999999999999995e-05
Step 1183, training loss: 4.98079252243042, total_norm: 2.387690544128418, lr: 5.9999999999999995e-05
Step 1184, training loss: 4.5686540603637695, total_norm: 2.49569034576416, lr: 5.9999999999999995e-05
Step 1185, training loss: 4.929242134094238, total_norm: 2.5598795413970947, lr: 5.9999999999999995e-05
Step 1186, training loss: 4.715475082397461, total_norm: 2.7224373817443848, lr: 5.9999999999999995e-05
Step 1187, training loss: 4.5984625816345215, total_norm: 2.5286781787872314, lr: 5.9999999999999995e-05
Step 1188, training loss: 4.782950401306152, total_norm: 2.7253217697143555, lr: 5.9999999999999995e-05
Step 1189, training loss: 4.988753318786621, total_norm: 2.377180814743042, lr: 5.9999999999999995e-05
Step 1190, training loss: 4.2731733322143555, total_norm: 2.6338460445404053, lr: 5.9999999999999995e-05
Step 1191, training loss: 4.1179914474487305, total_norm: 2.7120869159698486, lr: 5.9999999999999995e-05
Step 1192, training loss: 4.716065406799316, total_norm: 2.186434030532837, lr: 5.9999999999999995e-05
Step 1193, training loss: 4.536815166473389, total_norm: 2.5084500312805176, lr: 5.9999999999999995e-05
Step 1194, training loss: 4.921808242797852, total_norm: 2.3729000091552734, lr: 5.9999999999999995e-05
Step 1195, training loss: 4.735772132873535, total_norm: 2.282597780227661, lr: 5.9999999999999995e-05
Step 1196, training loss: 4.542085647583008, total_norm: 2.3036038875579834, lr: 5.9999999999999995e-05
Step 1197, training loss: 4.767192840576172, total_norm: 2.5985536575317383, lr: 5.9999999999999995e-05
Step 1198, training loss: 4.3482770919799805, total_norm: 2.5089497566223145, lr: 5.9999999999999995e-05
Step 1199, training loss: 4.88747501373291, total_norm: 2.3789656162261963, lr: 5.9999999999999995e-05
Step 1200, training loss: 4.973002910614014, total_norm: 2.44817852973938, lr: 5.9999999999999995e-05
Step 1200, validation loss: 5.809717178344727
Step 1201, training loss: 4.655788421630859, total_norm: 2.450899124145508, lr: 5.9999999999999995e-05
Step 1202, training loss: 5.034895420074463, total_norm: 2.3932244777679443, lr: 5.9999999999999995e-05
Step 1203, training loss: 5.132558345794678, total_norm: 2.2890632152557373, lr: 5.9999999999999995e-05
Step 1204, training loss: 5.031886100769043, total_norm: 2.3160178661346436, lr: 5.9999999999999995e-05
Step 1205, training loss: 4.6815056800842285, total_norm: 2.339277744293213, lr: 5.9999999999999995e-05
Step 1206, training loss: 4.991631031036377, total_norm: 2.3588664531707764, lr: 5.9999999999999995e-05
Step 1207, training loss: 5.145298480987549, total_norm: 2.3987197875976562, lr: 5.9999999999999995e-05
Step 1208, training loss: 4.936141014099121, total_norm: 2.2541189193725586, lr: 5.9999999999999995e-05
Step 1209, training loss: 4.504439353942871, total_norm: 2.336728811264038, lr: 5.9999999999999995e-05
Step 1210, training loss: 4.615571022033691, total_norm: 2.805180311203003, lr: 5.9999999999999995e-05
Step 1211, training loss: 5.092103004455566, total_norm: 2.411198616027832, lr: 5.9999999999999995e-05
Step 1212, training loss: 4.648633003234863, total_norm: 3.1509652137756348, lr: 5.9999999999999995e-05
Step 1213, training loss: 5.058611869812012, total_norm: 3.10642409324646, lr: 5.9999999999999995e-05
Step 1214, training loss: 4.789898872375488, total_norm: 3.074096441268921, lr: 5.9999999999999995e-05
Step 1215, training loss: 4.317750930786133, total_norm: 2.8641443252563477, lr: 5.9999999999999995e-05
Step 1216, training loss: 5.392069339752197, total_norm: 2.453929901123047, lr: 5.9999999999999995e-05
Step 1217, training loss: 5.251199722290039, total_norm: 2.612807512283325, lr: 5.9999999999999995e-05
Step 1218, training loss: 5.0356669425964355, total_norm: 2.879523277282715, lr: 5.9999999999999995e-05
Step 1219, training loss: 5.2275214195251465, total_norm: 2.659193515777588, lr: 5.9999999999999995e-05
Step 1220, training loss: 5.214454650878906, total_norm: 2.415148973464966, lr: 5.9999999999999995e-05
Step 1221, training loss: 4.407101631164551, total_norm: 4.285247802734375, lr: 5.9999999999999995e-05
Step 1222, training loss: 4.774188041687012, total_norm: 3.2111988067626953, lr: 5.9999999999999995e-05
Step 1223, training loss: 4.570549011230469, total_norm: 2.8930671215057373, lr: 5.9999999999999995e-05
Step 1224, training loss: 4.127376556396484, total_norm: 2.450456142425537, lr: 5.9999999999999995e-05
Step 1225, training loss: 4.367862701416016, total_norm: 2.547128200531006, lr: 5.9999999999999995e-05
Step 1226, training loss: 5.312648296356201, total_norm: 2.462494134902954, lr: 5.9999999999999995e-05
Step 1227, training loss: 3.6604108810424805, total_norm: 4.301913738250732, lr: 5.9999999999999995e-05
Step 1228, training loss: 4.494355201721191, total_norm: 3.157426357269287, lr: 5.9999999999999995e-05
Step 1229, training loss: 4.541585445404053, total_norm: 2.5520036220550537, lr: 5.9999999999999995e-05
Step 1230, training loss: 4.472857475280762, total_norm: 2.580589532852173, lr: 5.9999999999999995e-05
Step 1231, training loss: 4.6475629806518555, total_norm: 2.514944314956665, lr: 5.9999999999999995e-05
Step 1232, training loss: 5.14028263092041, total_norm: 2.4612154960632324, lr: 5.9999999999999995e-05
Step 1233, training loss: 5.363173484802246, total_norm: 2.5956931114196777, lr: 5.9999999999999995e-05
Step 1234, training loss: 5.212506294250488, total_norm: 3.3902015686035156, lr: 5.9999999999999995e-05
Step 1235, training loss: 5.234836578369141, total_norm: 3.1312477588653564, lr: 5.9999999999999995e-05
Step 1236, training loss: 4.660823822021484, total_norm: 3.0172250270843506, lr: 5.9999999999999995e-05
Step 1237, training loss: 5.369213581085205, total_norm: 3.203026056289673, lr: 5.9999999999999995e-05
Step 1238, training loss: 5.083044052124023, total_norm: 2.833029270172119, lr: 5.9999999999999995e-05
Step 1239, training loss: 5.184081554412842, total_norm: 2.5333287715911865, lr: 5.9999999999999995e-05
Step 1240, training loss: 4.786335468292236, total_norm: 3.1102311611175537, lr: 5.9999999999999995e-05
Step 1241, training loss: 4.839752674102783, total_norm: 2.79219651222229, lr: 5.9999999999999995e-05
Step 1242, training loss: 5.325554847717285, total_norm: 2.823594331741333, lr: 5.9999999999999995e-05
Step 1243, training loss: 5.302940368652344, total_norm: 3.3302676677703857, lr: 5.9999999999999995e-05
Step 1244, training loss: 5.462338924407959, total_norm: 2.5615313053131104, lr: 5.9999999999999995e-05
Step 1245, training loss: 5.393798351287842, total_norm: 2.4935169219970703, lr: 5.9999999999999995e-05
Step 1246, training loss: 5.324696063995361, total_norm: 2.9266300201416016, lr: 5.9999999999999995e-05
Step 1247, training loss: 5.451697826385498, total_norm: 2.5001413822174072, lr: 5.9999999999999995e-05
Step 1248, training loss: 5.425375938415527, total_norm: 2.6082041263580322, lr: 5.9999999999999995e-05
Step 1249, training loss: 5.41227388381958, total_norm: 2.836366653442383, lr: 5.9999999999999995e-05
Step 1250, training loss: 5.121374130249023, total_norm: 2.7808473110198975, lr: 5.9999999999999995e-05
Step 1250, validation loss: 5.790139198303223
Step 1251, training loss: 5.274472713470459, total_norm: 3.2618353366851807, lr: 5.9999999999999995e-05
Step 1252, training loss: 5.348445892333984, total_norm: 2.806316614151001, lr: 5.9999999999999995e-05
Step 1253, training loss: 5.616945266723633, total_norm: 2.567603826522827, lr: 5.9999999999999995e-05
Step 1254, training loss: 5.202647686004639, total_norm: 2.6909546852111816, lr: 5.9999999999999995e-05
Step 1255, training loss: 5.1088948249816895, total_norm: 2.4099068641662598, lr: 5.9999999999999995e-05
Step 1256, training loss: 4.873831272125244, total_norm: 3.608309030532837, lr: 5.9999999999999995e-05
Step 1257, training loss: 5.1423821449279785, total_norm: 2.6398377418518066, lr: 5.9999999999999995e-05
Step 1258, training loss: 5.368320465087891, total_norm: 2.291144371032715, lr: 5.9999999999999995e-05
Step 1259, training loss: 5.279543399810791, total_norm: 2.574861526489258, lr: 5.9999999999999995e-05
Step 1260, training loss: 5.784696578979492, total_norm: 2.9599530696868896, lr: 5.9999999999999995e-05
Step 1261, training loss: 4.991802215576172, total_norm: 2.60062575340271, lr: 5.9999999999999995e-05
Step 1262, training loss: 5.097559928894043, total_norm: 2.495370864868164, lr: 5.9999999999999995e-05
Step 1263, training loss: 5.0570502281188965, total_norm: 2.342522621154785, lr: 5.9999999999999995e-05
Step 1264, training loss: 5.1023783683776855, total_norm: 2.6261322498321533, lr: 5.9999999999999995e-05
Step 1265, training loss: 5.190300464630127, total_norm: 3.359931468963623, lr: 5.9999999999999995e-05
Step 1266, training loss: 5.498557090759277, total_norm: 2.6073720455169678, lr: 5.9999999999999995e-05
Step 1267, training loss: 5.341676712036133, total_norm: 2.8602941036224365, lr: 5.9999999999999995e-05
Step 1268, training loss: 5.012791633605957, total_norm: 2.436906576156616, lr: 5.9999999999999995e-05
Step 1269, training loss: 4.988328456878662, total_norm: 2.2710518836975098, lr: 5.9999999999999995e-05
Step 1270, training loss: 5.141544818878174, total_norm: 2.695178747177124, lr: 5.9999999999999995e-05
Step 1271, training loss: 4.829687595367432, total_norm: 3.0026297569274902, lr: 5.9999999999999995e-05
Step 1272, training loss: 4.750840187072754, total_norm: 2.7508435249328613, lr: 5.9999999999999995e-05
Step 1273, training loss: 4.800287246704102, total_norm: 2.646156072616577, lr: 5.9999999999999995e-05
Step 1274, training loss: 5.021808624267578, total_norm: 2.936411142349243, lr: 5.9999999999999995e-05
Step 1275, training loss: 5.197543144226074, total_norm: 2.6450083255767822, lr: 5.9999999999999995e-05
Step 1276, training loss: 4.980862617492676, total_norm: 2.460998773574829, lr: 5.9999999999999995e-05
Step 1277, training loss: 5.246352672576904, total_norm: 2.5375170707702637, lr: 5.9999999999999995e-05
Step 1278, training loss: 5.45441198348999, total_norm: 2.7731664180755615, lr: 5.9999999999999995e-05
Step 1279, training loss: 5.456253528594971, total_norm: 2.4791805744171143, lr: 5.9999999999999995e-05
Step 1280, training loss: 5.398635387420654, total_norm: 2.646294355392456, lr: 5.9999999999999995e-05
Step 1281, training loss: 5.405466556549072, total_norm: 2.639418125152588, lr: 5.9999999999999995e-05
Step 1282, training loss: 4.694286346435547, total_norm: 2.5708229541778564, lr: 5.9999999999999995e-05
Step 1283, training loss: 4.980223655700684, total_norm: 2.7647595405578613, lr: 5.9999999999999995e-05
Step 1284, training loss: 5.401646614074707, total_norm: 2.4395220279693604, lr: 5.9999999999999995e-05
Step 1285, training loss: 5.031818389892578, total_norm: 2.2819817066192627, lr: 5.9999999999999995e-05
Step 1286, training loss: 5.263829708099365, total_norm: 2.546572685241699, lr: 5.9999999999999995e-05
Step 1287, training loss: 4.237816333770752, total_norm: 3.0891520977020264, lr: 5.9999999999999995e-05
Step 1288, training loss: 5.136528968811035, total_norm: 2.793487787246704, lr: 5.9999999999999995e-05
Step 1289, training loss: 5.403570652008057, total_norm: 2.8838250637054443, lr: 5.9999999999999995e-05
Step 1290, training loss: 4.929010391235352, total_norm: 2.5892951488494873, lr: 5.9999999999999995e-05
Step 1291, training loss: 4.695550441741943, total_norm: 2.91335129737854, lr: 5.9999999999999995e-05
Step 1292, training loss: 5.017864227294922, total_norm: 2.540144443511963, lr: 5.9999999999999995e-05
Step 1293, training loss: 5.039992809295654, total_norm: 2.768073797225952, lr: 5.9999999999999995e-05
Step 1294, training loss: 4.983257293701172, total_norm: 2.483689785003662, lr: 5.9999999999999995e-05
Step 1295, training loss: 4.693938255310059, total_norm: 2.7351603507995605, lr: 5.9999999999999995e-05
Step 1296, training loss: 5.160110950469971, total_norm: 2.3496015071868896, lr: 5.9999999999999995e-05
Step 1297, training loss: 4.612398147583008, total_norm: 2.5955417156219482, lr: 5.9999999999999995e-05
Step 1298, training loss: 4.067907333374023, total_norm: 2.7482128143310547, lr: 5.9999999999999995e-05
Step 1299, training loss: 5.103367805480957, total_norm: 2.3087880611419678, lr: 5.9999999999999995e-05
Step 1300, training loss: 5.212830066680908, total_norm: 2.385733127593994, lr: 5.9999999999999995e-05
Step 1300, validation loss: 5.766376495361328
Step 1301, training loss: 4.6476149559021, total_norm: 2.629838466644287, lr: 5.9999999999999995e-05
Step 1302, training loss: 5.183835029602051, total_norm: 2.7377779483795166, lr: 5.9999999999999995e-05
Step 1303, training loss: 4.0470685958862305, total_norm: 3.258965253829956, lr: 5.9999999999999995e-05
Step 1304, training loss: 3.9353394508361816, total_norm: 3.3646585941314697, lr: 5.9999999999999995e-05
Step 1305, training loss: 4.459263801574707, total_norm: 2.3401150703430176, lr: 5.9999999999999995e-05
Step 1306, training loss: 3.8698105812072754, total_norm: 3.4269416332244873, lr: 5.9999999999999995e-05
Step 1307, training loss: 4.817513465881348, total_norm: 2.5410451889038086, lr: 5.9999999999999995e-05
Step 1308, training loss: 4.646327972412109, total_norm: 2.7880640029907227, lr: 5.9999999999999995e-05
Step 1309, training loss: 4.491298675537109, total_norm: 2.3905739784240723, lr: 5.9999999999999995e-05
Step 1310, training loss: 5.3162407875061035, total_norm: 2.6267311573028564, lr: 5.9999999999999995e-05
Step 1311, training loss: 5.458191871643066, total_norm: 2.6585845947265625, lr: 5.9999999999999995e-05
Step 1312, training loss: 4.908758163452148, total_norm: 2.7949745655059814, lr: 5.9999999999999995e-05
Step 1313, training loss: 4.941652774810791, total_norm: 2.632700204849243, lr: 5.9999999999999995e-05
Step 1314, training loss: 5.24373197555542, total_norm: 3.0003628730773926, lr: 5.9999999999999995e-05
Step 1315, training loss: 5.064033508300781, total_norm: 4.376560211181641, lr: 5.9999999999999995e-05
Step 1316, training loss: 5.272827625274658, total_norm: 2.9476840496063232, lr: 5.9999999999999995e-05
Step 1317, training loss: 5.533688068389893, total_norm: 2.8405065536499023, lr: 5.9999999999999995e-05
Step 1318, training loss: 4.935936450958252, total_norm: 4.257774829864502, lr: 5.9999999999999995e-05
Step 1319, training loss: 5.023513317108154, total_norm: 3.271303415298462, lr: 5.9999999999999995e-05
Step 1320, training loss: 5.266856670379639, total_norm: 2.971167802810669, lr: 5.9999999999999995e-05
Step 1321, training loss: 5.689456939697266, total_norm: 2.3998677730560303, lr: 5.9999999999999995e-05
Step 1322, training loss: 5.282388210296631, total_norm: 3.7350354194641113, lr: 5.9999999999999995e-05
Step 1323, training loss: 5.052581310272217, total_norm: 2.8449318408966064, lr: 5.9999999999999995e-05
Step 1324, training loss: 5.423739433288574, total_norm: 2.7767345905303955, lr: 5.9999999999999995e-05
Step 1325, training loss: 5.10650634765625, total_norm: 2.4904699325561523, lr: 5.9999999999999995e-05
Step 1326, training loss: 5.0971879959106445, total_norm: 2.3871374130249023, lr: 5.9999999999999995e-05
Step 1327, training loss: 4.84622859954834, total_norm: 2.79398512840271, lr: 5.9999999999999995e-05
Step 1328, training loss: 6.040035247802734, total_norm: 2.7526466846466064, lr: 5.9999999999999995e-05
Step 1329, training loss: 5.427135944366455, total_norm: 2.540431499481201, lr: 5.9999999999999995e-05
Step 1330, training loss: 5.088345527648926, total_norm: 2.5131359100341797, lr: 5.9999999999999995e-05
Step 1331, training loss: 4.886645317077637, total_norm: 3.122518301010132, lr: 5.9999999999999995e-05
Step 1332, training loss: 4.521758556365967, total_norm: 2.440873384475708, lr: 5.9999999999999995e-05
Step 1333, training loss: 4.81598424911499, total_norm: 2.714826822280884, lr: 5.9999999999999995e-05
Step 1334, training loss: 5.427168369293213, total_norm: 2.751307249069214, lr: 5.9999999999999995e-05
Step 1335, training loss: 4.99045991897583, total_norm: 2.3791708946228027, lr: 5.9999999999999995e-05
Step 1336, training loss: 4.67608642578125, total_norm: 2.5605356693267822, lr: 5.9999999999999995e-05
Step 1337, training loss: 4.910179615020752, total_norm: 2.4017281532287598, lr: 5.9999999999999995e-05
Step 1338, training loss: 4.5120134353637695, total_norm: 2.5341947078704834, lr: 5.9999999999999995e-05
Step 1339, training loss: 4.563196659088135, total_norm: 2.567498207092285, lr: 5.9999999999999995e-05
Step 1340, training loss: 5.940361976623535, total_norm: 2.3527417182922363, lr: 5.9999999999999995e-05
Step 1341, training loss: 5.091121673583984, total_norm: 2.4247019290924072, lr: 5.9999999999999995e-05
Step 1342, training loss: 4.655126571655273, total_norm: 2.7084414958953857, lr: 5.9999999999999995e-05
Step 1343, training loss: 5.822601318359375, total_norm: 3.6678218841552734, lr: 5.9999999999999995e-05
Step 1344, training loss: 4.671586513519287, total_norm: 3.484718084335327, lr: 5.9999999999999995e-05
Step 1345, training loss: 4.461649417877197, total_norm: 2.778169631958008, lr: 5.9999999999999995e-05
Step 1346, training loss: 4.995611667633057, total_norm: 2.650975465774536, lr: 5.9999999999999995e-05
Step 1347, training loss: 4.669118881225586, total_norm: 2.3593995571136475, lr: 5.9999999999999995e-05
Step 1348, training loss: 5.159090518951416, total_norm: 2.4776461124420166, lr: 5.9999999999999995e-05
Step 1349, training loss: 4.637073516845703, total_norm: 2.235812187194824, lr: 5.9999999999999995e-05
Step 1350, training loss: 5.0364227294921875, total_norm: 2.8177661895751953, lr: 5.9999999999999995e-05
Step 1350, validation loss: 5.753508567810059
Step 1351, training loss: 4.797039031982422, total_norm: 2.9244587421417236, lr: 5.9999999999999995e-05
Step 1352, training loss: 5.2751054763793945, total_norm: 2.7624387741088867, lr: 5.9999999999999995e-05
Step 1353, training loss: 4.766855239868164, total_norm: 3.1394858360290527, lr: 5.9999999999999995e-05
Step 1354, training loss: 4.91096305847168, total_norm: 2.376307487487793, lr: 5.9999999999999995e-05
Step 1355, training loss: 4.654439926147461, total_norm: 2.86688232421875, lr: 5.9999999999999995e-05
Step 1356, training loss: 5.674351692199707, total_norm: 3.8198678493499756, lr: 5.9999999999999995e-05
Step 1357, training loss: 5.23907470703125, total_norm: 3.1579761505126953, lr: 5.9999999999999995e-05
Step 1358, training loss: 4.890244960784912, total_norm: 2.7212162017822266, lr: 5.9999999999999995e-05
Step 1359, training loss: 5.316507816314697, total_norm: 2.574718952178955, lr: 5.9999999999999995e-05
Step 1360, training loss: 5.07474946975708, total_norm: 2.4546444416046143, lr: 5.9999999999999995e-05
Step 1361, training loss: 5.096069812774658, total_norm: 2.4723100662231445, lr: 5.9999999999999995e-05
Step 1362, training loss: 4.969204425811768, total_norm: 2.562108278274536, lr: 5.9999999999999995e-05
Step 1363, training loss: 4.6870927810668945, total_norm: 2.452902317047119, lr: 5.9999999999999995e-05
Step 1364, training loss: 5.311906814575195, total_norm: 2.411196708679199, lr: 5.9999999999999995e-05
Step 1365, training loss: 4.644038677215576, total_norm: 2.4531285762786865, lr: 5.9999999999999995e-05
Step 1366, training loss: 4.873885631561279, total_norm: 2.241234540939331, lr: 5.9999999999999995e-05
Step 1367, training loss: 4.567554473876953, total_norm: 2.2137503623962402, lr: 5.9999999999999995e-05
Step 1368, training loss: 4.12746000289917, total_norm: 2.854752779006958, lr: 5.9999999999999995e-05
Step 1369, training loss: 4.602120876312256, total_norm: 2.482923746109009, lr: 5.9999999999999995e-05
Step 1370, training loss: 4.906278610229492, total_norm: 2.4510715007781982, lr: 5.9999999999999995e-05
Step 1371, training loss: 4.766952037811279, total_norm: 2.4013235569000244, lr: 5.9999999999999995e-05
Step 1372, training loss: 4.4355010986328125, total_norm: 2.170910358428955, lr: 5.9999999999999995e-05
Step 1373, training loss: 4.386586666107178, total_norm: 3.583176374435425, lr: 5.9999999999999995e-05
Step 1374, training loss: 4.3890557289123535, total_norm: 2.261314868927002, lr: 5.9999999999999995e-05
Step 1375, training loss: 5.3882646560668945, total_norm: 2.6942434310913086, lr: 5.9999999999999995e-05
Step 1376, training loss: 4.733293056488037, total_norm: 2.3670594692230225, lr: 5.9999999999999995e-05
Step 1377, training loss: 4.51165771484375, total_norm: 2.579124927520752, lr: 5.9999999999999995e-05
Step 1378, training loss: 4.8250017166137695, total_norm: 2.4875874519348145, lr: 5.9999999999999995e-05
Step 1379, training loss: 4.770318031311035, total_norm: 2.8318533897399902, lr: 5.9999999999999995e-05
Step 1380, training loss: 4.566344738006592, total_norm: 3.337315082550049, lr: 5.9999999999999995e-05
Step 1381, training loss: 4.771876811981201, total_norm: 3.192441701889038, lr: 5.9999999999999995e-05
Step 1382, training loss: 5.113654613494873, total_norm: 2.5548014640808105, lr: 5.9999999999999995e-05
Step 1383, training loss: 5.418283462524414, total_norm: 3.2974178791046143, lr: 5.9999999999999995e-05
Step 1384, training loss: 5.104310512542725, total_norm: 2.6277413368225098, lr: 5.9999999999999995e-05
Step 1385, training loss: 5.692240238189697, total_norm: 2.6596384048461914, lr: 5.9999999999999995e-05
Step 1386, training loss: 4.8909149169921875, total_norm: 3.5086538791656494, lr: 5.9999999999999995e-05
Step 1387, training loss: 5.3473219871521, total_norm: 2.1504390239715576, lr: 5.9999999999999995e-05
Step 1388, training loss: 4.810144901275635, total_norm: 2.195997714996338, lr: 5.9999999999999995e-05
Step 1389, training loss: 5.416687965393066, total_norm: 2.3869450092315674, lr: 5.9999999999999995e-05
Step 1390, training loss: 4.61598014831543, total_norm: 2.9307687282562256, lr: 5.9999999999999995e-05
Step 1391, training loss: 4.820942401885986, total_norm: 2.9046194553375244, lr: 5.9999999999999995e-05
Step 1392, training loss: 4.814029216766357, total_norm: 2.5793323516845703, lr: 5.9999999999999995e-05
Step 1393, training loss: 5.1846113204956055, total_norm: 2.430513620376587, lr: 5.9999999999999995e-05
Step 1394, training loss: 4.842947483062744, total_norm: 2.3436176776885986, lr: 5.9999999999999995e-05
Step 1395, training loss: 5.1031341552734375, total_norm: 2.6701927185058594, lr: 5.9999999999999995e-05
Step 1396, training loss: 5.2371907234191895, total_norm: 2.817491054534912, lr: 5.9999999999999995e-05
Step 1397, training loss: 4.8469624519348145, total_norm: 2.777615785598755, lr: 5.9999999999999995e-05
Step 1398, training loss: 4.81049919128418, total_norm: 2.8202054500579834, lr: 5.9999999999999995e-05
Step 1399, training loss: 4.800060272216797, total_norm: 2.7589712142944336, lr: 5.9999999999999995e-05
Step 1400, training loss: 4.763214111328125, total_norm: 3.6562728881835938, lr: 5.9999999999999995e-05
Step 1400, validation loss: 5.755570411682129
Step 1401, training loss: 4.772189140319824, total_norm: 2.5766758918762207, lr: 5.9999999999999995e-05
Step 1402, training loss: 4.523040771484375, total_norm: 2.50935959815979, lr: 5.9999999999999995e-05
Step 1403, training loss: 4.794281482696533, total_norm: 2.6739814281463623, lr: 5.9999999999999995e-05
Step 1404, training loss: 4.902599334716797, total_norm: 2.613348960876465, lr: 5.9999999999999995e-05
Step 1405, training loss: 4.919300079345703, total_norm: 3.0294313430786133, lr: 5.9999999999999995e-05
Step 1406, training loss: 5.497147083282471, total_norm: 3.05690336227417, lr: 5.9999999999999995e-05
Step 1407, training loss: 5.15868616104126, total_norm: 2.7265658378601074, lr: 5.9999999999999995e-05
Step 1408, training loss: 5.072180271148682, total_norm: 3.1310713291168213, lr: 5.9999999999999995e-05
Step 1409, training loss: 5.503221035003662, total_norm: 3.0495822429656982, lr: 5.9999999999999995e-05
Step 1410, training loss: 4.913759231567383, total_norm: 2.4615280628204346, lr: 5.9999999999999995e-05
Step 1411, training loss: 5.390801906585693, total_norm: 2.646865129470825, lr: 5.9999999999999995e-05
Step 1412, training loss: 5.2742719650268555, total_norm: 2.722097396850586, lr: 5.9999999999999995e-05
Step 1413, training loss: 5.280059814453125, total_norm: 2.5602922439575195, lr: 5.9999999999999995e-05
Step 1414, training loss: 5.046548843383789, total_norm: 2.405762195587158, lr: 5.9999999999999995e-05
Step 1415, training loss: 5.177257537841797, total_norm: 2.886963129043579, lr: 5.9999999999999995e-05
Step 1416, training loss: 5.261783599853516, total_norm: 2.757416248321533, lr: 5.9999999999999995e-05
Step 1417, training loss: 4.713881969451904, total_norm: 2.860246419906616, lr: 5.9999999999999995e-05
Step 1418, training loss: 4.405768871307373, total_norm: 2.778451442718506, lr: 5.9999999999999995e-05
Step 1419, training loss: 4.62694787979126, total_norm: 2.6589267253875732, lr: 5.9999999999999995e-05
Step 1420, training loss: 5.0279622077941895, total_norm: 2.8963968753814697, lr: 5.9999999999999995e-05
Step 1421, training loss: 4.995530605316162, total_norm: 3.109922409057617, lr: 5.9999999999999995e-05
Step 1422, training loss: 5.043735504150391, total_norm: 2.826455593109131, lr: 5.9999999999999995e-05
Step 1423, training loss: 5.600900650024414, total_norm: 2.8857877254486084, lr: 5.9999999999999995e-05
Step 1424, training loss: 4.988590717315674, total_norm: 2.736902952194214, lr: 5.9999999999999995e-05
Step 1425, training loss: 4.730292320251465, total_norm: 2.6437628269195557, lr: 5.9999999999999995e-05
Step 1426, training loss: 5.2636027336120605, total_norm: 2.57963228225708, lr: 5.9999999999999995e-05
Step 1427, training loss: 4.808865070343018, total_norm: 2.5937271118164062, lr: 5.9999999999999995e-05
Step 1428, training loss: 5.021247863769531, total_norm: 2.6597015857696533, lr: 5.9999999999999995e-05
Step 1429, training loss: 4.954565525054932, total_norm: 2.6051132678985596, lr: 5.9999999999999995e-05
Step 1430, training loss: 4.386048793792725, total_norm: 3.0910723209381104, lr: 5.9999999999999995e-05
Step 1431, training loss: 4.4891462326049805, total_norm: 3.169234275817871, lr: 5.9999999999999995e-05
Step 1432, training loss: 3.9912803173065186, total_norm: 5.048015117645264, lr: 5.9999999999999995e-05
Step 1433, training loss: 4.3111467361450195, total_norm: 3.7928848266601562, lr: 5.9999999999999995e-05
Step 1434, training loss: 4.354274749755859, total_norm: 3.2412667274475098, lr: 5.9999999999999995e-05
Step 1435, training loss: 5.283355712890625, total_norm: 3.3037564754486084, lr: 5.9999999999999995e-05
Step 1436, training loss: 5.352376937866211, total_norm: 3.3495593070983887, lr: 5.9999999999999995e-05
Step 1437, training loss: 4.86295747756958, total_norm: 2.989318609237671, lr: 5.9999999999999995e-05
Step 1438, training loss: 5.233364105224609, total_norm: 3.010770320892334, lr: 5.9999999999999995e-05
Step 1439, training loss: 4.973204135894775, total_norm: 3.1659433841705322, lr: 5.9999999999999995e-05
Step 1440, training loss: 4.8892951011657715, total_norm: 3.400723457336426, lr: 5.9999999999999995e-05
Step 1441, training loss: 4.9993205070495605, total_norm: 2.6805388927459717, lr: 5.9999999999999995e-05
Step 1442, training loss: 4.656116962432861, total_norm: 2.7268965244293213, lr: 5.9999999999999995e-05
Step 1443, training loss: 4.588489055633545, total_norm: 2.690540313720703, lr: 5.9999999999999995e-05
Step 1444, training loss: 4.845329761505127, total_norm: 2.6754379272460938, lr: 5.9999999999999995e-05
Step 1445, training loss: 4.6522369384765625, total_norm: 2.3693923950195312, lr: 5.9999999999999995e-05
Step 1446, training loss: 4.8912153244018555, total_norm: 2.409935474395752, lr: 5.9999999999999995e-05
Step 1447, training loss: 4.844453811645508, total_norm: 3.1477725505828857, lr: 5.9999999999999995e-05
Step 1448, training loss: 4.665043354034424, total_norm: 3.299619436264038, lr: 5.9999999999999995e-05
Step 1449, training loss: 4.581908702850342, total_norm: 2.6267740726470947, lr: 5.9999999999999995e-05
Step 1450, training loss: 4.890068054199219, total_norm: 2.649942636489868, lr: 5.9999999999999995e-05
Step 1450, validation loss: 5.822644233703613
Step 1451, training loss: 4.60905647277832, total_norm: 2.372150421142578, lr: 5.9999999999999995e-05
Step 1452, training loss: 4.582982063293457, total_norm: 2.3815104961395264, lr: 5.9999999999999995e-05
Step 1453, training loss: 4.861475467681885, total_norm: 2.3158979415893555, lr: 5.9999999999999995e-05
Step 1454, training loss: 4.5215277671813965, total_norm: 2.591846466064453, lr: 5.9999999999999995e-05
Step 1455, training loss: 4.535852432250977, total_norm: 2.6860435009002686, lr: 5.9999999999999995e-05
Step 1456, training loss: 4.918743133544922, total_norm: 2.4566361904144287, lr: 5.9999999999999995e-05
Step 1457, training loss: 5.057224273681641, total_norm: 2.5820705890655518, lr: 5.9999999999999995e-05
Step 1458, training loss: 4.442798137664795, total_norm: 2.5678625106811523, lr: 5.9999999999999995e-05
Step 1459, training loss: 4.349310874938965, total_norm: 2.26129150390625, lr: 5.9999999999999995e-05
Step 1460, training loss: 4.589910984039307, total_norm: 2.3391005992889404, lr: 5.9999999999999995e-05
Step 1461, training loss: 4.875721454620361, total_norm: 2.7931642532348633, lr: 5.9999999999999995e-05
Step 1462, training loss: 4.865798473358154, total_norm: 2.4020915031433105, lr: 5.9999999999999995e-05
Step 1463, training loss: 5.42820405960083, total_norm: 2.619352102279663, lr: 5.9999999999999995e-05
Step 1464, training loss: 4.864119529724121, total_norm: 2.6731314659118652, lr: 5.9999999999999995e-05
Step 1465, training loss: 4.528347015380859, total_norm: 2.466697931289673, lr: 5.9999999999999995e-05
Step 1466, training loss: 3.997251272201538, total_norm: 2.8766865730285645, lr: 5.9999999999999995e-05
Step 1467, training loss: 4.201071739196777, total_norm: 2.5116703510284424, lr: 5.9999999999999995e-05
Step 1468, training loss: 4.663407802581787, total_norm: 2.3822014331817627, lr: 5.9999999999999995e-05
Step 1469, training loss: 4.945710182189941, total_norm: 2.367225408554077, lr: 5.9999999999999995e-05
Step 1470, training loss: 5.291579723358154, total_norm: 2.788172483444214, lr: 5.9999999999999995e-05
Step 1471, training loss: 5.137656211853027, total_norm: 2.71406626701355, lr: 5.9999999999999995e-05
Step 1472, training loss: 5.995690822601318, total_norm: 3.3019001483917236, lr: 5.9999999999999995e-05
Step 1473, training loss: 5.276671409606934, total_norm: 3.6565160751342773, lr: 5.9999999999999995e-05
Step 1474, training loss: 5.377155303955078, total_norm: 3.5784783363342285, lr: 5.9999999999999995e-05
Step 1475, training loss: 5.263617992401123, total_norm: 2.94610333442688, lr: 5.9999999999999995e-05
Step 1476, training loss: 5.510304927825928, total_norm: 2.7690868377685547, lr: 5.9999999999999995e-05
Step 1477, training loss: 5.1837382316589355, total_norm: 2.9394006729125977, lr: 5.9999999999999995e-05
Step 1478, training loss: 5.341029167175293, total_norm: 3.1590285301208496, lr: 5.9999999999999995e-05
Step 1479, training loss: 5.2429070472717285, total_norm: 3.0792951583862305, lr: 5.9999999999999995e-05
Step 1480, training loss: 5.55661678314209, total_norm: 2.8769774436950684, lr: 5.9999999999999995e-05
Step 1481, training loss: 5.2831525802612305, total_norm: 2.728508710861206, lr: 5.9999999999999995e-05
Step 1482, training loss: 4.9362640380859375, total_norm: 2.7001380920410156, lr: 5.9999999999999995e-05
Step 1483, training loss: 4.877964973449707, total_norm: 2.7753446102142334, lr: 5.9999999999999995e-05
Step 1484, training loss: 5.257894039154053, total_norm: 2.997385263442993, lr: 5.9999999999999995e-05
Step 1485, training loss: 4.6773176193237305, total_norm: 3.653224468231201, lr: 5.9999999999999995e-05
Step 1486, training loss: 4.947661876678467, total_norm: 2.632016897201538, lr: 5.9999999999999995e-05
Step 1487, training loss: 5.269521713256836, total_norm: 2.555504322052002, lr: 5.9999999999999995e-05
Step 1488, training loss: 4.954279899597168, total_norm: 2.29620099067688, lr: 5.9999999999999995e-05
Step 1489, training loss: 5.03719425201416, total_norm: 2.538623332977295, lr: 5.9999999999999995e-05
Step 1490, training loss: 5.011104583740234, total_norm: 2.5096001625061035, lr: 5.9999999999999995e-05
Step 1491, training loss: 5.277950286865234, total_norm: 2.7703697681427, lr: 5.9999999999999995e-05
Step 1492, training loss: 4.926836967468262, total_norm: 2.3711259365081787, lr: 5.9999999999999995e-05
Step 1493, training loss: 4.6626691818237305, total_norm: 2.590118646621704, lr: 5.9999999999999995e-05
Step 1494, training loss: 4.669164180755615, total_norm: 2.642946720123291, lr: 5.9999999999999995e-05
Step 1495, training loss: 4.931914806365967, total_norm: 2.84018874168396, lr: 5.9999999999999995e-05
Step 1496, training loss: 4.771845817565918, total_norm: 2.432612180709839, lr: 5.9999999999999995e-05
Step 1497, training loss: 5.149803638458252, total_norm: 2.503021001815796, lr: 5.9999999999999995e-05
Step 1498, training loss: 5.282413005828857, total_norm: 3.009223461151123, lr: 5.9999999999999995e-05
Step 1499, training loss: 5.12327241897583, total_norm: 2.5524630546569824, lr: 5.9999999999999995e-05
Step 1500, training loss: 5.155415058135986, total_norm: 2.3099403381347656, lr: 5.9999999999999995e-05
Step 1500, validation loss: 5.736114501953125
Step 1501, training loss: 4.594031810760498, total_norm: 3.0564286708831787, lr: 5.9999999999999995e-05
Step 1502, training loss: 5.533993244171143, total_norm: 2.5533547401428223, lr: 5.9999999999999995e-05
Step 1503, training loss: 5.139138221740723, total_norm: 2.402963161468506, lr: 5.9999999999999995e-05
Step 1504, training loss: 4.9084601402282715, total_norm: 2.742593765258789, lr: 5.9999999999999995e-05
Step 1505, training loss: 5.779386043548584, total_norm: 2.560204029083252, lr: 5.9999999999999995e-05
Step 1506, training loss: 5.982234954833984, total_norm: 3.6194143295288086, lr: 5.9999999999999995e-05
Step 1507, training loss: 5.362377166748047, total_norm: 2.95971417427063, lr: 5.9999999999999995e-05
Step 1508, training loss: 5.480232238769531, total_norm: 2.552647352218628, lr: 5.9999999999999995e-05
Step 1509, training loss: 5.6183648109436035, total_norm: 2.7341501712799072, lr: 5.9999999999999995e-05
Step 1510, training loss: 5.78383731842041, total_norm: 2.6816110610961914, lr: 5.9999999999999995e-05
Step 1511, training loss: 5.464351177215576, total_norm: 2.675105333328247, lr: 5.9999999999999995e-05
Step 1512, training loss: 5.064552307128906, total_norm: 2.749673366546631, lr: 5.9999999999999995e-05
Step 1513, training loss: 5.169304847717285, total_norm: 2.873149871826172, lr: 5.9999999999999995e-05
Step 1514, training loss: 5.479708194732666, total_norm: 3.4344019889831543, lr: 5.9999999999999995e-05
Step 1515, training loss: 5.328708648681641, total_norm: 3.121218204498291, lr: 5.9999999999999995e-05
Step 1516, training loss: 5.5751848220825195, total_norm: 2.448777914047241, lr: 5.9999999999999995e-05
Step 1517, training loss: 4.904973983764648, total_norm: 2.4383091926574707, lr: 5.9999999999999995e-05
Step 1518, training loss: 5.6789374351501465, total_norm: 2.954150915145874, lr: 5.9999999999999995e-05
Step 1519, training loss: 5.5858073234558105, total_norm: 3.1630821228027344, lr: 5.9999999999999995e-05
Step 1520, training loss: 5.2024664878845215, total_norm: 3.3126251697540283, lr: 5.9999999999999995e-05
Step 1521, training loss: 4.9194746017456055, total_norm: 3.7359845638275146, lr: 5.9999999999999995e-05
Step 1522, training loss: 5.3694610595703125, total_norm: 2.5925133228302, lr: 5.9999999999999995e-05
Step 1523, training loss: 4.6197509765625, total_norm: 2.5601134300231934, lr: 5.9999999999999995e-05
Step 1524, training loss: 4.693699836730957, total_norm: 2.768035888671875, lr: 5.9999999999999995e-05
Step 1525, training loss: 5.247530460357666, total_norm: 2.419086456298828, lr: 5.9999999999999995e-05
Step 1526, training loss: 4.840797424316406, total_norm: 2.56477689743042, lr: 5.9999999999999995e-05
Step 1527, training loss: 4.865076065063477, total_norm: 2.83259654045105, lr: 5.9999999999999995e-05
Step 1528, training loss: 4.953300952911377, total_norm: 2.459122657775879, lr: 5.9999999999999995e-05
Step 1529, training loss: 5.495831489562988, total_norm: 2.7085113525390625, lr: 5.9999999999999995e-05
Step 1530, training loss: 4.620568752288818, total_norm: 3.018080234527588, lr: 5.9999999999999995e-05
Step 1531, training loss: 4.844783306121826, total_norm: 2.808577299118042, lr: 5.9999999999999995e-05
Step 1532, training loss: 4.74100399017334, total_norm: 2.8933944702148438, lr: 5.9999999999999995e-05
Step 1533, training loss: 4.8148298263549805, total_norm: 3.184506893157959, lr: 5.9999999999999995e-05
Step 1534, training loss: 5.591027736663818, total_norm: 2.788888454437256, lr: 5.9999999999999995e-05
Step 1535, training loss: 4.783269882202148, total_norm: 2.578834295272827, lr: 5.9999999999999995e-05
Step 1536, training loss: 5.043894290924072, total_norm: 2.624654531478882, lr: 5.9999999999999995e-05
Step 1537, training loss: 4.79038143157959, total_norm: 3.2714638710021973, lr: 5.9999999999999995e-05
Step 1538, training loss: 4.928546905517578, total_norm: 2.599325180053711, lr: 5.9999999999999995e-05
Step 1539, training loss: 5.185914039611816, total_norm: 2.3054282665252686, lr: 5.9999999999999995e-05
Step 1540, training loss: 5.072020053863525, total_norm: 2.4923362731933594, lr: 5.9999999999999995e-05
Step 1541, training loss: 4.641766548156738, total_norm: 2.3826346397399902, lr: 5.9999999999999995e-05
Step 1542, training loss: 5.507460594177246, total_norm: 2.600259780883789, lr: 5.9999999999999995e-05
Step 1543, training loss: 5.844027519226074, total_norm: 2.735724925994873, lr: 5.9999999999999995e-05
Step 1544, training loss: 5.708878993988037, total_norm: 2.5248594284057617, lr: 5.9999999999999995e-05
Step 1545, training loss: 4.879476547241211, total_norm: 2.649200201034546, lr: 5.9999999999999995e-05
Step 1546, training loss: 4.524198532104492, total_norm: 2.556915760040283, lr: 5.9999999999999995e-05
Step 1547, training loss: 4.984346389770508, total_norm: 2.8561081886291504, lr: 5.9999999999999995e-05
Step 1548, training loss: 4.598454475402832, total_norm: 2.4626219272613525, lr: 5.9999999999999995e-05
Step 1549, training loss: 4.836972236633301, total_norm: 2.721338987350464, lr: 5.9999999999999995e-05
Step 1550, training loss: 5.271306037902832, total_norm: 2.8533666133880615, lr: 5.9999999999999995e-05
Step 1550, validation loss: 5.667533874511719
Step 1551, training loss: 5.463072776794434, total_norm: 3.2626326084136963, lr: 5.9999999999999995e-05
Step 1552, training loss: 5.039774417877197, total_norm: 3.6499547958374023, lr: 5.9999999999999995e-05
Step 1553, training loss: 4.996338367462158, total_norm: 3.0687036514282227, lr: 5.9999999999999995e-05
Step 1554, training loss: 5.198013782501221, total_norm: 4.900331974029541, lr: 5.9999999999999995e-05
Step 1555, training loss: 5.392379283905029, total_norm: 3.784736156463623, lr: 5.9999999999999995e-05
Step 1556, training loss: 5.340795040130615, total_norm: 4.7479023933410645, lr: 5.9999999999999995e-05
Step 1557, training loss: 5.441667556762695, total_norm: 3.3304855823516846, lr: 5.9999999999999995e-05
Step 1558, training loss: 5.590517520904541, total_norm: 3.2443113327026367, lr: 5.9999999999999995e-05
Step 1559, training loss: 5.05871057510376, total_norm: 4.116443634033203, lr: 5.9999999999999995e-05
Step 1560, training loss: 5.309016227722168, total_norm: 3.320216655731201, lr: 5.9999999999999995e-05
Step 1561, training loss: 4.964752197265625, total_norm: 3.1893868446350098, lr: 5.9999999999999995e-05
Step 1562, training loss: 5.071773052215576, total_norm: 4.633618354797363, lr: 5.9999999999999995e-05
Step 1563, training loss: 4.861029624938965, total_norm: 5.107738971710205, lr: 5.9999999999999995e-05
Step 1564, training loss: 5.059849262237549, total_norm: 3.467752456665039, lr: 5.9999999999999995e-05
Step 1565, training loss: 4.505800724029541, total_norm: 3.192530393600464, lr: 5.9999999999999995e-05
Step 1566, training loss: 5.040040493011475, total_norm: 3.1722488403320312, lr: 5.9999999999999995e-05
Step 1567, training loss: 4.982089996337891, total_norm: 3.4261279106140137, lr: 5.9999999999999995e-05
Step 1568, training loss: 4.7705769538879395, total_norm: 3.048433780670166, lr: 5.9999999999999995e-05
Step 1569, training loss: 4.630640029907227, total_norm: 3.6263463497161865, lr: 5.9999999999999995e-05
Step 1570, training loss: 4.299426555633545, total_norm: 4.69991397857666, lr: 5.9999999999999995e-05
Step 1571, training loss: 4.75121545791626, total_norm: 3.249239206314087, lr: 5.9999999999999995e-05
Step 1572, training loss: 5.048798084259033, total_norm: 3.873568534851074, lr: 5.9999999999999995e-05
Step 1573, training loss: 4.983863830566406, total_norm: 3.300520658493042, lr: 5.9999999999999995e-05
Step 1574, training loss: 4.6531219482421875, total_norm: 3.74515700340271, lr: 5.9999999999999995e-05
Step 1575, training loss: 5.1189656257629395, total_norm: 3.099806547164917, lr: 5.9999999999999995e-05
Step 1576, training loss: 4.633514881134033, total_norm: 2.6372785568237305, lr: 5.9999999999999995e-05
Step 1577, training loss: 4.593513488769531, total_norm: 2.642873764038086, lr: 5.9999999999999995e-05
Step 1578, training loss: 4.66643762588501, total_norm: 2.611876964569092, lr: 5.9999999999999995e-05
Step 1579, training loss: 4.9342827796936035, total_norm: 2.3303143978118896, lr: 5.9999999999999995e-05
Step 1580, training loss: 5.455460548400879, total_norm: 2.961138963699341, lr: 5.9999999999999995e-05
Step 1581, training loss: 4.597903251647949, total_norm: 3.7329342365264893, lr: 5.9999999999999995e-05
Step 1582, training loss: 4.8618974685668945, total_norm: 3.238285779953003, lr: 5.9999999999999995e-05
Step 1583, training loss: 4.975386142730713, total_norm: 4.285926818847656, lr: 5.9999999999999995e-05
Step 1584, training loss: 5.2690629959106445, total_norm: 3.9121787548065186, lr: 5.9999999999999995e-05
Step 1585, training loss: 5.458566188812256, total_norm: 3.281928539276123, lr: 5.9999999999999995e-05
Step 1586, training loss: 5.026067733764648, total_norm: 3.405811071395874, lr: 5.9999999999999995e-05
Step 1587, training loss: 5.033167362213135, total_norm: 2.8246049880981445, lr: 5.9999999999999995e-05
Step 1588, training loss: 5.510228157043457, total_norm: 2.778862476348877, lr: 5.9999999999999995e-05
Step 1589, training loss: 5.211209774017334, total_norm: 3.550661563873291, lr: 5.9999999999999995e-05
Step 1590, training loss: 4.983749866485596, total_norm: 4.246668338775635, lr: 5.9999999999999995e-05
Step 1591, training loss: 5.263688087463379, total_norm: 3.3156991004943848, lr: 5.9999999999999995e-05
Step 1592, training loss: 5.376531600952148, total_norm: 3.060781717300415, lr: 5.9999999999999995e-05
Step 1593, training loss: 5.278831481933594, total_norm: 4.010789394378662, lr: 5.9999999999999995e-05
Step 1594, training loss: 4.9685378074646, total_norm: 3.347088098526001, lr: 5.9999999999999995e-05
Step 1595, training loss: 4.628317356109619, total_norm: 3.262385129928589, lr: 5.9999999999999995e-05
Step 1596, training loss: 5.188048839569092, total_norm: 2.4314796924591064, lr: 5.9999999999999995e-05
Step 1597, training loss: 4.847395420074463, total_norm: 2.803671360015869, lr: 5.9999999999999995e-05
Step 1598, training loss: 4.924344539642334, total_norm: 2.4848883152008057, lr: 5.9999999999999995e-05
Step 1599, training loss: 4.591538906097412, total_norm: 2.5641329288482666, lr: 5.9999999999999995e-05
Step 1600, training loss: 4.929337978363037, total_norm: 2.164935827255249, lr: 5.9999999999999995e-05
Step 1600, validation loss: 5.681558609008789
Step 1601, training loss: 4.8853936195373535, total_norm: 2.2501273155212402, lr: 5.9999999999999995e-05
Step 1602, training loss: 5.081836700439453, total_norm: 2.187020778656006, lr: 5.9999999999999995e-05
Step 1603, training loss: 4.398035049438477, total_norm: 2.514390468597412, lr: 5.9999999999999995e-05
Step 1604, training loss: 4.6533308029174805, total_norm: 2.36539888381958, lr: 5.9999999999999995e-05
Step 1605, training loss: 4.3920392990112305, total_norm: 2.626426935195923, lr: 5.9999999999999995e-05
Step 1606, training loss: 5.678971290588379, total_norm: 3.039945125579834, lr: 5.9999999999999995e-05
Step 1607, training loss: 5.090217590332031, total_norm: 2.8777425289154053, lr: 5.9999999999999995e-05
Step 1608, training loss: 4.617693901062012, total_norm: 2.8133997917175293, lr: 5.9999999999999995e-05
Step 1609, training loss: 4.607027053833008, total_norm: 2.7161014080047607, lr: 5.9999999999999995e-05
Step 1610, training loss: 5.0914626121521, total_norm: 2.664102077484131, lr: 5.9999999999999995e-05
Step 1611, training loss: 4.739195346832275, total_norm: 2.7064661979675293, lr: 5.9999999999999995e-05
Step 1612, training loss: 5.215203762054443, total_norm: 3.0941524505615234, lr: 5.9999999999999995e-05
Step 1613, training loss: 4.594912528991699, total_norm: 2.914987564086914, lr: 5.9999999999999995e-05
Step 1614, training loss: 5.270097732543945, total_norm: 2.4548301696777344, lr: 5.9999999999999995e-05
Step 1615, training loss: 4.432486534118652, total_norm: 2.466261625289917, lr: 5.9999999999999995e-05
Step 1616, training loss: 4.872506618499756, total_norm: 3.1915276050567627, lr: 5.9999999999999995e-05
Step 1617, training loss: 4.66808557510376, total_norm: 2.856694221496582, lr: 5.9999999999999995e-05
Step 1618, training loss: 4.065697193145752, total_norm: 3.068101406097412, lr: 5.9999999999999995e-05
Step 1619, training loss: 5.1035332679748535, total_norm: 2.788923501968384, lr: 5.9999999999999995e-05
Step 1620, training loss: 3.8812270164489746, total_norm: 2.458603858947754, lr: 5.9999999999999995e-05
Step 1621, training loss: 4.972269535064697, total_norm: 2.8801066875457764, lr: 5.9999999999999995e-05
Step 1622, training loss: 5.141579627990723, total_norm: 2.796576499938965, lr: 5.9999999999999995e-05
Step 1623, training loss: 4.1470417976379395, total_norm: 2.386981964111328, lr: 5.9999999999999995e-05
Step 1624, training loss: 3.7166976928710938, total_norm: 2.7843737602233887, lr: 5.9999999999999995e-05
Step 1625, training loss: 4.261104583740234, total_norm: 2.435196876525879, lr: 5.9999999999999995e-05
Step 1626, training loss: 4.645857334136963, total_norm: 2.5819644927978516, lr: 5.9999999999999995e-05
Step 1627, training loss: 5.098683834075928, total_norm: 2.5930495262145996, lr: 5.9999999999999995e-05
Step 1628, training loss: 4.325977802276611, total_norm: 2.773451805114746, lr: 5.9999999999999995e-05
Step 1629, training loss: 3.96911358833313, total_norm: 2.612522602081299, lr: 5.9999999999999995e-05
Step 1630, training loss: 3.9890642166137695, total_norm: 2.2684216499328613, lr: 5.9999999999999995e-05
Step 1631, training loss: 3.9156458377838135, total_norm: 2.5620615482330322, lr: 5.9999999999999995e-05
Step 1632, training loss: 4.583289623260498, total_norm: 2.466371774673462, lr: 5.9999999999999995e-05
Step 1633, training loss: 4.409444808959961, total_norm: 2.3313498497009277, lr: 5.9999999999999995e-05
Step 1634, training loss: 3.9159457683563232, total_norm: 2.586205005645752, lr: 5.9999999999999995e-05
Step 1635, training loss: 4.946427822113037, total_norm: 2.455781936645508, lr: 5.9999999999999995e-05
Step 1636, training loss: 4.664175033569336, total_norm: 2.5875895023345947, lr: 5.9999999999999995e-05
Step 1637, training loss: 4.247087001800537, total_norm: 2.1839828491210938, lr: 5.9999999999999995e-05
Step 1638, training loss: 4.116568565368652, total_norm: 2.2200820446014404, lr: 5.9999999999999995e-05
Step 1639, training loss: 4.226635456085205, total_norm: 2.382796049118042, lr: 5.9999999999999995e-05
Step 1640, training loss: 4.003398895263672, total_norm: 2.3382208347320557, lr: 5.9999999999999995e-05
Step 1641, training loss: 4.760909557342529, total_norm: 2.405618906021118, lr: 5.9999999999999995e-05
Step 1642, training loss: 4.9137067794799805, total_norm: 2.3930275440216064, lr: 5.9999999999999995e-05
Step 1643, training loss: 3.8294637203216553, total_norm: 2.4573326110839844, lr: 5.9999999999999995e-05
Step 1644, training loss: 3.9855237007141113, total_norm: 2.7036948204040527, lr: 5.9999999999999995e-05
Step 1645, training loss: 5.3074235916137695, total_norm: 3.3081040382385254, lr: 5.9999999999999995e-05
Step 1646, training loss: 4.8034443855285645, total_norm: 2.5890493392944336, lr: 5.9999999999999995e-05
Step 1647, training loss: 4.007410049438477, total_norm: 3.367591142654419, lr: 5.9999999999999995e-05
Step 1648, training loss: 4.192187786102295, total_norm: 3.210958242416382, lr: 5.9999999999999995e-05
Step 1649, training loss: 5.368558406829834, total_norm: 2.852680206298828, lr: 5.9999999999999995e-05
Step 1650, training loss: 5.183628559112549, total_norm: 2.635593891143799, lr: 5.9999999999999995e-05
Step 1650, validation loss: 5.652914047241211
Step 1651, training loss: 4.4362969398498535, total_norm: 3.339932918548584, lr: 5.9999999999999995e-05
Step 1652, training loss: 5.053134441375732, total_norm: 2.7565903663635254, lr: 5.9999999999999995e-05
Step 1653, training loss: 4.395144939422607, total_norm: 2.8191041946411133, lr: 5.9999999999999995e-05
Step 1654, training loss: 4.329931259155273, total_norm: 3.15028715133667, lr: 5.9999999999999995e-05
Step 1655, training loss: 4.198770523071289, total_norm: 2.4967751502990723, lr: 5.9999999999999995e-05
Step 1656, training loss: 4.534699440002441, total_norm: 2.5138843059539795, lr: 5.9999999999999995e-05
Step 1657, training loss: 4.288938045501709, total_norm: 2.3628182411193848, lr: 5.9999999999999995e-05
Step 1658, training loss: 4.850221157073975, total_norm: 2.5470926761627197, lr: 5.9999999999999995e-05
Step 1659, training loss: 4.810417175292969, total_norm: 2.5600321292877197, lr: 5.9999999999999995e-05
Step 1660, training loss: 4.409455299377441, total_norm: 2.255615472793579, lr: 5.9999999999999995e-05
Step 1661, training loss: 3.9626026153564453, total_norm: 2.658238649368286, lr: 5.9999999999999995e-05
Step 1662, training loss: 4.9347615242004395, total_norm: 2.830289840698242, lr: 5.9999999999999995e-05
Step 1663, training loss: 5.043395042419434, total_norm: 2.704340696334839, lr: 5.9999999999999995e-05
Step 1664, training loss: 4.587738037109375, total_norm: 2.312513589859009, lr: 5.9999999999999995e-05
Step 1665, training loss: 5.131047248840332, total_norm: 2.581801652908325, lr: 5.9999999999999995e-05
Step 1666, training loss: 4.595310211181641, total_norm: 2.4567506313323975, lr: 5.9999999999999995e-05
Step 1667, training loss: 5.1603875160217285, total_norm: 2.76214599609375, lr: 5.9999999999999995e-05
Step 1668, training loss: 5.319286823272705, total_norm: 2.882533073425293, lr: 5.9999999999999995e-05
Step 1669, training loss: 4.339576721191406, total_norm: 2.5443062782287598, lr: 5.9999999999999995e-05
Step 1670, training loss: 4.9333391189575195, total_norm: 2.63325572013855, lr: 5.9999999999999995e-05
Step 1671, training loss: 4.854067802429199, total_norm: 2.574510097503662, lr: 5.9999999999999995e-05
Step 1672, training loss: 4.753593921661377, total_norm: 2.567427635192871, lr: 5.9999999999999995e-05
Step 1673, training loss: 4.729553699493408, total_norm: 2.3592705726623535, lr: 5.9999999999999995e-05
Step 1674, training loss: 4.405488014221191, total_norm: 2.781456470489502, lr: 5.9999999999999995e-05
Step 1675, training loss: 4.7290425300598145, total_norm: 3.341550827026367, lr: 5.9999999999999995e-05
Step 1676, training loss: 5.792243480682373, total_norm: 3.0425033569335938, lr: 5.9999999999999995e-05
Step 1677, training loss: 5.230372905731201, total_norm: 3.3332598209381104, lr: 5.9999999999999995e-05
Step 1678, training loss: 5.121328830718994, total_norm: 3.494922399520874, lr: 5.9999999999999995e-05
Step 1679, training loss: 4.884998798370361, total_norm: 3.1109259128570557, lr: 5.9999999999999995e-05
Step 1680, training loss: 5.435207843780518, total_norm: 2.88419771194458, lr: 5.9999999999999995e-05
Step 1681, training loss: 5.194639682769775, total_norm: 2.7980668544769287, lr: 5.9999999999999995e-05
Step 1682, training loss: 4.6166887283325195, total_norm: 3.290234327316284, lr: 5.9999999999999995e-05
Step 1683, training loss: 4.290756702423096, total_norm: 3.4994711875915527, lr: 5.9999999999999995e-05
Step 1684, training loss: 4.459502220153809, total_norm: 3.164198637008667, lr: 5.9999999999999995e-05
Step 1685, training loss: 4.1981916427612305, total_norm: 3.0633082389831543, lr: 5.9999999999999995e-05
Step 1686, training loss: 4.732996940612793, total_norm: 2.605785369873047, lr: 5.9999999999999995e-05
Step 1687, training loss: 4.93403434753418, total_norm: 2.501875400543213, lr: 5.9999999999999995e-05
Step 1688, training loss: 4.786145210266113, total_norm: 3.096334218978882, lr: 5.9999999999999995e-05
Step 1689, training loss: 5.011003017425537, total_norm: 2.675827741622925, lr: 5.9999999999999995e-05
Step 1690, training loss: 4.559728622436523, total_norm: 2.670762777328491, lr: 5.9999999999999995e-05
Step 1691, training loss: 4.365120887756348, total_norm: 2.732935667037964, lr: 5.9999999999999995e-05
Step 1692, training loss: 4.661375045776367, total_norm: 2.5099005699157715, lr: 5.9999999999999995e-05
Step 1693, training loss: 4.7359771728515625, total_norm: 3.018352508544922, lr: 5.9999999999999995e-05
Step 1694, training loss: 4.396100044250488, total_norm: 2.8147943019866943, lr: 5.9999999999999995e-05
Step 1695, training loss: 4.540264129638672, total_norm: 2.529912233352661, lr: 5.9999999999999995e-05
Step 1696, training loss: 4.435603618621826, total_norm: 2.4548895359039307, lr: 5.9999999999999995e-05
Step 1697, training loss: 4.7819342613220215, total_norm: 2.5586280822753906, lr: 5.9999999999999995e-05
Step 1698, training loss: 5.40730094909668, total_norm: 2.7230277061462402, lr: 5.9999999999999995e-05
Step 1699, training loss: 4.725922107696533, total_norm: 2.8622395992279053, lr: 5.9999999999999995e-05
Step 1700, training loss: 4.911463737487793, total_norm: 3.7558562755584717, lr: 5.9999999999999995e-05
Step 1700, validation loss: 5.699927806854248
Step 1701, training loss: 4.3349785804748535, total_norm: 3.13454008102417, lr: 5.9999999999999995e-05
Step 1702, training loss: 4.747111797332764, total_norm: 2.383291482925415, lr: 5.9999999999999995e-05
Step 1703, training loss: 4.224710464477539, total_norm: 2.850142002105713, lr: 5.9999999999999995e-05
Step 1704, training loss: 4.546302795410156, total_norm: 2.5059802532196045, lr: 5.9999999999999995e-05
Step 1705, training loss: 4.662717342376709, total_norm: 3.2626912593841553, lr: 5.9999999999999995e-05
Step 1706, training loss: 4.538118839263916, total_norm: 2.6008753776550293, lr: 5.9999999999999995e-05
Step 1707, training loss: 4.895402908325195, total_norm: 3.19411039352417, lr: 5.9999999999999995e-05
Step 1708, training loss: 4.602946758270264, total_norm: 2.4566073417663574, lr: 5.9999999999999995e-05
Step 1709, training loss: 4.7387824058532715, total_norm: 2.623079538345337, lr: 5.9999999999999995e-05
Step 1710, training loss: 4.837152481079102, total_norm: 2.912480115890503, lr: 5.9999999999999995e-05
Step 1711, training loss: 4.7784647941589355, total_norm: 2.5208897590637207, lr: 5.9999999999999995e-05
Step 1712, training loss: 4.361902713775635, total_norm: 2.465161085128784, lr: 5.9999999999999995e-05
Step 1713, training loss: 4.7112555503845215, total_norm: 2.5946555137634277, lr: 5.9999999999999995e-05
Step 1714, training loss: 4.402099132537842, total_norm: 2.810873508453369, lr: 5.9999999999999995e-05
Step 1715, training loss: 4.282593250274658, total_norm: 2.661869525909424, lr: 5.9999999999999995e-05
Step 1716, training loss: 4.493252277374268, total_norm: 3.03855037689209, lr: 5.9999999999999995e-05
Step 1717, training loss: 4.738772869110107, total_norm: 2.645110845565796, lr: 5.9999999999999995e-05
Step 1718, training loss: 3.950640916824341, total_norm: 2.771026849746704, lr: 5.9999999999999995e-05
Step 1719, training loss: 3.823695182800293, total_norm: 2.7492306232452393, lr: 5.9999999999999995e-05
Step 1720, training loss: 4.452744007110596, total_norm: 2.42526912689209, lr: 5.9999999999999995e-05
Step 1721, training loss: 4.265141487121582, total_norm: 2.6511597633361816, lr: 5.9999999999999995e-05
Step 1722, training loss: 4.702981948852539, total_norm: 2.6496376991271973, lr: 5.9999999999999995e-05
Step 1723, training loss: 4.51552152633667, total_norm: 2.3914551734924316, lr: 5.9999999999999995e-05
Step 1724, training loss: 4.320179462432861, total_norm: 2.674398899078369, lr: 5.9999999999999995e-05
Step 1725, training loss: 4.5292558670043945, total_norm: 2.6414177417755127, lr: 5.9999999999999995e-05
Step 1726, training loss: 4.103058815002441, total_norm: 2.5943567752838135, lr: 5.9999999999999995e-05
Step 1727, training loss: 4.658112049102783, total_norm: 2.600323438644409, lr: 5.9999999999999995e-05
Step 1728, training loss: 4.756217002868652, total_norm: 2.6859874725341797, lr: 5.9999999999999995e-05
Step 1729, training loss: 4.452217102050781, total_norm: 2.511277914047241, lr: 5.9999999999999995e-05
Step 1730, training loss: 4.831547260284424, total_norm: 2.5147409439086914, lr: 5.9999999999999995e-05
Step 1731, training loss: 4.947843074798584, total_norm: 2.561613082885742, lr: 5.9999999999999995e-05
Step 1732, training loss: 4.812370300292969, total_norm: 2.459847927093506, lr: 5.9999999999999995e-05
Step 1733, training loss: 4.450692176818848, total_norm: 2.438502073287964, lr: 5.9999999999999995e-05
Step 1734, training loss: 4.776641368865967, total_norm: 2.4203641414642334, lr: 5.9999999999999995e-05
Step 1735, training loss: 4.989307403564453, total_norm: 2.537905216217041, lr: 5.9999999999999995e-05
Step 1736, training loss: 4.726856231689453, total_norm: 2.5377066135406494, lr: 5.9999999999999995e-05
Step 1737, training loss: 4.253256797790527, total_norm: 2.50156307220459, lr: 5.9999999999999995e-05
Step 1738, training loss: 4.295567989349365, total_norm: 2.9206485748291016, lr: 5.9999999999999995e-05
Step 1739, training loss: 4.871013641357422, total_norm: 2.581400156021118, lr: 5.9999999999999995e-05
Step 1740, training loss: 4.363141059875488, total_norm: 3.4001808166503906, lr: 5.9999999999999995e-05
Step 1741, training loss: 4.810988426208496, total_norm: 3.4869468212127686, lr: 5.9999999999999995e-05
Step 1742, training loss: 4.5371527671813965, total_norm: 3.4579551219940186, lr: 5.9999999999999995e-05
Step 1743, training loss: 4.0148091316223145, total_norm: 3.1006453037261963, lr: 5.9999999999999995e-05
Step 1744, training loss: 5.164006233215332, total_norm: 2.5209131240844727, lr: 5.9999999999999995e-05
Step 1745, training loss: 4.943894863128662, total_norm: 2.4360883235931396, lr: 5.9999999999999995e-05
Step 1746, training loss: 4.760610580444336, total_norm: 2.9771461486816406, lr: 5.9999999999999995e-05
Step 1747, training loss: 5.008507251739502, total_norm: 3.6016604900360107, lr: 5.9999999999999995e-05
Step 1748, training loss: 4.920138835906982, total_norm: 2.76920747756958, lr: 5.9999999999999995e-05
Step 1749, training loss: 4.059370994567871, total_norm: 3.077396869659424, lr: 5.9999999999999995e-05
Step 1750, training loss: 4.461564064025879, total_norm: 2.810807228088379, lr: 5.9999999999999995e-05
Step 1750, validation loss: 5.684792518615723
Step 1751, training loss: 4.24571418762207, total_norm: 2.8284852504730225, lr: 5.9999999999999995e-05
Step 1752, training loss: 3.8126261234283447, total_norm: 2.713620662689209, lr: 5.9999999999999995e-05
Step 1753, training loss: 4.062961578369141, total_norm: 2.5745866298675537, lr: 5.9999999999999995e-05
Step 1754, training loss: 5.14031457901001, total_norm: 2.8472790718078613, lr: 5.9999999999999995e-05
Step 1755, training loss: 3.2789933681488037, total_norm: 3.0798757076263428, lr: 5.9999999999999995e-05
Step 1756, training loss: 4.186675548553467, total_norm: 2.9269866943359375, lr: 5.9999999999999995e-05
Step 1757, training loss: 4.319726467132568, total_norm: 2.9600462913513184, lr: 5.9999999999999995e-05
Step 1758, training loss: 4.21282434463501, total_norm: 2.772186517715454, lr: 5.9999999999999995e-05
Step 1759, training loss: 4.421682357788086, total_norm: 2.673624277114868, lr: 5.9999999999999995e-05
Step 1760, training loss: 4.949117660522461, total_norm: 2.5076053142547607, lr: 5.9999999999999995e-05
Step 1761, training loss: 5.163369655609131, total_norm: 2.719763994216919, lr: 5.9999999999999995e-05
Step 1762, training loss: 5.0040388107299805, total_norm: 3.2112722396850586, lr: 5.9999999999999995e-05
Step 1763, training loss: 5.002340793609619, total_norm: 2.9499576091766357, lr: 5.9999999999999995e-05
Step 1764, training loss: 4.379273414611816, total_norm: 3.0232889652252197, lr: 5.9999999999999995e-05
Step 1765, training loss: 5.216063976287842, total_norm: 3.239924430847168, lr: 5.9999999999999995e-05
Step 1766, training loss: 4.873844146728516, total_norm: 3.0844099521636963, lr: 5.9999999999999995e-05
Step 1767, training loss: 4.9953742027282715, total_norm: 2.634979486465454, lr: 5.9999999999999995e-05
Step 1768, training loss: 4.560832500457764, total_norm: 2.8075177669525146, lr: 5.9999999999999995e-05
Step 1769, training loss: 4.63072395324707, total_norm: 2.726940155029297, lr: 5.9999999999999995e-05
Step 1770, training loss: 5.1246771812438965, total_norm: 3.027961254119873, lr: 5.9999999999999995e-05
Step 1771, training loss: 5.083957195281982, total_norm: 4.104750156402588, lr: 5.9999999999999995e-05
Step 1772, training loss: 5.233336925506592, total_norm: 2.9270994663238525, lr: 5.9999999999999995e-05
Step 1773, training loss: 5.209289073944092, total_norm: 2.8025283813476562, lr: 5.9999999999999995e-05
Step 1774, training loss: 5.13552188873291, total_norm: 2.873152256011963, lr: 5.9999999999999995e-05
Step 1775, training loss: 5.223710536956787, total_norm: 2.8698456287384033, lr: 5.9999999999999995e-05
Step 1776, training loss: 5.215427398681641, total_norm: 2.6879472732543945, lr: 5.9999999999999995e-05
Step 1777, training loss: 5.167749404907227, total_norm: 3.12637996673584, lr: 5.9999999999999995e-05
Step 1778, training loss: 4.91208553314209, total_norm: 3.1584012508392334, lr: 5.9999999999999995e-05
Step 1779, training loss: 4.975632190704346, total_norm: 2.8410274982452393, lr: 5.9999999999999995e-05
Step 1780, training loss: 5.132623195648193, total_norm: 2.9762237071990967, lr: 5.9999999999999995e-05
Step 1781, training loss: 5.378970623016357, total_norm: 2.712054967880249, lr: 5.9999999999999995e-05
Step 1782, training loss: 4.937276840209961, total_norm: 2.8756656646728516, lr: 5.9999999999999995e-05
Step 1783, training loss: 4.86538028717041, total_norm: 2.6020617485046387, lr: 5.9999999999999995e-05
Step 1784, training loss: 4.479420185089111, total_norm: 3.550668239593506, lr: 5.9999999999999995e-05
Step 1785, training loss: 4.858877658843994, total_norm: 2.6805343627929688, lr: 5.9999999999999995e-05
Step 1786, training loss: 5.170831203460693, total_norm: 2.5370595455169678, lr: 5.9999999999999995e-05
Step 1787, training loss: 5.049000263214111, total_norm: 2.7374565601348877, lr: 5.9999999999999995e-05
Step 1788, training loss: 5.568777561187744, total_norm: 2.9926493167877197, lr: 5.9999999999999995e-05
Step 1789, training loss: 4.741904258728027, total_norm: 2.8305139541625977, lr: 5.9999999999999995e-05
Step 1790, training loss: 4.903003215789795, total_norm: 2.608377456665039, lr: 5.9999999999999995e-05
Step 1791, training loss: 4.818783760070801, total_norm: 2.597464084625244, lr: 5.9999999999999995e-05
Step 1792, training loss: 4.875756740570068, total_norm: 2.7070505619049072, lr: 5.9999999999999995e-05
Step 1793, training loss: 4.9689483642578125, total_norm: 3.55472731590271, lr: 5.9999999999999995e-05
Step 1794, training loss: 5.268097400665283, total_norm: 2.9250357151031494, lr: 5.9999999999999995e-05
Step 1795, training loss: 5.13794469833374, total_norm: 2.8503124713897705, lr: 5.9999999999999995e-05
Step 1796, training loss: 4.776169776916504, total_norm: 2.5277023315429688, lr: 5.9999999999999995e-05
Step 1797, training loss: 4.764562129974365, total_norm: 2.4389567375183105, lr: 5.9999999999999995e-05
Step 1798, training loss: 4.825343132019043, total_norm: 2.9999711513519287, lr: 5.9999999999999995e-05
Step 1799, training loss: 4.54934549331665, total_norm: 3.813337802886963, lr: 5.9999999999999995e-05
Step 1800, training loss: 4.472894191741943, total_norm: 2.994426727294922, lr: 5.9999999999999995e-05
Step 1800, validation loss: 5.671928405761719
Step 1801, training loss: 4.557591438293457, total_norm: 2.66282057762146, lr: 5.9999999999999995e-05
Step 1802, training loss: 4.809228897094727, total_norm: 3.228686571121216, lr: 5.9999999999999995e-05
Step 1803, training loss: 4.995297908782959, total_norm: 3.238895893096924, lr: 5.9999999999999995e-05
Step 1804, training loss: 4.720441818237305, total_norm: 2.7238733768463135, lr: 5.9999999999999995e-05
Step 1805, training loss: 5.020617485046387, total_norm: 2.645657539367676, lr: 5.9999999999999995e-05
Step 1806, training loss: 5.2186384201049805, total_norm: 2.930284261703491, lr: 5.9999999999999995e-05
Step 1807, training loss: 5.241962432861328, total_norm: 2.791391372680664, lr: 5.9999999999999995e-05
Step 1808, training loss: 5.151403903961182, total_norm: 2.969874143600464, lr: 5.9999999999999995e-05
Step 1809, training loss: 5.243171215057373, total_norm: 3.086822509765625, lr: 5.9999999999999995e-05
Step 1810, training loss: 4.430268287658691, total_norm: 2.9976236820220947, lr: 5.9999999999999995e-05
Step 1811, training loss: 4.716979026794434, total_norm: 2.8799498081207275, lr: 5.9999999999999995e-05
Step 1812, training loss: 5.22805643081665, total_norm: 2.7952942848205566, lr: 5.9999999999999995e-05
Step 1813, training loss: 4.802864074707031, total_norm: 2.5736477375030518, lr: 5.9999999999999995e-05
Step 1814, training loss: 5.062585830688477, total_norm: 2.94437837600708, lr: 5.9999999999999995e-05
Step 1815, training loss: 3.9737143516540527, total_norm: 2.630744457244873, lr: 5.9999999999999995e-05
Step 1816, training loss: 4.94765043258667, total_norm: 2.962862491607666, lr: 5.9999999999999995e-05
Step 1817, training loss: 5.175178527832031, total_norm: 3.2050323486328125, lr: 5.9999999999999995e-05
Step 1818, training loss: 4.71189546585083, total_norm: 2.6030211448669434, lr: 5.9999999999999995e-05
Step 1819, training loss: 4.386605739593506, total_norm: 3.008873462677002, lr: 5.9999999999999995e-05
Step 1820, training loss: 4.757955551147461, total_norm: 2.8951146602630615, lr: 5.9999999999999995e-05
Step 1821, training loss: 4.8049726486206055, total_norm: 2.899703025817871, lr: 5.9999999999999995e-05
Step 1822, training loss: 4.821839332580566, total_norm: 2.78954815864563, lr: 5.9999999999999995e-05
Step 1823, training loss: 4.510707378387451, total_norm: 2.773624897003174, lr: 5.9999999999999995e-05
Step 1824, training loss: 4.95569372177124, total_norm: 2.466092586517334, lr: 5.9999999999999995e-05
Step 1825, training loss: 4.369487285614014, total_norm: 2.545372247695923, lr: 5.9999999999999995e-05
Step 1826, training loss: 3.8782145977020264, total_norm: 2.473874807357788, lr: 5.9999999999999995e-05
Step 1827, training loss: 4.895634174346924, total_norm: 2.8355844020843506, lr: 5.9999999999999995e-05
Step 1828, training loss: 4.990382194519043, total_norm: 2.9267518520355225, lr: 5.9999999999999995e-05
Step 1829, training loss: 4.455672740936279, total_norm: 2.7770211696624756, lr: 5.9999999999999995e-05
Step 1830, training loss: 4.959728240966797, total_norm: 2.835301399230957, lr: 5.9999999999999995e-05
Step 1831, training loss: 3.699335813522339, total_norm: 2.842787027359009, lr: 5.9999999999999995e-05
Step 1832, training loss: 3.596879482269287, total_norm: 3.0949819087982178, lr: 5.9999999999999995e-05
Step 1833, training loss: 4.208132266998291, total_norm: 2.6172592639923096, lr: 5.9999999999999995e-05
Step 1834, training loss: 3.574167490005493, total_norm: 3.4323503971099854, lr: 5.9999999999999995e-05
Step 1835, training loss: 4.618443489074707, total_norm: 2.9179680347442627, lr: 5.9999999999999995e-05
Step 1836, training loss: 4.388632774353027, total_norm: 2.8979592323303223, lr: 5.9999999999999995e-05
Step 1837, training loss: 4.2695112228393555, total_norm: 2.5640828609466553, lr: 5.9999999999999995e-05
Step 1838, training loss: 5.143063068389893, total_norm: 2.692641019821167, lr: 5.9999999999999995e-05
Step 1839, training loss: 5.300751209259033, total_norm: 3.146106004714966, lr: 5.9999999999999995e-05
Step 1840, training loss: 4.690898895263672, total_norm: 2.8841490745544434, lr: 5.9999999999999995e-05
Step 1841, training loss: 4.718447685241699, total_norm: 2.780858039855957, lr: 5.9999999999999995e-05
Step 1842, training loss: 5.011510848999023, total_norm: 3.0268476009368896, lr: 5.9999999999999995e-05
Step 1843, training loss: 4.788775444030762, total_norm: 4.28322172164917, lr: 5.9999999999999995e-05
Step 1844, training loss: 5.01775598526001, total_norm: 3.241905689239502, lr: 5.9999999999999995e-05
Step 1845, training loss: 5.332345008850098, total_norm: 3.232686996459961, lr: 5.9999999999999995e-05
Step 1846, training loss: 4.727830410003662, total_norm: 4.210243225097656, lr: 5.9999999999999995e-05
Step 1847, training loss: 4.819361209869385, total_norm: 3.8711769580841064, lr: 5.9999999999999995e-05
Step 1848, training loss: 4.985153675079346, total_norm: 3.1051714420318604, lr: 5.9999999999999995e-05
Step 1849, training loss: 5.489798545837402, total_norm: 2.511094808578491, lr: 5.9999999999999995e-05
Step 1850, training loss: 4.987634181976318, total_norm: 3.918985366821289, lr: 5.9999999999999995e-05
Step 1850, validation loss: 5.598670482635498
Step 1851, training loss: 4.735110282897949, total_norm: 3.68293833732605, lr: 5.9999999999999995e-05
Step 1852, training loss: 5.194638729095459, total_norm: 3.310774087905884, lr: 5.9999999999999995e-05
Step 1853, training loss: 4.762251377105713, total_norm: 2.9435267448425293, lr: 5.9999999999999995e-05
Step 1854, training loss: 4.761433124542236, total_norm: 2.9124295711517334, lr: 5.9999999999999995e-05
Step 1855, training loss: 4.536780834197998, total_norm: 3.2061617374420166, lr: 5.9999999999999995e-05
Step 1856, training loss: 5.834500789642334, total_norm: 3.327751398086548, lr: 5.9999999999999995e-05
Step 1857, training loss: 5.224188327789307, total_norm: 3.1744418144226074, lr: 5.9999999999999995e-05
Step 1858, training loss: 4.849703311920166, total_norm: 2.7355048656463623, lr: 5.9999999999999995e-05
Step 1859, training loss: 4.638582706451416, total_norm: 2.9042820930480957, lr: 5.9999999999999995e-05
Step 1860, training loss: 4.222532749176025, total_norm: 2.8513245582580566, lr: 5.9999999999999995e-05
Step 1861, training loss: 4.520955562591553, total_norm: 2.877570152282715, lr: 5.9999999999999995e-05
Step 1862, training loss: 5.227431774139404, total_norm: 2.9559502601623535, lr: 5.9999999999999995e-05
Step 1863, training loss: 4.757841110229492, total_norm: 2.4823198318481445, lr: 5.9999999999999995e-05
Step 1864, training loss: 4.408426284790039, total_norm: 2.339092254638672, lr: 5.9999999999999995e-05
Step 1865, training loss: 4.664205551147461, total_norm: 2.4890341758728027, lr: 5.9999999999999995e-05
Step 1866, training loss: 4.245257377624512, total_norm: 2.6309547424316406, lr: 5.9999999999999995e-05
Step 1867, training loss: 4.304717540740967, total_norm: 2.4489967823028564, lr: 5.9999999999999995e-05
Step 1868, training loss: 5.731909275054932, total_norm: 2.5072383880615234, lr: 5.9999999999999995e-05
Step 1869, training loss: 4.842185974121094, total_norm: 2.6183056831359863, lr: 5.9999999999999995e-05
Step 1870, training loss: 4.381268501281738, total_norm: 3.242591381072998, lr: 5.9999999999999995e-05
Step 1871, training loss: 5.588413715362549, total_norm: 3.8324899673461914, lr: 5.9999999999999995e-05
Step 1872, training loss: 4.444125652313232, total_norm: 3.670819044113159, lr: 5.9999999999999995e-05
Step 1873, training loss: 4.2434773445129395, total_norm: 3.1062865257263184, lr: 5.9999999999999995e-05
Step 1874, training loss: 4.742352485656738, total_norm: 2.7202086448669434, lr: 5.9999999999999995e-05
Step 1875, training loss: 4.450821399688721, total_norm: 2.5734665393829346, lr: 5.9999999999999995e-05
Step 1876, training loss: 4.983727931976318, total_norm: 2.7225637435913086, lr: 5.9999999999999995e-05
Step 1877, training loss: 4.445254325866699, total_norm: 2.363501787185669, lr: 5.9999999999999995e-05
Step 1878, training loss: 4.8171820640563965, total_norm: 3.1267244815826416, lr: 5.9999999999999995e-05
Step 1879, training loss: 4.5932416915893555, total_norm: 3.0108697414398193, lr: 5.9999999999999995e-05
Step 1880, training loss: 5.082479953765869, total_norm: 3.4069736003875732, lr: 5.9999999999999995e-05
Step 1881, training loss: 4.548772811889648, total_norm: 3.1539509296417236, lr: 5.9999999999999995e-05
Step 1882, training loss: 4.697751998901367, total_norm: 2.782040596008301, lr: 5.9999999999999995e-05
Step 1883, training loss: 4.387511730194092, total_norm: 3.1772797107696533, lr: 5.9999999999999995e-05
Step 1884, training loss: 5.444449424743652, total_norm: 3.822460174560547, lr: 5.9999999999999995e-05
Step 1885, training loss: 5.06233549118042, total_norm: 3.386404037475586, lr: 5.9999999999999995e-05
Step 1886, training loss: 4.679467678070068, total_norm: 3.2060399055480957, lr: 5.9999999999999995e-05
Step 1887, training loss: 5.117311000823975, total_norm: 3.0817904472351074, lr: 5.9999999999999995e-05
Step 1888, training loss: 4.842625617980957, total_norm: 2.780047655105591, lr: 5.9999999999999995e-05
Step 1889, training loss: 4.855530738830566, total_norm: 2.7902255058288574, lr: 5.9999999999999995e-05
Step 1890, training loss: 4.668289661407471, total_norm: 3.0376243591308594, lr: 5.9999999999999995e-05
Step 1891, training loss: 4.472958564758301, total_norm: 2.651737689971924, lr: 5.9999999999999995e-05
Step 1892, training loss: 5.133193016052246, total_norm: 2.7067105770111084, lr: 5.9999999999999995e-05
Step 1893, training loss: 4.40090799331665, total_norm: 2.701796531677246, lr: 5.9999999999999995e-05
Step 1894, training loss: 4.684584617614746, total_norm: 2.4997806549072266, lr: 5.9999999999999995e-05
Step 1895, training loss: 4.398330211639404, total_norm: 2.3463430404663086, lr: 5.9999999999999995e-05
Step 1896, training loss: 3.8373236656188965, total_norm: 2.917414665222168, lr: 5.9999999999999995e-05
Step 1897, training loss: 4.332076072692871, total_norm: 2.486459732055664, lr: 5.9999999999999995e-05
Step 1898, training loss: 4.680669784545898, total_norm: 2.6563968658447266, lr: 5.9999999999999995e-05
Step 1899, training loss: 4.576388359069824, total_norm: 2.5494704246520996, lr: 5.9999999999999995e-05
Step 1900, training loss: 4.256336688995361, total_norm: 2.328904628753662, lr: 5.9999999999999995e-05
Step 1900, validation loss: 5.678715705871582
Step 1901, training loss: 4.133504390716553, total_norm: 3.336313486099243, lr: 5.9999999999999995e-05
Step 1902, training loss: 4.189892768859863, total_norm: 2.5316905975341797, lr: 5.9999999999999995e-05
Step 1903, training loss: 5.202383041381836, total_norm: 2.8680977821350098, lr: 5.9999999999999995e-05
Step 1904, training loss: 4.516500473022461, total_norm: 2.4622011184692383, lr: 5.9999999999999995e-05
Step 1905, training loss: 4.256897449493408, total_norm: 2.654806137084961, lr: 5.9999999999999995e-05
Step 1906, training loss: 4.631488800048828, total_norm: 2.7253503799438477, lr: 5.9999999999999995e-05
Step 1907, training loss: 4.572984218597412, total_norm: 2.717243194580078, lr: 5.9999999999999995e-05
Step 1908, training loss: 4.328578948974609, total_norm: 3.312871217727661, lr: 5.9999999999999995e-05
Step 1909, training loss: 4.5727434158325195, total_norm: 3.3279621601104736, lr: 5.9999999999999995e-05
Step 1910, training loss: 4.930854797363281, total_norm: 2.721808910369873, lr: 5.9999999999999995e-05
Step 1911, training loss: 5.183406829833984, total_norm: 3.9531478881835938, lr: 5.9999999999999995e-05
Step 1912, training loss: 4.913963794708252, total_norm: 2.7208824157714844, lr: 5.9999999999999995e-05
Step 1913, training loss: 5.496678352355957, total_norm: 3.070979595184326, lr: 5.9999999999999995e-05
Step 1914, training loss: 4.686459064483643, total_norm: 3.184800148010254, lr: 5.9999999999999995e-05
Step 1915, training loss: 5.142895221710205, total_norm: 2.459839344024658, lr: 5.9999999999999995e-05
Step 1916, training loss: 4.616558074951172, total_norm: 2.513441562652588, lr: 5.9999999999999995e-05
Step 1917, training loss: 5.247649192810059, total_norm: 2.789198875427246, lr: 5.9999999999999995e-05
Step 1918, training loss: 4.348917484283447, total_norm: 3.5243051052093506, lr: 5.9999999999999995e-05
Step 1919, training loss: 4.6039276123046875, total_norm: 2.6758439540863037, lr: 5.9999999999999995e-05
Step 1920, training loss: 4.549247741699219, total_norm: 3.0206644535064697, lr: 5.9999999999999995e-05
Step 1921, training loss: 4.991403579711914, total_norm: 2.6340858936309814, lr: 5.9999999999999995e-05
Step 1922, training loss: 4.612222671508789, total_norm: 2.4333345890045166, lr: 5.9999999999999995e-05
Step 1923, training loss: 4.91097354888916, total_norm: 3.5665442943573, lr: 5.9999999999999995e-05
Step 1924, training loss: 4.991515636444092, total_norm: 3.5151097774505615, lr: 5.9999999999999995e-05
Step 1925, training loss: 4.616893291473389, total_norm: 3.2163379192352295, lr: 5.9999999999999995e-05
Step 1926, training loss: 4.599292278289795, total_norm: 3.422074317932129, lr: 5.9999999999999995e-05
Step 1927, training loss: 4.552539825439453, total_norm: 3.155625104904175, lr: 5.9999999999999995e-05
Step 1928, training loss: 4.508505344390869, total_norm: 3.5179824829101562, lr: 5.9999999999999995e-05
Step 1929, training loss: 4.535360336303711, total_norm: 2.910565137863159, lr: 5.9999999999999995e-05
Step 1930, training loss: 4.236557960510254, total_norm: 2.6266438961029053, lr: 5.9999999999999995e-05
Step 1931, training loss: 4.602430820465088, total_norm: 2.91135835647583, lr: 5.9999999999999995e-05
Step 1932, training loss: 4.692530632019043, total_norm: 2.603400707244873, lr: 5.9999999999999995e-05
Step 1933, training loss: 4.714233875274658, total_norm: 3.057115316390991, lr: 5.9999999999999995e-05
Step 1934, training loss: 5.292533874511719, total_norm: 3.0377492904663086, lr: 5.9999999999999995e-05
Step 1935, training loss: 4.918312072753906, total_norm: 2.774477243423462, lr: 5.9999999999999995e-05
Step 1936, training loss: 4.880704402923584, total_norm: 3.239454507827759, lr: 5.9999999999999995e-05
Step 1937, training loss: 5.268436908721924, total_norm: 2.7856149673461914, lr: 5.9999999999999995e-05
Step 1938, training loss: 4.635613441467285, total_norm: 2.7389628887176514, lr: 5.9999999999999995e-05
Step 1939, training loss: 5.16303014755249, total_norm: 2.8681516647338867, lr: 5.9999999999999995e-05
Step 1940, training loss: 5.051483631134033, total_norm: 2.7542529106140137, lr: 5.9999999999999995e-05
Step 1941, training loss: 5.049149036407471, total_norm: 3.041653633117676, lr: 5.9999999999999995e-05
Step 1942, training loss: 4.823604583740234, total_norm: 2.6318252086639404, lr: 5.9999999999999995e-05
Step 1943, training loss: 4.942127704620361, total_norm: 2.9662110805511475, lr: 5.9999999999999995e-05
Step 1944, training loss: 5.054305076599121, total_norm: 2.6552135944366455, lr: 5.9999999999999995e-05
Step 1945, training loss: 4.42519474029541, total_norm: 2.984590768814087, lr: 5.9999999999999995e-05
Step 1946, training loss: 4.143337249755859, total_norm: 3.1854634284973145, lr: 5.9999999999999995e-05
Step 1947, training loss: 4.3592119216918945, total_norm: 2.828080415725708, lr: 5.9999999999999995e-05
Step 1948, training loss: 4.810897350311279, total_norm: 3.178096294403076, lr: 5.9999999999999995e-05
Step 1949, training loss: 4.79293966293335, total_norm: 3.2204742431640625, lr: 5.9999999999999995e-05
Step 1950, training loss: 4.8401198387146, total_norm: 2.8313000202178955, lr: 5.9999999999999995e-05
Step 1950, validation loss: 5.780114650726318
Step 1951, training loss: 5.432783126831055, total_norm: 3.083402633666992, lr: 5.9999999999999995e-05
Step 1952, training loss: 4.78564453125, total_norm: 2.7310149669647217, lr: 5.9999999999999995e-05
Step 1953, training loss: 4.497867107391357, total_norm: 2.8535850048065186, lr: 5.9999999999999995e-05
Step 1954, training loss: 5.040246486663818, total_norm: 2.9641075134277344, lr: 5.9999999999999995e-05
Step 1955, training loss: 4.6024489402771, total_norm: 2.8356168270111084, lr: 5.9999999999999995e-05
Step 1956, training loss: 4.80368709564209, total_norm: 2.924591064453125, lr: 5.9999999999999995e-05
Step 1957, training loss: 4.77303409576416, total_norm: 3.052332878112793, lr: 5.9999999999999995e-05
Step 1958, training loss: 4.233894348144531, total_norm: 3.2546777725219727, lr: 5.9999999999999995e-05
Step 1959, training loss: 4.243011951446533, total_norm: 3.051812171936035, lr: 5.9999999999999995e-05
Step 1960, training loss: 3.657672166824341, total_norm: 6.631248474121094, lr: 5.9999999999999995e-05
Step 1961, training loss: 4.0274176597595215, total_norm: 4.575288772583008, lr: 5.9999999999999995e-05
Step 1962, training loss: 4.062880516052246, total_norm: 4.115255832672119, lr: 5.9999999999999995e-05
Step 1963, training loss: 5.14846658706665, total_norm: 3.3457977771759033, lr: 5.9999999999999995e-05
Step 1964, training loss: 5.145179748535156, total_norm: 3.281524658203125, lr: 5.9999999999999995e-05
Step 1965, training loss: 4.624839782714844, total_norm: 3.2576143741607666, lr: 5.9999999999999995e-05
Step 1966, training loss: 5.057662487030029, total_norm: 3.627917766571045, lr: 5.9999999999999995e-05
Step 1967, training loss: 4.7566328048706055, total_norm: 3.6753461360931396, lr: 5.9999999999999995e-05
Step 1968, training loss: 4.653316497802734, total_norm: 3.6975433826446533, lr: 5.9999999999999995e-05
Step 1969, training loss: 4.812637805938721, total_norm: 3.112271547317505, lr: 5.9999999999999995e-05
Step 1970, training loss: 4.4443254470825195, total_norm: 3.0523900985717773, lr: 5.9999999999999995e-05
Step 1971, training loss: 4.329977989196777, total_norm: 2.924929141998291, lr: 5.9999999999999995e-05
Step 1972, training loss: 4.624273777008057, total_norm: 3.039485454559326, lr: 5.9999999999999995e-05
Step 1973, training loss: 4.432457447052002, total_norm: 2.557961940765381, lr: 5.9999999999999995e-05
Step 1974, training loss: 4.715660572052002, total_norm: 2.4723751544952393, lr: 5.9999999999999995e-05
Step 1975, training loss: 4.600986957550049, total_norm: 2.9778568744659424, lr: 5.9999999999999995e-05
Step 1976, training loss: 4.441460609436035, total_norm: 3.0613598823547363, lr: 5.9999999999999995e-05
Step 1977, training loss: 4.34053373336792, total_norm: 2.6979594230651855, lr: 5.9999999999999995e-05
Step 1978, training loss: 4.6737470626831055, total_norm: 2.6562488079071045, lr: 5.9999999999999995e-05
Step 1979, training loss: 4.436233997344971, total_norm: 2.6562297344207764, lr: 5.9999999999999995e-05
Step 1980, training loss: 4.406432151794434, total_norm: 2.613870620727539, lr: 5.9999999999999995e-05
Step 1981, training loss: 4.66572380065918, total_norm: 2.458026647567749, lr: 5.9999999999999995e-05
Step 1982, training loss: 4.325179100036621, total_norm: 2.736135482788086, lr: 5.9999999999999995e-05
Step 1983, training loss: 4.338261127471924, total_norm: 2.932722330093384, lr: 5.9999999999999995e-05
Step 1984, training loss: 4.727479934692383, total_norm: 2.717503309249878, lr: 5.9999999999999995e-05
Step 1985, training loss: 4.86312198638916, total_norm: 2.911261796951294, lr: 5.9999999999999995e-05
Step 1986, training loss: 4.22524881362915, total_norm: 2.4997036457061768, lr: 5.9999999999999995e-05
Step 1987, training loss: 4.162227630615234, total_norm: 2.4028713703155518, lr: 5.9999999999999995e-05
Step 1988, training loss: 4.383476734161377, total_norm: 2.4344253540039062, lr: 5.9999999999999995e-05
Step 1989, training loss: 4.669311046600342, total_norm: 2.686575174331665, lr: 5.9999999999999995e-05
Step 1990, training loss: 4.641568660736084, total_norm: 2.602932929992676, lr: 5.9999999999999995e-05
Step 1991, training loss: 5.220827102661133, total_norm: 2.80187726020813, lr: 5.9999999999999995e-05
Step 1992, training loss: 4.621153354644775, total_norm: 2.8167307376861572, lr: 5.9999999999999995e-05
Step 1993, training loss: 4.291761875152588, total_norm: 2.4973063468933105, lr: 5.9999999999999995e-05
Step 1994, training loss: 3.790814161300659, total_norm: 2.8232414722442627, lr: 5.9999999999999995e-05
Step 1995, training loss: 3.9890003204345703, total_norm: 2.6611897945404053, lr: 5.9999999999999995e-05
Step 1996, training loss: 4.47894811630249, total_norm: 2.7451913356781006, lr: 5.9999999999999995e-05
Step 1997, training loss: 4.768973350524902, total_norm: 2.7503159046173096, lr: 5.9999999999999995e-05
Step 1998, training loss: 5.0985918045043945, total_norm: 2.9502322673797607, lr: 5.9999999999999995e-05
Step 1999, training loss: 4.92102575302124, total_norm: 2.9469265937805176, lr: 5.9999999999999995e-05
Step 2000, training loss: 5.798817157745361, total_norm: 3.714433431625366, lr: 5.9999999999999995e-05
Step 2000, validation loss: 5.718443393707275
Step 2001, training loss: 5.066727638244629, total_norm: 3.739824056625366, lr: 5.9999999999999995e-05
Step 2002, training loss: 5.209372520446777, total_norm: 4.29609489440918, lr: 5.9999999999999995e-05
Step 2003, training loss: 5.0523881912231445, total_norm: 3.278870105743408, lr: 5.9999999999999995e-05
Step 2004, training loss: 5.291629314422607, total_norm: 3.2441985607147217, lr: 5.9999999999999995e-05
Step 2005, training loss: 5.029338359832764, total_norm: 3.1892173290252686, lr: 5.9999999999999995e-05
Step 2006, training loss: 5.034048080444336, total_norm: 3.3158373832702637, lr: 5.9999999999999995e-05
Step 2007, training loss: 4.939008712768555, total_norm: 3.3035216331481934, lr: 5.9999999999999995e-05
Step 2008, training loss: 5.30716609954834, total_norm: 3.064830780029297, lr: 5.9999999999999995e-05
Step 2009, training loss: 5.059082984924316, total_norm: 2.9682388305664062, lr: 5.9999999999999995e-05
Step 2010, training loss: 4.663227081298828, total_norm: 3.005808115005493, lr: 5.9999999999999995e-05
Step 2011, training loss: 4.606453895568848, total_norm: 3.0212061405181885, lr: 5.9999999999999995e-05
Step 2012, training loss: 5.041342258453369, total_norm: 2.9709877967834473, lr: 5.9999999999999995e-05
Step 2013, training loss: 4.4295878410339355, total_norm: 3.795652151107788, lr: 5.9999999999999995e-05
Step 2014, training loss: 4.710160732269287, total_norm: 2.6936771869659424, lr: 5.9999999999999995e-05
Step 2015, training loss: 5.043000221252441, total_norm: 2.7137770652770996, lr: 5.9999999999999995e-05
Step 2016, training loss: 4.725755214691162, total_norm: 2.6094963550567627, lr: 5.9999999999999995e-05
Step 2017, training loss: 4.842892169952393, total_norm: 2.7038090229034424, lr: 5.9999999999999995e-05
Step 2018, training loss: 4.743499755859375, total_norm: 2.796598434448242, lr: 5.9999999999999995e-05
Step 2019, training loss: 5.026042938232422, total_norm: 3.666780471801758, lr: 5.9999999999999995e-05
Step 2020, training loss: 4.676961898803711, total_norm: 2.7339882850646973, lr: 5.9999999999999995e-05
Step 2021, training loss: 4.410836696624756, total_norm: 2.721791982650757, lr: 5.9999999999999995e-05
Step 2022, training loss: 4.427156448364258, total_norm: 3.0445261001586914, lr: 5.9999999999999995e-05
Step 2023, training loss: 4.720736503601074, total_norm: 3.2102644443511963, lr: 5.9999999999999995e-05
Step 2024, training loss: 4.568987846374512, total_norm: 2.771360158920288, lr: 5.9999999999999995e-05
Step 2025, training loss: 4.956586837768555, total_norm: 2.965161085128784, lr: 5.9999999999999995e-05
Step 2026, training loss: 5.125695705413818, total_norm: 3.486610174179077, lr: 5.9999999999999995e-05
Step 2027, training loss: 4.912951946258545, total_norm: 2.9257805347442627, lr: 5.9999999999999995e-05
Step 2028, training loss: 4.976782321929932, total_norm: 2.5771729946136475, lr: 5.9999999999999995e-05
Step 2029, training loss: 4.392984390258789, total_norm: 2.8475286960601807, lr: 5.9999999999999995e-05
Step 2030, training loss: 5.314280033111572, total_norm: 2.8522872924804688, lr: 5.9999999999999995e-05
Step 2031, training loss: 4.919746398925781, total_norm: 2.7146809101104736, lr: 5.9999999999999995e-05
Step 2032, training loss: 4.7023138999938965, total_norm: 3.15153169631958, lr: 5.9999999999999995e-05
Step 2033, training loss: 5.5924553871154785, total_norm: 2.898794412612915, lr: 5.9999999999999995e-05
Step 2034, training loss: 5.7917914390563965, total_norm: 4.184483051300049, lr: 5.9999999999999995e-05
Step 2035, training loss: 5.1392316818237305, total_norm: 3.3279776573181152, lr: 5.9999999999999995e-05
Step 2036, training loss: 5.287104606628418, total_norm: 2.8354358673095703, lr: 5.9999999999999995e-05
Step 2037, training loss: 5.382452011108398, total_norm: 3.021700143814087, lr: 5.9999999999999995e-05
Step 2038, training loss: 5.600490093231201, total_norm: 3.0742015838623047, lr: 5.9999999999999995e-05
Step 2039, training loss: 5.235945701599121, total_norm: 3.1863489151000977, lr: 5.9999999999999995e-05
Step 2040, training loss: 4.7803826332092285, total_norm: 3.098752737045288, lr: 5.9999999999999995e-05
Step 2041, training loss: 4.914532661437988, total_norm: 2.9324891567230225, lr: 5.9999999999999995e-05
Step 2042, training loss: 5.247664928436279, total_norm: 3.6167473793029785, lr: 5.9999999999999995e-05
Step 2043, training loss: 5.1246161460876465, total_norm: 3.5227768421173096, lr: 5.9999999999999995e-05
Step 2044, training loss: 5.368311882019043, total_norm: 2.942570209503174, lr: 5.9999999999999995e-05
Step 2045, training loss: 4.683806896209717, total_norm: 2.792618751525879, lr: 5.9999999999999995e-05
Step 2046, training loss: 5.459595680236816, total_norm: 3.1005427837371826, lr: 5.9999999999999995e-05
Step 2047, training loss: 5.349281311035156, total_norm: 3.5242252349853516, lr: 5.9999999999999995e-05
Step 2048, training loss: 4.924005508422852, total_norm: 3.5867249965667725, lr: 5.9999999999999995e-05
Step 2049, training loss: 4.629880905151367, total_norm: 3.8276093006134033, lr: 5.9999999999999995e-05
Step 2050, training loss: 5.165051460266113, total_norm: 2.770049810409546, lr: 5.9999999999999995e-05
Step 2050, validation loss: 5.559841156005859
Step 2051, training loss: 4.395726203918457, total_norm: 2.6952905654907227, lr: 5.9999999999999995e-05
Step 2052, training loss: 4.450814723968506, total_norm: 3.0621509552001953, lr: 5.9999999999999995e-05
Step 2053, training loss: 5.048031330108643, total_norm: 2.63775897026062, lr: 5.9999999999999995e-05
Step 2054, training loss: 4.635104656219482, total_norm: 2.798832416534424, lr: 5.9999999999999995e-05
Step 2055, training loss: 4.640176773071289, total_norm: 3.078253984451294, lr: 5.9999999999999995e-05
Step 2056, training loss: 4.7197675704956055, total_norm: 2.5683400630950928, lr: 5.9999999999999995e-05
Step 2057, training loss: 5.28977108001709, total_norm: 3.1943602561950684, lr: 5.9999999999999995e-05
Step 2058, training loss: 4.368255138397217, total_norm: 3.208815097808838, lr: 5.9999999999999995e-05
Step 2059, training loss: 4.562981128692627, total_norm: 2.776609182357788, lr: 5.9999999999999995e-05
Step 2060, training loss: 4.514512062072754, total_norm: 3.276515245437622, lr: 5.9999999999999995e-05
Step 2061, training loss: 4.556789875030518, total_norm: 2.951002597808838, lr: 5.9999999999999995e-05
Step 2062, training loss: 5.3878326416015625, total_norm: 3.0087997913360596, lr: 5.9999999999999995e-05
Step 2063, training loss: 4.52557373046875, total_norm: 2.5291929244995117, lr: 5.9999999999999995e-05
Step 2064, training loss: 4.881762504577637, total_norm: 3.0565080642700195, lr: 5.9999999999999995e-05
Step 2065, training loss: 4.615179538726807, total_norm: 3.5636065006256104, lr: 5.9999999999999995e-05
Step 2066, training loss: 4.771600723266602, total_norm: 3.00213885307312, lr: 5.9999999999999995e-05
Step 2067, training loss: 5.009901523590088, total_norm: 2.6701016426086426, lr: 5.9999999999999995e-05
Step 2068, training loss: 4.907698154449463, total_norm: 2.8007476329803467, lr: 5.9999999999999995e-05
Step 2069, training loss: 4.41985559463501, total_norm: 2.420039653778076, lr: 5.9999999999999995e-05
Step 2070, training loss: 5.316714286804199, total_norm: 3.04293155670166, lr: 5.9999999999999995e-05
Step 2071, training loss: 5.637848854064941, total_norm: 3.2924768924713135, lr: 5.9999999999999995e-05
Step 2072, training loss: 5.539821624755859, total_norm: 3.1039183139801025, lr: 5.9999999999999995e-05
Step 2073, training loss: 4.700033187866211, total_norm: 3.1973519325256348, lr: 5.9999999999999995e-05
Step 2074, training loss: 4.319045543670654, total_norm: 2.900930881500244, lr: 5.9999999999999995e-05
Step 2075, training loss: 4.81644868850708, total_norm: 2.8997137546539307, lr: 5.9999999999999995e-05
Step 2076, training loss: 4.406160354614258, total_norm: 2.7717950344085693, lr: 5.9999999999999995e-05
Step 2077, training loss: 4.624518871307373, total_norm: 2.96602201461792, lr: 5.9999999999999995e-05
Step 2078, training loss: 5.06020450592041, total_norm: 3.0783493518829346, lr: 5.9999999999999995e-05
Step 2079, training loss: 5.25167989730835, total_norm: 3.766180992126465, lr: 5.9999999999999995e-05
Step 2080, training loss: 4.802475929260254, total_norm: 3.795436143875122, lr: 5.9999999999999995e-05
Step 2081, training loss: 4.729107856750488, total_norm: 3.7007076740264893, lr: 5.9999999999999995e-05
Step 2082, training loss: 4.867593288421631, total_norm: 4.277777671813965, lr: 5.9999999999999995e-05
Step 2083, training loss: 5.102301597595215, total_norm: 4.007533073425293, lr: 5.9999999999999995e-05
Step 2084, training loss: 5.09293794631958, total_norm: 3.3322136402130127, lr: 5.9999999999999995e-05
Step 2085, training loss: 5.197216033935547, total_norm: 3.0612332820892334, lr: 5.9999999999999995e-05
Step 2086, training loss: 5.410472393035889, total_norm: 3.259150505065918, lr: 5.9999999999999995e-05
Step 2087, training loss: 4.790611743927002, total_norm: 3.856302261352539, lr: 5.9999999999999995e-05
Step 2088, training loss: 5.093875408172607, total_norm: 3.2219552993774414, lr: 5.9999999999999995e-05
Step 2089, training loss: 4.721925735473633, total_norm: 3.0987589359283447, lr: 5.9999999999999995e-05
Step 2090, training loss: 4.822018623352051, total_norm: 4.5968546867370605, lr: 5.9999999999999995e-05
Step 2091, training loss: 4.61539888381958, total_norm: 5.011560440063477, lr: 5.9999999999999995e-05
Step 2092, training loss: 4.798555374145508, total_norm: 3.5228004455566406, lr: 5.9999999999999995e-05
Step 2093, training loss: 4.199276447296143, total_norm: 3.469906806945801, lr: 5.9999999999999995e-05
Step 2094, training loss: 4.806340217590332, total_norm: 3.171499490737915, lr: 5.9999999999999995e-05
Step 2095, training loss: 4.679046630859375, total_norm: 3.7502598762512207, lr: 5.9999999999999995e-05
Step 2096, training loss: 4.566035270690918, total_norm: 3.378761053085327, lr: 5.9999999999999995e-05
Step 2097, training loss: 4.415516376495361, total_norm: 3.396146774291992, lr: 5.9999999999999995e-05
Step 2098, training loss: 3.9929749965667725, total_norm: 3.7019906044006348, lr: 5.9999999999999995e-05
Step 2099, training loss: 4.490927696228027, total_norm: 3.065415620803833, lr: 5.9999999999999995e-05
Step 2100, training loss: 4.751753807067871, total_norm: 3.54964542388916, lr: 5.9999999999999995e-05
Step 2100, validation loss: 5.273242950439453
Step 2101, training loss: 4.713003158569336, total_norm: 3.0701935291290283, lr: 5.9999999999999995e-05
Step 2102, training loss: 4.3724846839904785, total_norm: 3.3741464614868164, lr: 5.9999999999999995e-05
Step 2103, training loss: 4.878397464752197, total_norm: 3.2635080814361572, lr: 5.9999999999999995e-05
Step 2104, training loss: 4.382411956787109, total_norm: 2.7353403568267822, lr: 5.9999999999999995e-05
Step 2105, training loss: 4.332932472229004, total_norm: 2.5944042205810547, lr: 5.9999999999999995e-05
Step 2106, training loss: 4.405376434326172, total_norm: 2.6844425201416016, lr: 5.9999999999999995e-05
Step 2107, training loss: 4.7273759841918945, total_norm: 2.5329079627990723, lr: 5.9999999999999995e-05
Step 2108, training loss: 5.193881511688232, total_norm: 3.1471505165100098, lr: 5.9999999999999995e-05
Step 2109, training loss: 4.298569202423096, total_norm: 3.6150829792022705, lr: 5.9999999999999995e-05
Step 2110, training loss: 4.59237003326416, total_norm: 3.3666634559631348, lr: 5.9999999999999995e-05
Step 2111, training loss: 4.732900619506836, total_norm: 2.660301446914673, lr: 5.9999999999999995e-05
Step 2112, training loss: 5.025864124298096, total_norm: 3.3028993606567383, lr: 5.9999999999999995e-05
Step 2113, training loss: 5.2035746574401855, total_norm: 3.2848217487335205, lr: 5.9999999999999995e-05
Step 2114, training loss: 4.800975799560547, total_norm: 3.448382616043091, lr: 5.9999999999999995e-05
Step 2115, training loss: 4.785101413726807, total_norm: 2.9490597248077393, lr: 5.9999999999999995e-05
Step 2116, training loss: 5.232422351837158, total_norm: 3.0781309604644775, lr: 5.9999999999999995e-05
Step 2117, training loss: 4.820664882659912, total_norm: 4.4035749435424805, lr: 5.9999999999999995e-05
Step 2118, training loss: 4.608522415161133, total_norm: 4.758510112762451, lr: 5.9999999999999995e-05
Step 2119, training loss: 4.971850872039795, total_norm: 3.5642852783203125, lr: 5.9999999999999995e-05
Step 2120, training loss: 5.13771390914917, total_norm: 3.4128472805023193, lr: 5.9999999999999995e-05
Step 2121, training loss: 5.00140380859375, total_norm: 4.8455986976623535, lr: 5.9999999999999995e-05
Step 2122, training loss: 4.711784362792969, total_norm: 4.262328147888184, lr: 5.9999999999999995e-05
Step 2123, training loss: 4.344901084899902, total_norm: 3.8294293880462646, lr: 5.9999999999999995e-05
Step 2124, training loss: 4.983729362487793, total_norm: 2.6110312938690186, lr: 5.9999999999999995e-05
Step 2125, training loss: 4.64838981628418, total_norm: 2.891177177429199, lr: 5.9999999999999995e-05
Step 2126, training loss: 4.6787309646606445, total_norm: 2.5758841037750244, lr: 5.9999999999999995e-05
Step 2127, training loss: 4.316512107849121, total_norm: 2.9769580364227295, lr: 5.9999999999999995e-05
Step 2128, training loss: 4.764183044433594, total_norm: 2.4884958267211914, lr: 5.9999999999999995e-05
Step 2129, training loss: 4.66981315612793, total_norm: 2.6203596591949463, lr: 5.9999999999999995e-05
Step 2130, training loss: 4.894989013671875, total_norm: 2.501528739929199, lr: 5.9999999999999995e-05
Step 2131, training loss: 4.186271667480469, total_norm: 2.7075002193450928, lr: 5.9999999999999995e-05
Step 2132, training loss: 4.44022798538208, total_norm: 2.5704386234283447, lr: 5.9999999999999995e-05
Step 2133, training loss: 4.219089031219482, total_norm: 2.8579063415527344, lr: 5.9999999999999995e-05
Step 2134, training loss: 5.537250518798828, total_norm: 3.217349052429199, lr: 5.9999999999999995e-05
Step 2135, training loss: 4.864975452423096, total_norm: 3.138273000717163, lr: 5.9999999999999995e-05
Step 2136, training loss: 4.395571231842041, total_norm: 3.1346607208251953, lr: 5.9999999999999995e-05
Step 2137, training loss: 4.405436038970947, total_norm: 3.049575090408325, lr: 5.9999999999999995e-05
Step 2138, training loss: 4.911177158355713, total_norm: 3.1022818088531494, lr: 5.9999999999999995e-05
Step 2139, training loss: 4.559189796447754, total_norm: 2.784576892852783, lr: 5.9999999999999995e-05
Step 2140, training loss: 5.019315242767334, total_norm: 3.2580928802490234, lr: 5.9999999999999995e-05
Step 2141, training loss: 4.428857326507568, total_norm: 3.053659439086914, lr: 5.9999999999999995e-05
Step 2142, training loss: 5.115986347198486, total_norm: 2.8961963653564453, lr: 5.9999999999999995e-05
Step 2143, training loss: 4.25158166885376, total_norm: 2.7575812339782715, lr: 5.9999999999999995e-05
Step 2144, training loss: 4.704037666320801, total_norm: 3.2657597064971924, lr: 5.9999999999999995e-05
Step 2145, training loss: 4.480032920837402, total_norm: 3.066012144088745, lr: 5.9999999999999995e-05
Step 2146, training loss: 3.8348803520202637, total_norm: 3.020549774169922, lr: 5.9999999999999995e-05
Step 2147, training loss: 4.923032760620117, total_norm: 3.145648956298828, lr: 5.9999999999999995e-05
Step 2148, training loss: 3.681147813796997, total_norm: 2.8197152614593506, lr: 5.9999999999999995e-05
Step 2149, training loss: 4.801187515258789, total_norm: 3.191033363342285, lr: 5.9999999999999995e-05
Step 2150, training loss: 5.012174606323242, total_norm: 3.1786601543426514, lr: 5.9999999999999995e-05
Step 2150, validation loss: 5.4837870597839355
Step 2151, training loss: 3.9737987518310547, total_norm: 2.8119382858276367, lr: 5.9999999999999995e-05
Step 2152, training loss: 3.561643600463867, total_norm: 2.7525253295898438, lr: 5.9999999999999995e-05
Step 2153, training loss: 4.095592021942139, total_norm: 2.72015118598938, lr: 5.9999999999999995e-05
Step 2154, training loss: 4.477118015289307, total_norm: 2.7563040256500244, lr: 5.9999999999999995e-05
Step 2155, training loss: 4.925395965576172, total_norm: 2.7881126403808594, lr: 5.9999999999999995e-05
Step 2156, training loss: 4.161777019500732, total_norm: 2.9335954189300537, lr: 5.9999999999999995e-05
Step 2157, training loss: 3.7857327461242676, total_norm: 2.602374315261841, lr: 5.9999999999999995e-05
Step 2158, training loss: 3.835780143737793, total_norm: 2.6024398803710938, lr: 5.9999999999999995e-05
Step 2159, training loss: 3.745732545852661, total_norm: 2.682030200958252, lr: 5.9999999999999995e-05
Step 2160, training loss: 4.412444114685059, total_norm: 2.584900140762329, lr: 5.9999999999999995e-05
Step 2161, training loss: 4.241076469421387, total_norm: 2.499314308166504, lr: 5.9999999999999995e-05
Step 2162, training loss: 3.771479368209839, total_norm: 2.6036367416381836, lr: 5.9999999999999995e-05
Step 2163, training loss: 4.788480758666992, total_norm: 2.622811794281006, lr: 5.9999999999999995e-05
Step 2164, training loss: 4.503605842590332, total_norm: 2.5697855949401855, lr: 5.9999999999999995e-05
Step 2165, training loss: 4.088469982147217, total_norm: 2.4873762130737305, lr: 5.9999999999999995e-05
Step 2166, training loss: 3.991025447845459, total_norm: 2.481929302215576, lr: 5.9999999999999995e-05
Step 2167, training loss: 4.074817657470703, total_norm: 2.4948315620422363, lr: 5.9999999999999995e-05
Step 2168, training loss: 3.8570525646209717, total_norm: 2.4344489574432373, lr: 5.9999999999999995e-05
Step 2169, training loss: 4.54986047744751, total_norm: 2.674194574356079, lr: 5.9999999999999995e-05
Step 2170, training loss: 4.728811264038086, total_norm: 2.606759786605835, lr: 5.9999999999999995e-05
Step 2171, training loss: 3.666102170944214, total_norm: 2.4072763919830322, lr: 5.9999999999999995e-05
Step 2172, training loss: 3.7974159717559814, total_norm: 2.617478609085083, lr: 5.9999999999999995e-05
Step 2173, training loss: 5.115692138671875, total_norm: 3.5751960277557373, lr: 5.9999999999999995e-05
Step 2174, training loss: 4.661229610443115, total_norm: 3.0046589374542236, lr: 5.9999999999999995e-05
Step 2175, training loss: 3.774904251098633, total_norm: 3.5685698986053467, lr: 5.9999999999999995e-05
Step 2176, training loss: 3.9356095790863037, total_norm: 3.4226810932159424, lr: 5.9999999999999995e-05
Step 2177, training loss: 5.205991744995117, total_norm: 3.1558821201324463, lr: 5.9999999999999995e-05
Step 2178, training loss: 4.973073482513428, total_norm: 2.865046739578247, lr: 5.9999999999999995e-05
Step 2179, training loss: 4.173325061798096, total_norm: 3.5919387340545654, lr: 5.9999999999999995e-05
Step 2180, training loss: 4.880065441131592, total_norm: 3.1472854614257812, lr: 5.9999999999999995e-05
Step 2181, training loss: 4.23625373840332, total_norm: 2.933568239212036, lr: 5.9999999999999995e-05
Step 2182, training loss: 4.174143314361572, total_norm: 3.3078224658966064, lr: 5.9999999999999995e-05
Step 2183, training loss: 4.009890079498291, total_norm: 2.7784202098846436, lr: 5.9999999999999995e-05
Step 2184, training loss: 4.395545959472656, total_norm: 2.6822237968444824, lr: 5.9999999999999995e-05
Step 2185, training loss: 4.134057521820068, total_norm: 2.747736930847168, lr: 5.9999999999999995e-05
Step 2186, training loss: 4.678459167480469, total_norm: 2.8933398723602295, lr: 5.9999999999999995e-05
Step 2187, training loss: 4.623229026794434, total_norm: 2.847691297531128, lr: 5.9999999999999995e-05
Step 2188, training loss: 4.284232139587402, total_norm: 2.4564883708953857, lr: 5.9999999999999995e-05
Step 2189, training loss: 3.811845302581787, total_norm: 2.7853829860687256, lr: 5.9999999999999995e-05
Step 2190, training loss: 4.798881530761719, total_norm: 3.107858180999756, lr: 5.9999999999999995e-05
Step 2191, training loss: 4.9118876457214355, total_norm: 3.0537025928497314, lr: 5.9999999999999995e-05
Step 2192, training loss: 4.4372076988220215, total_norm: 2.554429292678833, lr: 5.9999999999999995e-05
Step 2193, training loss: 4.9697651863098145, total_norm: 2.8276660442352295, lr: 5.9999999999999995e-05
Step 2194, training loss: 4.429353713989258, total_norm: 2.7462992668151855, lr: 5.9999999999999995e-05
Step 2195, training loss: 5.033059597015381, total_norm: 3.0105276107788086, lr: 5.9999999999999995e-05
Step 2196, training loss: 5.159821510314941, total_norm: 3.078988552093506, lr: 5.9999999999999995e-05
Step 2197, training loss: 4.19882869720459, total_norm: 2.739025115966797, lr: 5.9999999999999995e-05
Step 2198, training loss: 4.790055751800537, total_norm: 3.0483243465423584, lr: 5.9999999999999995e-05
Step 2199, training loss: 4.721495151519775, total_norm: 2.836775541305542, lr: 5.9999999999999995e-05
Step 2200, training loss: 4.595261573791504, total_norm: 2.7773146629333496, lr: 5.9999999999999995e-05
Step 2200, validation loss: 5.499937057495117
Step 2201, training loss: 4.574525356292725, total_norm: 2.7037131786346436, lr: 5.9999999999999995e-05
Step 2202, training loss: 4.234827995300293, total_norm: 2.911695718765259, lr: 5.9999999999999995e-05
Step 2203, training loss: 4.5427021980285645, total_norm: 3.0259783267974854, lr: 5.9999999999999995e-05
Step 2204, training loss: 5.59766149520874, total_norm: 3.3766379356384277, lr: 5.9999999999999995e-05
Step 2205, training loss: 4.985953330993652, total_norm: 3.886885404586792, lr: 5.9999999999999995e-05
Step 2206, training loss: 4.824365615844727, total_norm: 4.111175060272217, lr: 5.9999999999999995e-05
Step 2207, training loss: 4.625506401062012, total_norm: 3.4318301677703857, lr: 5.9999999999999995e-05
Step 2208, training loss: 5.1906867027282715, total_norm: 3.066481113433838, lr: 5.9999999999999995e-05
Step 2209, training loss: 4.954360485076904, total_norm: 4.943706512451172, lr: 5.9999999999999995e-05
Step 2210, training loss: 4.296356201171875, total_norm: 3.4460742473602295, lr: 5.9999999999999995e-05
Step 2211, training loss: 3.9842913150787354, total_norm: 3.47078800201416, lr: 5.9999999999999995e-05
Step 2212, training loss: 4.1620707511901855, total_norm: 3.387092351913452, lr: 5.9999999999999995e-05
Step 2213, training loss: 3.9472784996032715, total_norm: 3.3093085289001465, lr: 5.9999999999999995e-05
Step 2214, training loss: 4.491746425628662, total_norm: 2.690209150314331, lr: 5.9999999999999995e-05
Step 2215, training loss: 4.707319736480713, total_norm: 2.6979939937591553, lr: 5.9999999999999995e-05
Step 2216, training loss: 4.539819717407227, total_norm: 2.8899123668670654, lr: 5.9999999999999995e-05
Step 2217, training loss: 4.795522212982178, total_norm: 2.776479482650757, lr: 5.9999999999999995e-05
Step 2218, training loss: 4.366354942321777, total_norm: 2.7414917945861816, lr: 5.9999999999999995e-05
Step 2219, training loss: 4.173303604125977, total_norm: 2.957519292831421, lr: 5.9999999999999995e-05
Step 2220, training loss: 4.491035461425781, total_norm: 2.756019353866577, lr: 5.9999999999999995e-05
Step 2221, training loss: 4.520686149597168, total_norm: 3.110128164291382, lr: 5.9999999999999995e-05
Step 2222, training loss: 4.163127899169922, total_norm: 3.069796323776245, lr: 5.9999999999999995e-05
Step 2223, training loss: 4.310608386993408, total_norm: 2.7175419330596924, lr: 5.9999999999999995e-05
Step 2224, training loss: 4.222954273223877, total_norm: 2.613698720932007, lr: 5.9999999999999995e-05
Step 2225, training loss: 4.577634334564209, total_norm: 3.0188944339752197, lr: 5.9999999999999995e-05
Step 2226, training loss: 5.186185359954834, total_norm: 2.9258885383605957, lr: 5.9999999999999995e-05
Step 2227, training loss: 4.4942827224731445, total_norm: 3.045168876647949, lr: 5.9999999999999995e-05
Step 2228, training loss: 4.6342034339904785, total_norm: 3.6237080097198486, lr: 5.9999999999999995e-05
Step 2229, training loss: 4.075396537780762, total_norm: 3.174380302429199, lr: 5.9999999999999995e-05
Step 2230, training loss: 4.529201984405518, total_norm: 2.663602113723755, lr: 5.9999999999999995e-05
Step 2231, training loss: 4.034541130065918, total_norm: 3.404107093811035, lr: 5.9999999999999995e-05
Step 2232, training loss: 4.35690450668335, total_norm: 2.81791353225708, lr: 5.9999999999999995e-05
Step 2233, training loss: 4.419148921966553, total_norm: 3.382894515991211, lr: 5.9999999999999995e-05
Step 2234, training loss: 4.332278251647949, total_norm: 2.711826801300049, lr: 5.9999999999999995e-05
Step 2235, training loss: 4.696544647216797, total_norm: 3.072723388671875, lr: 5.9999999999999995e-05
Step 2236, training loss: 4.457045078277588, total_norm: 3.0719096660614014, lr: 5.9999999999999995e-05
Step 2237, training loss: 4.580667018890381, total_norm: 3.2036659717559814, lr: 5.9999999999999995e-05
Step 2238, training loss: 4.634584903717041, total_norm: 3.3182101249694824, lr: 5.9999999999999995e-05
Step 2239, training loss: 4.58651065826416, total_norm: 2.8499703407287598, lr: 5.9999999999999995e-05
Step 2240, training loss: 4.210700511932373, total_norm: 2.7822794914245605, lr: 5.9999999999999995e-05
Step 2241, training loss: 4.546808242797852, total_norm: 2.8312861919403076, lr: 5.9999999999999995e-05
Step 2242, training loss: 4.140117168426514, total_norm: 3.317477226257324, lr: 5.9999999999999995e-05
Step 2243, training loss: 4.070112705230713, total_norm: 3.1689188480377197, lr: 5.9999999999999995e-05
Step 2244, training loss: 4.25599479675293, total_norm: 2.8365261554718018, lr: 5.9999999999999995e-05
Step 2245, training loss: 4.542782783508301, total_norm: 2.8039309978485107, lr: 5.9999999999999995e-05
Step 2246, training loss: 3.7141709327697754, total_norm: 2.9408891201019287, lr: 5.9999999999999995e-05
Step 2247, training loss: 3.631809949874878, total_norm: 2.7542240619659424, lr: 5.9999999999999995e-05
Step 2248, training loss: 4.241545677185059, total_norm: 2.6009163856506348, lr: 5.9999999999999995e-05
Step 2249, training loss: 4.030431747436523, total_norm: 2.578007459640503, lr: 5.9999999999999995e-05
Step 2250, training loss: 4.503890037536621, total_norm: 2.8453915119171143, lr: 5.9999999999999995e-05
Step 2250, validation loss: 5.521151542663574
Step 2251, training loss: 4.313060760498047, total_norm: 3.0561623573303223, lr: 5.9999999999999995e-05
Step 2252, training loss: 4.11309814453125, total_norm: 2.9406518936157227, lr: 5.9999999999999995e-05
Step 2253, training loss: 4.358200550079346, total_norm: 2.8815338611602783, lr: 5.9999999999999995e-05
Step 2254, training loss: 3.920544385910034, total_norm: 2.764845848083496, lr: 5.9999999999999995e-05
Step 2255, training loss: 4.482614517211914, total_norm: 2.859988212585449, lr: 5.9999999999999995e-05
Step 2256, training loss: 4.583502292633057, total_norm: 3.112823009490967, lr: 5.9999999999999995e-05
Step 2257, training loss: 4.3250837326049805, total_norm: 2.911597728729248, lr: 5.9999999999999995e-05
Step 2258, training loss: 4.702097415924072, total_norm: 2.635281801223755, lr: 5.9999999999999995e-05
Step 2259, training loss: 4.8192009925842285, total_norm: 2.681013584136963, lr: 5.9999999999999995e-05
Step 2260, training loss: 4.684387683868408, total_norm: 2.806588649749756, lr: 5.9999999999999995e-05
Step 2261, training loss: 4.292703628540039, total_norm: 2.843289613723755, lr: 5.9999999999999995e-05
Step 2262, training loss: 4.632481575012207, total_norm: 2.5477564334869385, lr: 5.9999999999999995e-05
Step 2263, training loss: 4.883728981018066, total_norm: 2.830559015274048, lr: 5.9999999999999995e-05
Step 2264, training loss: 4.577132225036621, total_norm: 3.1334025859832764, lr: 5.9999999999999995e-05
Step 2265, training loss: 4.060149669647217, total_norm: 2.584245443344116, lr: 5.9999999999999995e-05
Step 2266, training loss: 4.073766708374023, total_norm: 3.267669439315796, lr: 5.9999999999999995e-05
Step 2267, training loss: 4.676826000213623, total_norm: 2.890751838684082, lr: 5.9999999999999995e-05
Step 2268, training loss: 4.0936408042907715, total_norm: 3.15187668800354, lr: 5.9999999999999995e-05
Step 2269, training loss: 4.5186638832092285, total_norm: 3.7471375465393066, lr: 5.9999999999999995e-05
Step 2270, training loss: 4.254397392272949, total_norm: 3.3441638946533203, lr: 5.9999999999999995e-05
Step 2271, training loss: 3.7576873302459717, total_norm: 3.461360454559326, lr: 5.9999999999999995e-05
Step 2272, training loss: 4.967446804046631, total_norm: 2.808765411376953, lr: 5.9999999999999995e-05
Step 2273, training loss: 4.765480041503906, total_norm: 2.8451449871063232, lr: 5.9999999999999995e-05
Step 2274, training loss: 4.577253341674805, total_norm: 3.45560622215271, lr: 5.9999999999999995e-05
Step 2275, training loss: 4.860498905181885, total_norm: 3.9317679405212402, lr: 5.9999999999999995e-05
Step 2276, training loss: 4.7127604484558105, total_norm: 3.0881330966949463, lr: 5.9999999999999995e-05
Step 2277, training loss: 3.8681938648223877, total_norm: 3.423550605773926, lr: 5.9999999999999995e-05
Step 2278, training loss: 4.264073371887207, total_norm: 2.9830234050750732, lr: 5.9999999999999995e-05
Step 2279, training loss: 4.058867931365967, total_norm: 2.511193037033081, lr: 5.9999999999999995e-05
Step 2280, training loss: 3.634211778640747, total_norm: 2.8220834732055664, lr: 5.9999999999999995e-05
Step 2281, training loss: 3.9073526859283447, total_norm: 2.6504077911376953, lr: 5.9999999999999995e-05
Step 2282, training loss: 5.01838493347168, total_norm: 3.07157564163208, lr: 5.9999999999999995e-05
Step 2283, training loss: 3.1270759105682373, total_norm: 2.7332687377929688, lr: 5.9999999999999995e-05
Step 2284, training loss: 4.0144782066345215, total_norm: 3.011096477508545, lr: 5.9999999999999995e-05
Step 2285, training loss: 4.170497894287109, total_norm: 2.880964756011963, lr: 5.9999999999999995e-05
Step 2286, training loss: 4.0316267013549805, total_norm: 3.185650110244751, lr: 5.9999999999999995e-05
Step 2287, training loss: 4.238705635070801, total_norm: 2.862607479095459, lr: 5.9999999999999995e-05
Step 2288, training loss: 4.795563220977783, total_norm: 2.7270774841308594, lr: 5.9999999999999995e-05
Step 2289, training loss: 4.997189998626709, total_norm: 3.149921417236328, lr: 5.9999999999999995e-05
Step 2290, training loss: 4.803498268127441, total_norm: 3.397315740585327, lr: 5.9999999999999995e-05
Step 2291, training loss: 4.792296886444092, total_norm: 3.1865074634552, lr: 5.9999999999999995e-05
Step 2292, training loss: 4.159472942352295, total_norm: 2.90804386138916, lr: 5.9999999999999995e-05
Step 2293, training loss: 5.039291858673096, total_norm: 3.6996514797210693, lr: 5.9999999999999995e-05
Step 2294, training loss: 4.709204196929932, total_norm: 3.193418502807617, lr: 5.9999999999999995e-05
Step 2295, training loss: 4.8343000411987305, total_norm: 2.8379158973693848, lr: 5.9999999999999995e-05
Step 2296, training loss: 4.344013690948486, total_norm: 2.8213236331939697, lr: 5.9999999999999995e-05
Step 2297, training loss: 4.467806816101074, total_norm: 3.11240816116333, lr: 5.9999999999999995e-05
Step 2298, training loss: 4.971690654754639, total_norm: 3.284550905227661, lr: 5.9999999999999995e-05
Step 2299, training loss: 4.861091613769531, total_norm: 4.390084266662598, lr: 5.9999999999999995e-05
Step 2300, training loss: 5.012690544128418, total_norm: 3.3271467685699463, lr: 5.9999999999999995e-05
Step 2300, validation loss: 5.654496669769287
Step 2301, training loss: 5.001239776611328, total_norm: 3.173549175262451, lr: 5.9999999999999995e-05
Step 2302, training loss: 4.9606170654296875, total_norm: 3.2157390117645264, lr: 5.9999999999999995e-05
Step 2303, training loss: 5.017665386199951, total_norm: 3.4737114906311035, lr: 5.9999999999999995e-05
Step 2304, training loss: 5.082968235015869, total_norm: 3.286928415298462, lr: 5.9999999999999995e-05
Step 2305, training loss: 4.962582588195801, total_norm: 3.3158092498779297, lr: 5.9999999999999995e-05
Step 2306, training loss: 4.727300643920898, total_norm: 3.4226927757263184, lr: 5.9999999999999995e-05
Step 2307, training loss: 4.711149215698242, total_norm: 2.5685741901397705, lr: 5.9999999999999995e-05
Step 2308, training loss: 4.936704635620117, total_norm: 3.00233793258667, lr: 5.9999999999999995e-05
Step 2309, training loss: 5.170438766479492, total_norm: 3.2713255882263184, lr: 5.9999999999999995e-05
Step 2310, training loss: 4.694324970245361, total_norm: 3.043924570083618, lr: 5.9999999999999995e-05
Step 2311, training loss: 4.65265417098999, total_norm: 2.7645370960235596, lr: 5.9999999999999995e-05
Step 2312, training loss: 4.168351650238037, total_norm: 3.6084225177764893, lr: 5.9999999999999995e-05
Step 2313, training loss: 4.621603012084961, total_norm: 2.7671375274658203, lr: 5.9999999999999995e-05
Step 2314, training loss: 4.977284908294678, total_norm: 2.853720188140869, lr: 5.9999999999999995e-05
Step 2315, training loss: 4.8802809715271, total_norm: 2.7243311405181885, lr: 5.9999999999999995e-05
Step 2316, training loss: 5.390798091888428, total_norm: 3.330854654312134, lr: 5.9999999999999995e-05
Step 2317, training loss: 4.556663990020752, total_norm: 2.7602484226226807, lr: 5.9999999999999995e-05
Step 2318, training loss: 4.754156112670898, total_norm: 2.7324328422546387, lr: 5.9999999999999995e-05
Step 2319, training loss: 4.625746726989746, total_norm: 3.0873045921325684, lr: 5.9999999999999995e-05
Step 2320, training loss: 4.694495677947998, total_norm: 2.8913705348968506, lr: 5.9999999999999995e-05
Step 2321, training loss: 4.719318866729736, total_norm: 3.769953489303589, lr: 5.9999999999999995e-05
Step 2322, training loss: 5.035909175872803, total_norm: 3.255675792694092, lr: 5.9999999999999995e-05
Step 2323, training loss: 4.968899726867676, total_norm: 3.1735000610351562, lr: 5.9999999999999995e-05
Step 2324, training loss: 4.567864894866943, total_norm: 2.9880874156951904, lr: 5.9999999999999995e-05
Step 2325, training loss: 4.596957683563232, total_norm: 2.7259511947631836, lr: 5.9999999999999995e-05
Step 2326, training loss: 4.608053684234619, total_norm: 3.175809621810913, lr: 5.9999999999999995e-05
Step 2327, training loss: 4.3083696365356445, total_norm: 3.701510190963745, lr: 5.9999999999999995e-05
Step 2328, training loss: 4.273532390594482, total_norm: 3.409761428833008, lr: 5.9999999999999995e-05
Step 2329, training loss: 4.392983436584473, total_norm: 2.974167823791504, lr: 5.9999999999999995e-05
Step 2330, training loss: 4.634222507476807, total_norm: 3.194308280944824, lr: 5.9999999999999995e-05
Step 2331, training loss: 4.805169105529785, total_norm: 3.053276777267456, lr: 5.9999999999999995e-05
Step 2332, training loss: 4.564345359802246, total_norm: 2.9039783477783203, lr: 5.9999999999999995e-05
Step 2333, training loss: 4.830523490905762, total_norm: 3.1072826385498047, lr: 5.9999999999999995e-05
Step 2334, training loss: 5.0182108879089355, total_norm: 3.157254695892334, lr: 5.9999999999999995e-05
Step 2335, training loss: 5.023435115814209, total_norm: 3.080522298812866, lr: 5.9999999999999995e-05
Step 2336, training loss: 4.935155868530273, total_norm: 3.303131341934204, lr: 5.9999999999999995e-05
Step 2337, training loss: 5.082916736602783, total_norm: 3.28920316696167, lr: 5.9999999999999995e-05
Step 2338, training loss: 4.2485761642456055, total_norm: 3.315992832183838, lr: 5.9999999999999995e-05
Step 2339, training loss: 4.542912483215332, total_norm: 3.4944005012512207, lr: 5.9999999999999995e-05
Step 2340, training loss: 5.107510089874268, total_norm: 3.343273639678955, lr: 5.9999999999999995e-05
Step 2341, training loss: 4.6263813972473145, total_norm: 2.832021951675415, lr: 5.9999999999999995e-05
Step 2342, training loss: 4.897881507873535, total_norm: 3.193924903869629, lr: 5.9999999999999995e-05
Step 2343, training loss: 3.799844741821289, total_norm: 2.490882158279419, lr: 5.9999999999999995e-05
Step 2344, training loss: 4.834841251373291, total_norm: 3.1641151905059814, lr: 5.9999999999999995e-05
Step 2345, training loss: 5.03550386428833, total_norm: 4.207052707672119, lr: 5.9999999999999995e-05
Step 2346, training loss: 4.5483927726745605, total_norm: 2.928715229034424, lr: 5.9999999999999995e-05
Step 2347, training loss: 4.124321937561035, total_norm: 2.855989933013916, lr: 5.9999999999999995e-05
Step 2348, training loss: 4.5534820556640625, total_norm: 2.9405105113983154, lr: 5.9999999999999995e-05
Step 2349, training loss: 4.624999523162842, total_norm: 3.449699878692627, lr: 5.9999999999999995e-05
Step 2350, training loss: 4.694673538208008, total_norm: 3.385672092437744, lr: 5.9999999999999995e-05
Step 2350, validation loss: 5.629424571990967
Step 2351, training loss: 4.4130096435546875, total_norm: 3.3137176036834717, lr: 5.9999999999999995e-05
Step 2352, training loss: 4.782825469970703, total_norm: 2.7941553592681885, lr: 5.9999999999999995e-05
Step 2353, training loss: 4.184601306915283, total_norm: 2.55696964263916, lr: 5.9999999999999995e-05
Step 2354, training loss: 3.7622411251068115, total_norm: 2.5957190990448, lr: 5.9999999999999995e-05
Step 2355, training loss: 4.761695861816406, total_norm: 3.310021162033081, lr: 5.9999999999999995e-05
Step 2356, training loss: 4.838984966278076, total_norm: 3.392118215560913, lr: 5.9999999999999995e-05
Step 2357, training loss: 4.316342830657959, total_norm: 3.2721900939941406, lr: 5.9999999999999995e-05
Step 2358, training loss: 4.821961402893066, total_norm: 3.712540626525879, lr: 5.9999999999999995e-05
Step 2359, training loss: 3.477778196334839, total_norm: 3.4863672256469727, lr: 5.9999999999999995e-05
Step 2360, training loss: 3.380441904067993, total_norm: 3.1954715251922607, lr: 5.9999999999999995e-05
Step 2361, training loss: 4.031427383422852, total_norm: 2.705083131790161, lr: 5.9999999999999995e-05
Step 2362, training loss: 3.3891842365264893, total_norm: 3.0811328887939453, lr: 5.9999999999999995e-05
Step 2363, training loss: 4.49939489364624, total_norm: 3.605391263961792, lr: 5.9999999999999995e-05
Step 2364, training loss: 4.226139545440674, total_norm: 3.3338725566864014, lr: 5.9999999999999995e-05
Step 2365, training loss: 4.125903129577637, total_norm: 3.2951388359069824, lr: 5.9999999999999995e-05
Step 2366, training loss: 5.031520843505859, total_norm: 3.5410385131835938, lr: 5.9999999999999995e-05
Step 2367, training loss: 5.174808025360107, total_norm: 3.4367547035217285, lr: 5.9999999999999995e-05
Step 2368, training loss: 4.517470359802246, total_norm: 3.096742868423462, lr: 5.9999999999999995e-05
Step 2369, training loss: 4.582959175109863, total_norm: 3.26663875579834, lr: 5.9999999999999995e-05
Step 2370, training loss: 4.8853583335876465, total_norm: 3.7501862049102783, lr: 5.9999999999999995e-05
Step 2371, training loss: 4.578946590423584, total_norm: 4.584105491638184, lr: 5.9999999999999995e-05
Step 2372, training loss: 4.856948375701904, total_norm: 3.6636786460876465, lr: 5.9999999999999995e-05
Step 2373, training loss: 5.174627780914307, total_norm: 3.3795318603515625, lr: 5.9999999999999995e-05
Step 2374, training loss: 4.560775279998779, total_norm: 4.88982629776001, lr: 5.9999999999999995e-05
Step 2375, training loss: 4.650266170501709, total_norm: 4.572711944580078, lr: 5.9999999999999995e-05
Step 2376, training loss: 4.7952799797058105, total_norm: 3.1660044193267822, lr: 5.9999999999999995e-05
Step 2377, training loss: 5.383665561676025, total_norm: 3.162174701690674, lr: 5.9999999999999995e-05
Step 2378, training loss: 4.792265892028809, total_norm: 3.6483352184295654, lr: 5.9999999999999995e-05
Step 2379, training loss: 4.5394697189331055, total_norm: 3.620776414871216, lr: 5.9999999999999995e-05
Step 2380, training loss: 5.011794567108154, total_norm: 3.674567699432373, lr: 5.9999999999999995e-05
Step 2381, training loss: 4.57278299331665, total_norm: 3.4751007556915283, lr: 5.9999999999999995e-05
Step 2382, training loss: 4.5930399894714355, total_norm: 3.6713762283325195, lr: 5.9999999999999995e-05
Step 2383, training loss: 4.388070106506348, total_norm: 3.794853925704956, lr: 5.9999999999999995e-05
Step 2384, training loss: 5.689490795135498, total_norm: 3.511101722717285, lr: 5.9999999999999995e-05
Step 2385, training loss: 5.087708473205566, total_norm: 3.776474952697754, lr: 5.9999999999999995e-05
Step 2386, training loss: 4.693071365356445, total_norm: 3.4864611625671387, lr: 5.9999999999999995e-05
Step 2387, training loss: 4.472962856292725, total_norm: 3.2840797901153564, lr: 5.9999999999999995e-05
Step 2388, training loss: 4.065097332000732, total_norm: 3.02365779876709, lr: 5.9999999999999995e-05
Step 2389, training loss: 4.320960521697998, total_norm: 2.64442777633667, lr: 5.9999999999999995e-05
Step 2390, training loss: 5.042128562927246, total_norm: 3.178325891494751, lr: 5.9999999999999995e-05
Step 2391, training loss: 4.589458465576172, total_norm: 2.620697021484375, lr: 5.9999999999999995e-05
Step 2392, training loss: 4.250600814819336, total_norm: 2.532259941101074, lr: 5.9999999999999995e-05
Step 2393, training loss: 4.470232963562012, total_norm: 2.604800224304199, lr: 5.9999999999999995e-05
Step 2394, training loss: 4.060182094573975, total_norm: 2.5978987216949463, lr: 5.9999999999999995e-05
Step 2395, training loss: 4.152449131011963, total_norm: 2.6306445598602295, lr: 5.9999999999999995e-05
Step 2396, training loss: 5.574843406677246, total_norm: 3.1373677253723145, lr: 5.9999999999999995e-05
Step 2397, training loss: 4.642489910125732, total_norm: 3.2373411655426025, lr: 5.9999999999999995e-05
Step 2398, training loss: 4.146868705749512, total_norm: 2.972137689590454, lr: 5.9999999999999995e-05
Step 2399, training loss: 5.4579644203186035, total_norm: 4.0855231285095215, lr: 5.9999999999999995e-05
Step 2400, training loss: 4.309929847717285, total_norm: 3.797926664352417, lr: 5.9999999999999995e-05
Step 2400, validation loss: 5.672078609466553
Step 2401, training loss: 4.127604961395264, total_norm: 3.9822888374328613, lr: 5.9999999999999995e-05
Step 2402, training loss: 4.559768199920654, total_norm: 3.1713807582855225, lr: 5.9999999999999995e-05
Step 2403, training loss: 4.305517196655273, total_norm: 2.7519702911376953, lr: 5.9999999999999995e-05
Step 2404, training loss: 4.865792751312256, total_norm: 3.044795274734497, lr: 5.9999999999999995e-05
Step 2405, training loss: 4.309553146362305, total_norm: 2.5136942863464355, lr: 5.9999999999999995e-05
Step 2406, training loss: 4.708714008331299, total_norm: 3.1790449619293213, lr: 5.9999999999999995e-05
Step 2407, training loss: 4.43334436416626, total_norm: 3.172895669937134, lr: 5.9999999999999995e-05
Step 2408, training loss: 4.924560546875, total_norm: 3.9683873653411865, lr: 5.9999999999999995e-05
Step 2409, training loss: 4.388564586639404, total_norm: 3.702927827835083, lr: 5.9999999999999995e-05
Step 2410, training loss: 4.525050640106201, total_norm: 3.201009750366211, lr: 5.9999999999999995e-05
Step 2411, training loss: 4.190124034881592, total_norm: 3.202181100845337, lr: 5.9999999999999995e-05
Step 2412, training loss: 5.233335018157959, total_norm: 3.5976619720458984, lr: 5.9999999999999995e-05
Step 2413, training loss: 4.941219806671143, total_norm: 4.282318592071533, lr: 5.9999999999999995e-05
Step 2414, training loss: 4.5355401039123535, total_norm: 3.509650707244873, lr: 5.9999999999999995e-05
Step 2415, training loss: 4.970126152038574, total_norm: 3.552738904953003, lr: 5.9999999999999995e-05
Step 2416, training loss: 4.642675399780273, total_norm: 3.1731441020965576, lr: 5.9999999999999995e-05
Step 2417, training loss: 4.664998531341553, total_norm: 3.0762531757354736, lr: 5.9999999999999995e-05
Step 2418, training loss: 4.42807674407959, total_norm: 3.2101035118103027, lr: 5.9999999999999995e-05
Step 2419, training loss: 4.312004566192627, total_norm: 2.9945268630981445, lr: 5.9999999999999995e-05
Step 2420, training loss: 4.970361709594727, total_norm: 3.144082546234131, lr: 5.9999999999999995e-05
Step 2421, training loss: 4.210814476013184, total_norm: 2.911823034286499, lr: 5.9999999999999995e-05
Step 2422, training loss: 4.505568981170654, total_norm: 2.6207056045532227, lr: 5.9999999999999995e-05
Step 2423, training loss: 4.261773109436035, total_norm: 2.5715341567993164, lr: 5.9999999999999995e-05
Step 2424, training loss: 3.65054988861084, total_norm: 2.9831478595733643, lr: 5.9999999999999995e-05
Step 2425, training loss: 4.145723819732666, total_norm: 2.541050910949707, lr: 5.9999999999999995e-05
Step 2426, training loss: 4.499476432800293, total_norm: 2.8337972164154053, lr: 5.9999999999999995e-05
Step 2427, training loss: 4.455992221832275, total_norm: 2.739476442337036, lr: 5.9999999999999995e-05
Step 2428, training loss: 4.0897088050842285, total_norm: 2.4667534828186035, lr: 5.9999999999999995e-05
Step 2429, training loss: 3.9476091861724854, total_norm: 2.91058611869812, lr: 5.9999999999999995e-05
Step 2430, training loss: 4.021899223327637, total_norm: 2.540501832962036, lr: 5.9999999999999995e-05
Step 2431, training loss: 5.070724964141846, total_norm: 2.9684083461761475, lr: 5.9999999999999995e-05
Step 2432, training loss: 4.346006393432617, total_norm: 2.696370840072632, lr: 5.9999999999999995e-05
Step 2433, training loss: 4.0797529220581055, total_norm: 2.662278890609741, lr: 5.9999999999999995e-05
Step 2434, training loss: 4.47750186920166, total_norm: 2.9183480739593506, lr: 5.9999999999999995e-05
Step 2435, training loss: 4.408116817474365, total_norm: 2.8473753929138184, lr: 5.9999999999999995e-05
Step 2436, training loss: 4.1776957511901855, total_norm: 3.4285600185394287, lr: 5.9999999999999995e-05
Step 2437, training loss: 4.42601203918457, total_norm: 3.7606289386749268, lr: 5.9999999999999995e-05
Step 2438, training loss: 4.790355205535889, total_norm: 3.050771474838257, lr: 5.9999999999999995e-05
Step 2439, training loss: 4.9937825202941895, total_norm: 3.982233762741089, lr: 5.9999999999999995e-05
Step 2440, training loss: 4.731832027435303, total_norm: 2.976388692855835, lr: 5.9999999999999995e-05
Step 2441, training loss: 5.324376106262207, total_norm: 3.437490463256836, lr: 5.9999999999999995e-05
Step 2442, training loss: 4.534599781036377, total_norm: 3.407832145690918, lr: 5.9999999999999995e-05
Step 2443, training loss: 4.982825756072998, total_norm: 2.6997618675231934, lr: 5.9999999999999995e-05
Step 2444, training loss: 4.472891330718994, total_norm: 3.0224504470825195, lr: 5.9999999999999995e-05
Step 2445, training loss: 5.091657638549805, total_norm: 3.0324347019195557, lr: 5.9999999999999995e-05
Step 2446, training loss: 4.090404033660889, total_norm: 3.823432445526123, lr: 5.9999999999999995e-05
Step 2447, training loss: 4.433699607849121, total_norm: 3.027946949005127, lr: 5.9999999999999995e-05
Step 2448, training loss: 4.341762542724609, total_norm: 3.0422849655151367, lr: 5.9999999999999995e-05
Step 2449, training loss: 4.843249797821045, total_norm: 2.9274888038635254, lr: 5.9999999999999995e-05
Step 2450, training loss: 4.4179229736328125, total_norm: 2.6507256031036377, lr: 5.9999999999999995e-05
Step 2450, validation loss: 5.636777877807617
Step 2451, training loss: 4.709587574005127, total_norm: 3.419942855834961, lr: 5.9999999999999995e-05
Step 2452, training loss: 4.693070888519287, total_norm: 3.5853326320648193, lr: 5.9999999999999995e-05
Step 2453, training loss: 4.370358467102051, total_norm: 3.3370063304901123, lr: 5.9999999999999995e-05
Step 2454, training loss: 4.3184895515441895, total_norm: 3.315049648284912, lr: 5.9999999999999995e-05
Step 2455, training loss: 4.256709575653076, total_norm: 3.175105333328247, lr: 5.9999999999999995e-05
Step 2456, training loss: 4.218038082122803, total_norm: 3.766906499862671, lr: 5.9999999999999995e-05
Step 2457, training loss: 4.2873992919921875, total_norm: 2.9847252368927, lr: 5.9999999999999995e-05
Step 2458, training loss: 4.019474029541016, total_norm: 2.868881940841675, lr: 5.9999999999999995e-05
Step 2459, training loss: 4.4008469581604, total_norm: 3.0366768836975098, lr: 5.9999999999999995e-05
Step 2460, training loss: 4.468661785125732, total_norm: 3.054697036743164, lr: 5.9999999999999995e-05
Step 2461, training loss: 4.443563461303711, total_norm: 2.963712692260742, lr: 5.9999999999999995e-05
Step 2462, training loss: 5.084698677062988, total_norm: 3.3299901485443115, lr: 5.9999999999999995e-05
Step 2463, training loss: 4.679571151733398, total_norm: 2.8161919116973877, lr: 5.9999999999999995e-05
Step 2464, training loss: 4.700669765472412, total_norm: 3.28822922706604, lr: 5.9999999999999995e-05
Step 2465, training loss: 5.082854270935059, total_norm: 2.9661896228790283, lr: 5.9999999999999995e-05
Step 2466, training loss: 4.414924621582031, total_norm: 3.0081191062927246, lr: 5.9999999999999995e-05
Step 2467, training loss: 4.9953837394714355, total_norm: 3.2534372806549072, lr: 5.9999999999999995e-05
Step 2468, training loss: 4.8756279945373535, total_norm: 3.2498178482055664, lr: 5.9999999999999995e-05
Step 2469, training loss: 4.872744083404541, total_norm: 3.085085868835449, lr: 5.9999999999999995e-05
Step 2470, training loss: 4.636102676391602, total_norm: 2.746950387954712, lr: 5.9999999999999995e-05
Step 2471, training loss: 4.729706764221191, total_norm: 2.9069483280181885, lr: 5.9999999999999995e-05
Step 2472, training loss: 4.866160869598389, total_norm: 2.9142301082611084, lr: 5.9999999999999995e-05
Step 2473, training loss: 4.226417541503906, total_norm: 3.0181338787078857, lr: 5.9999999999999995e-05
Step 2474, training loss: 3.928104877471924, total_norm: 3.5475940704345703, lr: 5.9999999999999995e-05
Step 2475, training loss: 4.147695064544678, total_norm: 3.0865583419799805, lr: 5.9999999999999995e-05
Step 2476, training loss: 4.634554862976074, total_norm: 3.038698434829712, lr: 5.9999999999999995e-05
Step 2477, training loss: 4.620118618011475, total_norm: 3.0579311847686768, lr: 5.9999999999999995e-05
Step 2478, training loss: 4.6691718101501465, total_norm: 3.0837156772613525, lr: 5.9999999999999995e-05
Step 2479, training loss: 5.28667688369751, total_norm: 3.217398166656494, lr: 5.9999999999999995e-05
Step 2480, training loss: 4.64361047744751, total_norm: 3.0137436389923096, lr: 5.9999999999999995e-05
Step 2481, training loss: 4.313906192779541, total_norm: 3.170558214187622, lr: 5.9999999999999995e-05
Step 2482, training loss: 4.878201007843018, total_norm: 3.2753000259399414, lr: 5.9999999999999995e-05
Step 2483, training loss: 4.425560474395752, total_norm: 2.9847285747528076, lr: 5.9999999999999995e-05
Step 2484, training loss: 4.63208532333374, total_norm: 3.068255662918091, lr: 5.9999999999999995e-05
Step 2485, training loss: 4.5992584228515625, total_norm: 3.049652338027954, lr: 5.9999999999999995e-05
Step 2486, training loss: 4.106286525726318, total_norm: 3.4216322898864746, lr: 5.9999999999999995e-05
Step 2487, training loss: 4.053736209869385, total_norm: 3.3388822078704834, lr: 5.9999999999999995e-05
Step 2488, training loss: 3.3602142333984375, total_norm: 6.719936847686768, lr: 5.9999999999999995e-05
Step 2489, training loss: 3.773954391479492, total_norm: 4.451617240905762, lr: 5.9999999999999995e-05
Step 2490, training loss: 3.8567378520965576, total_norm: 3.6743319034576416, lr: 5.9999999999999995e-05
Step 2491, training loss: 5.00933837890625, total_norm: 3.209860324859619, lr: 5.9999999999999995e-05
Step 2492, training loss: 4.98652458190918, total_norm: 3.5297582149505615, lr: 5.9999999999999995e-05
Step 2493, training loss: 4.469635486602783, total_norm: 3.708307981491089, lr: 5.9999999999999995e-05
Step 2494, training loss: 4.861745834350586, total_norm: 3.718592882156372, lr: 5.9999999999999995e-05
Step 2495, training loss: 4.535299301147461, total_norm: 3.5477983951568604, lr: 5.9999999999999995e-05
Step 2496, training loss: 4.424738883972168, total_norm: 3.341710329055786, lr: 5.9999999999999995e-05
Step 2497, training loss: 4.666759490966797, total_norm: 3.489677667617798, lr: 5.9999999999999995e-05
Step 2498, training loss: 4.224794864654541, total_norm: 3.07832670211792, lr: 5.9999999999999995e-05
Step 2499, training loss: 4.133154392242432, total_norm: 3.1387927532196045, lr: 5.9999999999999995e-05
Step 2500, training loss: 4.453113555908203, total_norm: 3.537738800048828, lr: 5.9999999999999995e-05
Step 2500, validation loss: 5.685946464538574
Step 2501, training loss: 4.2754716873168945, total_norm: 2.7444727420806885, lr: 5.9999999999999995e-05
Step 2502, training loss: 4.561120986938477, total_norm: 2.8150267601013184, lr: 5.9999999999999995e-05
Step 2503, training loss: 4.431891918182373, total_norm: 3.454129934310913, lr: 5.9999999999999995e-05
Step 2504, training loss: 4.278830528259277, total_norm: 4.3998942375183105, lr: 5.9999999999999995e-05
Step 2505, training loss: 4.163447856903076, total_norm: 3.042773723602295, lr: 5.9999999999999995e-05
Step 2506, training loss: 4.517514228820801, total_norm: 3.281818389892578, lr: 5.9999999999999995e-05
Step 2507, training loss: 4.292050361633301, total_norm: 2.99471378326416, lr: 5.9999999999999995e-05
Step 2508, training loss: 4.251585006713867, total_norm: 3.001556873321533, lr: 5.9999999999999995e-05
Step 2509, training loss: 4.51395845413208, total_norm: 2.9783904552459717, lr: 5.9999999999999995e-05
Step 2510, training loss: 4.170459270477295, total_norm: 2.9969873428344727, lr: 5.9999999999999995e-05
Step 2511, training loss: 4.159128665924072, total_norm: 3.215545177459717, lr: 5.9999999999999995e-05
Step 2512, training loss: 4.554692268371582, total_norm: 3.123586654663086, lr: 5.9999999999999995e-05
Step 2513, training loss: 4.706330299377441, total_norm: 3.0650856494903564, lr: 5.9999999999999995e-05
Step 2514, training loss: 4.039679527282715, total_norm: 2.6475939750671387, lr: 5.9999999999999995e-05
Step 2515, training loss: 4.017960071563721, total_norm: 2.6898772716522217, lr: 5.9999999999999995e-05
Step 2516, training loss: 4.1972503662109375, total_norm: 2.6424007415771484, lr: 5.9999999999999995e-05
Step 2517, training loss: 4.495460033416748, total_norm: 2.879230260848999, lr: 5.9999999999999995e-05
Step 2518, training loss: 4.4586615562438965, total_norm: 2.757894992828369, lr: 5.9999999999999995e-05
Step 2519, training loss: 5.064252853393555, total_norm: 3.118562936782837, lr: 5.9999999999999995e-05
Step 2520, training loss: 4.398194789886475, total_norm: 2.8303842544555664, lr: 5.9999999999999995e-05
Step 2521, training loss: 4.098027229309082, total_norm: 2.670257329940796, lr: 5.9999999999999995e-05
Step 2522, training loss: 3.6412603855133057, total_norm: 2.9688217639923096, lr: 5.9999999999999995e-05
Step 2523, training loss: 3.821106433868408, total_norm: 2.8449957370758057, lr: 5.9999999999999995e-05
Step 2524, training loss: 4.319845676422119, total_norm: 2.838681697845459, lr: 5.9999999999999995e-05
Step 2525, training loss: 4.623538970947266, total_norm: 3.139876127243042, lr: 5.9999999999999995e-05
Step 2526, training loss: 4.931212425231934, total_norm: 3.2369673252105713, lr: 5.9999999999999995e-05
Step 2527, training loss: 4.7277021408081055, total_norm: 3.4441134929656982, lr: 5.9999999999999995e-05
Step 2528, training loss: 5.5675153732299805, total_norm: 4.048879146575928, lr: 5.9999999999999995e-05
Step 2529, training loss: 4.851136684417725, total_norm: 3.72363018989563, lr: 5.9999999999999995e-05
Step 2530, training loss: 5.049479961395264, total_norm: 4.1152262687683105, lr: 5.9999999999999995e-05
Step 2531, training loss: 4.813898086547852, total_norm: 3.4013357162475586, lr: 5.9999999999999995e-05
Step 2532, training loss: 5.08693265914917, total_norm: 3.5311758518218994, lr: 5.9999999999999995e-05
Step 2533, training loss: 4.88603401184082, total_norm: 3.4662792682647705, lr: 5.9999999999999995e-05
Step 2534, training loss: 4.780777931213379, total_norm: 3.539590358734131, lr: 5.9999999999999995e-05
Step 2535, training loss: 4.691946983337402, total_norm: 3.346529960632324, lr: 5.9999999999999995e-05
Step 2536, training loss: 5.09923791885376, total_norm: 3.2345454692840576, lr: 5.9999999999999995e-05
Step 2537, training loss: 4.882685661315918, total_norm: 3.0085840225219727, lr: 5.9999999999999995e-05
Step 2538, training loss: 4.445346832275391, total_norm: 2.8340508937835693, lr: 5.9999999999999995e-05
Step 2539, training loss: 4.4126410484313965, total_norm: 3.1457316875457764, lr: 5.9999999999999995e-05
Step 2540, training loss: 4.861693382263184, total_norm: 2.9847042560577393, lr: 5.9999999999999995e-05
Step 2541, training loss: 4.203275203704834, total_norm: 3.2243261337280273, lr: 5.9999999999999995e-05
Step 2542, training loss: 4.5030293464660645, total_norm: 3.081954002380371, lr: 5.9999999999999995e-05
Step 2543, training loss: 4.855286598205566, total_norm: 3.147491455078125, lr: 5.9999999999999995e-05
Step 2544, training loss: 4.515686511993408, total_norm: 3.0198910236358643, lr: 5.9999999999999995e-05
Step 2545, training loss: 4.678268909454346, total_norm: 2.873349666595459, lr: 5.9999999999999995e-05
Step 2546, training loss: 4.538743495941162, total_norm: 2.9131317138671875, lr: 5.9999999999999995e-05
Step 2547, training loss: 4.83971643447876, total_norm: 3.2493202686309814, lr: 5.9999999999999995e-05
Step 2548, training loss: 4.476736068725586, total_norm: 2.809570789337158, lr: 5.9999999999999995e-05
Step 2549, training loss: 4.180987358093262, total_norm: 2.8984458446502686, lr: 5.9999999999999995e-05
Step 2550, training loss: 4.249675750732422, total_norm: 2.973008394241333, lr: 5.9999999999999995e-05
Step 2550, validation loss: 5.536986351013184
Step 2551, training loss: 4.557241916656494, total_norm: 3.2178244590759277, lr: 5.9999999999999995e-05
Step 2552, training loss: 4.426609992980957, total_norm: 3.015807867050171, lr: 5.9999999999999995e-05
Step 2553, training loss: 4.799387454986572, total_norm: 3.0046370029449463, lr: 5.9999999999999995e-05
Step 2554, training loss: 4.982180595397949, total_norm: 3.6432714462280273, lr: 5.9999999999999995e-05
Step 2555, training loss: 4.733144760131836, total_norm: 3.041045665740967, lr: 5.9999999999999995e-05
Step 2556, training loss: 4.824702739715576, total_norm: 2.7620739936828613, lr: 5.9999999999999995e-05
Step 2557, training loss: 4.225621223449707, total_norm: 2.9732112884521484, lr: 5.9999999999999995e-05
Step 2558, training loss: 5.149292945861816, total_norm: 3.053352117538452, lr: 5.9999999999999995e-05
Step 2559, training loss: 4.735422611236572, total_norm: 2.8347952365875244, lr: 5.9999999999999995e-05
Step 2560, training loss: 4.529772758483887, total_norm: 3.0863420963287354, lr: 5.9999999999999995e-05
Step 2561, training loss: 5.433051109313965, total_norm: 2.9557077884674072, lr: 5.9999999999999995e-05
Step 2562, training loss: 5.639835357666016, total_norm: 4.691275119781494, lr: 5.9999999999999995e-05
Step 2563, training loss: 4.978784084320068, total_norm: 3.9771246910095215, lr: 5.9999999999999995e-05
Step 2564, training loss: 5.125515460968018, total_norm: 2.9731240272521973, lr: 5.9999999999999995e-05
Step 2565, training loss: 5.209307670593262, total_norm: 3.385874032974243, lr: 5.9999999999999995e-05
Step 2566, training loss: 5.438755989074707, total_norm: 3.290004014968872, lr: 5.9999999999999995e-05
Step 2567, training loss: 5.03909158706665, total_norm: 3.439248561859131, lr: 5.9999999999999995e-05
Step 2568, training loss: 4.556920528411865, total_norm: 3.345364570617676, lr: 5.9999999999999995e-05
Step 2569, training loss: 4.702342987060547, total_norm: 3.1868858337402344, lr: 5.9999999999999995e-05
Step 2570, training loss: 5.0426506996154785, total_norm: 3.618967056274414, lr: 5.9999999999999995e-05
Step 2571, training loss: 4.9646477699279785, total_norm: 3.7100400924682617, lr: 5.9999999999999995e-05
Step 2572, training loss: 5.175184726715088, total_norm: 3.175889253616333, lr: 5.9999999999999995e-05
Step 2573, training loss: 4.519638538360596, total_norm: 2.9979820251464844, lr: 5.9999999999999995e-05
Step 2574, training loss: 5.285830020904541, total_norm: 3.0984766483306885, lr: 5.9999999999999995e-05
Step 2575, training loss: 5.158277988433838, total_norm: 3.8329174518585205, lr: 5.9999999999999995e-05
Step 2576, training loss: 4.7100982666015625, total_norm: 3.593391180038452, lr: 5.9999999999999995e-05
Step 2577, training loss: 4.383825302124023, total_norm: 3.676032066345215, lr: 5.9999999999999995e-05
Step 2578, training loss: 4.9928107261657715, total_norm: 2.971081256866455, lr: 5.9999999999999995e-05
Step 2579, training loss: 4.186694622039795, total_norm: 2.824925661087036, lr: 5.9999999999999995e-05
Step 2580, training loss: 4.224808692932129, total_norm: 3.12060546875, lr: 5.9999999999999995e-05
Step 2581, training loss: 4.897940635681152, total_norm: 2.8291327953338623, lr: 5.9999999999999995e-05
Step 2582, training loss: 4.446146011352539, total_norm: 2.888279914855957, lr: 5.9999999999999995e-05
Step 2583, training loss: 4.444528102874756, total_norm: 3.2605366706848145, lr: 5.9999999999999995e-05
Step 2584, training loss: 4.548807621002197, total_norm: 2.897739887237549, lr: 5.9999999999999995e-05
Step 2585, training loss: 5.123528003692627, total_norm: 3.2948129177093506, lr: 5.9999999999999995e-05
Step 2586, training loss: 4.175017356872559, total_norm: 2.9474873542785645, lr: 5.9999999999999995e-05
Step 2587, training loss: 4.372196197509766, total_norm: 2.7330148220062256, lr: 5.9999999999999995e-05
Step 2588, training loss: 4.3435211181640625, total_norm: 2.9923579692840576, lr: 5.9999999999999995e-05
Step 2589, training loss: 4.37209415435791, total_norm: 2.808366298675537, lr: 5.9999999999999995e-05
Step 2590, training loss: 5.231700897216797, total_norm: 3.0770423412323, lr: 5.9999999999999995e-05
Step 2591, training loss: 4.353837490081787, total_norm: 2.6400187015533447, lr: 5.9999999999999995e-05
Step 2592, training loss: 4.758408546447754, total_norm: 3.253615140914917, lr: 5.9999999999999995e-05
Step 2593, training loss: 4.476901054382324, total_norm: 3.645035743713379, lr: 5.9999999999999995e-05
Step 2594, training loss: 4.6319756507873535, total_norm: 3.3556594848632812, lr: 5.9999999999999995e-05
Step 2595, training loss: 4.884286403656006, total_norm: 2.9565012454986572, lr: 5.9999999999999995e-05
Step 2596, training loss: 4.7832722663879395, total_norm: 3.070631265640259, lr: 5.9999999999999995e-05
Step 2597, training loss: 4.257079124450684, total_norm: 2.6974997520446777, lr: 5.9999999999999995e-05
Step 2598, training loss: 5.148139953613281, total_norm: 3.222093105316162, lr: 5.9999999999999995e-05
Step 2599, training loss: 5.464359283447266, total_norm: 3.8950159549713135, lr: 5.9999999999999995e-05
Step 2600, training loss: 5.386724472045898, total_norm: 3.8039913177490234, lr: 5.9999999999999995e-05
Step 2600, validation loss: 5.464803218841553
Step 2601, training loss: 4.546407699584961, total_norm: 3.645533561706543, lr: 5.9999999999999995e-05
Step 2602, training loss: 4.153305530548096, total_norm: 3.064438819885254, lr: 5.9999999999999995e-05
Step 2603, training loss: 4.668417453765869, total_norm: 3.1155946254730225, lr: 5.9999999999999995e-05
Step 2604, training loss: 4.251358985900879, total_norm: 3.033186674118042, lr: 5.9999999999999995e-05
Step 2605, training loss: 4.477564811706543, total_norm: 3.124877691268921, lr: 5.9999999999999995e-05
Step 2606, training loss: 4.921975135803223, total_norm: 3.859520196914673, lr: 5.9999999999999995e-05
Step 2607, training loss: 5.088376522064209, total_norm: 4.66978120803833, lr: 5.9999999999999995e-05
Step 2608, training loss: 4.61665153503418, total_norm: 4.075546741485596, lr: 5.9999999999999995e-05
Step 2609, training loss: 4.524670124053955, total_norm: 4.922713756561279, lr: 5.9999999999999995e-05
Step 2610, training loss: 4.575164318084717, total_norm: 4.8661370277404785, lr: 5.9999999999999995e-05
Step 2611, training loss: 4.8327202796936035, total_norm: 5.710488796234131, lr: 5.9999999999999995e-05
Step 2612, training loss: 4.898758411407471, total_norm: 4.299412727355957, lr: 5.9999999999999995e-05
Step 2613, training loss: 5.001964092254639, total_norm: 4.043186187744141, lr: 5.9999999999999995e-05
Step 2614, training loss: 5.27067756652832, total_norm: 3.8862555027008057, lr: 5.9999999999999995e-05
Step 2615, training loss: 4.56431770324707, total_norm: 3.918935775756836, lr: 5.9999999999999995e-05
Step 2616, training loss: 4.881855010986328, total_norm: 3.13700795173645, lr: 5.9999999999999995e-05
Step 2617, training loss: 4.485811710357666, total_norm: 2.8420157432556152, lr: 5.9999999999999995e-05
Step 2618, training loss: 4.585504055023193, total_norm: 3.60546612739563, lr: 5.9999999999999995e-05
Step 2619, training loss: 4.368733882904053, total_norm: 4.585489273071289, lr: 5.9999999999999995e-05
Step 2620, training loss: 4.565920829772949, total_norm: 3.822204351425171, lr: 5.9999999999999995e-05
Step 2621, training loss: 3.9048774242401123, total_norm: 3.341095447540283, lr: 5.9999999999999995e-05
Step 2622, training loss: 4.504593849182129, total_norm: 3.3148269653320312, lr: 5.9999999999999995e-05
Step 2623, training loss: 4.382352828979492, total_norm: 3.4170713424682617, lr: 5.9999999999999995e-05
Step 2624, training loss: 4.334975719451904, total_norm: 3.1746151447296143, lr: 5.9999999999999995e-05
Step 2625, training loss: 4.19825553894043, total_norm: 3.6793441772460938, lr: 5.9999999999999995e-05
Step 2626, training loss: 3.752208948135376, total_norm: 4.069519519805908, lr: 5.9999999999999995e-05
Step 2627, training loss: 4.3070549964904785, total_norm: 3.3702785968780518, lr: 5.9999999999999995e-05
Step 2628, training loss: 4.566896915435791, total_norm: 3.9834511280059814, lr: 5.9999999999999995e-05
Step 2629, training loss: 4.48698091506958, total_norm: 3.256864070892334, lr: 5.9999999999999995e-05
Step 2630, training loss: 4.19465970993042, total_norm: 4.303339004516602, lr: 5.9999999999999995e-05
Step 2631, training loss: 4.701616287231445, total_norm: 3.397660732269287, lr: 5.9999999999999995e-05
Step 2632, training loss: 4.216696739196777, total_norm: 2.728991985321045, lr: 5.9999999999999995e-05
Step 2633, training loss: 4.1502556800842285, total_norm: 2.855928659439087, lr: 5.9999999999999995e-05
Step 2634, training loss: 4.229044437408447, total_norm: 3.0186190605163574, lr: 5.9999999999999995e-05
Step 2635, training loss: 4.584061622619629, total_norm: 2.757841110229492, lr: 5.9999999999999995e-05
Step 2636, training loss: 4.998286724090576, total_norm: 3.5243043899536133, lr: 5.9999999999999995e-05
Step 2637, training loss: 4.111754417419434, total_norm: 5.462734222412109, lr: 5.9999999999999995e-05
Step 2638, training loss: 4.428030014038086, total_norm: 3.941959857940674, lr: 5.9999999999999995e-05
Step 2639, training loss: 4.523026466369629, total_norm: 2.7456753253936768, lr: 5.9999999999999995e-05
Step 2640, training loss: 4.889278411865234, total_norm: 3.8344333171844482, lr: 5.9999999999999995e-05
Step 2641, training loss: 5.051878929138184, total_norm: 3.326479434967041, lr: 5.9999999999999995e-05
Step 2642, training loss: 4.664922714233398, total_norm: 3.728933334350586, lr: 5.9999999999999995e-05
Step 2643, training loss: 4.600764751434326, total_norm: 3.177502155303955, lr: 5.9999999999999995e-05
Step 2644, training loss: 5.098341464996338, total_norm: 3.625530481338501, lr: 5.9999999999999995e-05
Step 2645, training loss: 4.642983436584473, total_norm: 4.099725723266602, lr: 5.9999999999999995e-05
Step 2646, training loss: 4.402595520019531, total_norm: 4.824307441711426, lr: 5.9999999999999995e-05
Step 2647, training loss: 4.75850248336792, total_norm: 3.7518506050109863, lr: 5.9999999999999995e-05
Step 2648, training loss: 4.9737868309021, total_norm: 3.8179104328155518, lr: 5.9999999999999995e-05
Step 2649, training loss: 4.761260986328125, total_norm: 5.33027982711792, lr: 5.9999999999999995e-05
Step 2650, training loss: 4.53402853012085, total_norm: 4.634488105773926, lr: 5.9999999999999995e-05
Step 2650, validation loss: 5.422188758850098
Step 2651, training loss: 4.141366958618164, total_norm: 3.6487042903900146, lr: 5.9999999999999995e-05
Step 2652, training loss: 4.822717189788818, total_norm: 3.011765480041504, lr: 5.9999999999999995e-05
Step 2653, training loss: 4.4379563331604, total_norm: 2.850966453552246, lr: 5.9999999999999995e-05
Step 2654, training loss: 4.508347988128662, total_norm: 2.8725433349609375, lr: 5.9999999999999995e-05
Step 2655, training loss: 4.146972179412842, total_norm: 3.54668927192688, lr: 5.9999999999999995e-05
Step 2656, training loss: 4.601676940917969, total_norm: 2.9083051681518555, lr: 5.9999999999999995e-05
Step 2657, training loss: 4.505542278289795, total_norm: 2.9422147274017334, lr: 5.9999999999999995e-05
Step 2658, training loss: 4.735414981842041, total_norm: 2.857356071472168, lr: 5.9999999999999995e-05
Step 2659, training loss: 4.032688617706299, total_norm: 3.149651527404785, lr: 5.9999999999999995e-05
Step 2660, training loss: 4.290274620056152, total_norm: 2.783336877822876, lr: 5.9999999999999995e-05
Step 2661, training loss: 4.092763900756836, total_norm: 3.1336021423339844, lr: 5.9999999999999995e-05
Step 2662, training loss: 5.39565896987915, total_norm: 3.407728910446167, lr: 5.9999999999999995e-05
Step 2663, training loss: 4.680073261260986, total_norm: 3.5272390842437744, lr: 5.9999999999999995e-05
Step 2664, training loss: 4.210354804992676, total_norm: 3.3619227409362793, lr: 5.9999999999999995e-05
Step 2665, training loss: 4.2469072341918945, total_norm: 3.207368850708008, lr: 5.9999999999999995e-05
Step 2666, training loss: 4.776554107666016, total_norm: 3.5623254776000977, lr: 5.9999999999999995e-05
Step 2667, training loss: 4.436052322387695, total_norm: 3.3153538703918457, lr: 5.9999999999999995e-05
Step 2668, training loss: 4.862891674041748, total_norm: 2.8394577503204346, lr: 5.9999999999999995e-05
Step 2669, training loss: 4.301652431488037, total_norm: 3.144033193588257, lr: 5.9999999999999995e-05
Step 2670, training loss: 4.970947742462158, total_norm: 3.1034443378448486, lr: 5.9999999999999995e-05
Step 2671, training loss: 4.109015464782715, total_norm: 3.1799654960632324, lr: 5.9999999999999995e-05
Step 2672, training loss: 4.584125518798828, total_norm: 3.6156318187713623, lr: 5.9999999999999995e-05
Step 2673, training loss: 4.329361915588379, total_norm: 3.128429412841797, lr: 5.9999999999999995e-05
Step 2674, training loss: 3.7100045680999756, total_norm: 3.1275277137756348, lr: 5.9999999999999995e-05
Step 2675, training loss: 4.767183780670166, total_norm: 3.473604917526245, lr: 5.9999999999999995e-05
Step 2676, training loss: 3.520442008972168, total_norm: 2.806649923324585, lr: 5.9999999999999995e-05
Step 2677, training loss: 4.628880977630615, total_norm: 3.322774887084961, lr: 5.9999999999999995e-05
Step 2678, training loss: 4.882679462432861, total_norm: 3.5624985694885254, lr: 5.9999999999999995e-05
Step 2679, training loss: 3.8306095600128174, total_norm: 3.155407667160034, lr: 5.9999999999999995e-05
Step 2680, training loss: 3.401141405105591, total_norm: 2.832263708114624, lr: 5.9999999999999995e-05
Step 2681, training loss: 3.9428701400756836, total_norm: 3.0943217277526855, lr: 5.9999999999999995e-05
Step 2682, training loss: 4.348541736602783, total_norm: 3.12117600440979, lr: 5.9999999999999995e-05
Step 2683, training loss: 4.8084869384765625, total_norm: 3.013184070587158, lr: 5.9999999999999995e-05
Step 2684, training loss: 4.025238513946533, total_norm: 2.9602949619293213, lr: 5.9999999999999995e-05
Step 2685, training loss: 3.6102356910705566, total_norm: 2.8503260612487793, lr: 5.9999999999999995e-05
Step 2686, training loss: 3.7188591957092285, total_norm: 2.982286214828491, lr: 5.9999999999999995e-05
Step 2687, training loss: 3.6088271141052246, total_norm: 2.850238084793091, lr: 5.9999999999999995e-05
Step 2688, training loss: 4.278347969055176, total_norm: 2.833081007003784, lr: 5.9999999999999995e-05
Step 2689, training loss: 4.091334819793701, total_norm: 2.601168632507324, lr: 5.9999999999999995e-05
Step 2690, training loss: 3.659886121749878, total_norm: 2.813183307647705, lr: 5.9999999999999995e-05
Step 2691, training loss: 4.672871112823486, total_norm: 2.8215365409851074, lr: 5.9999999999999995e-05
Step 2692, training loss: 4.377760410308838, total_norm: 2.859700918197632, lr: 5.9999999999999995e-05
Step 2693, training loss: 3.9757752418518066, total_norm: 2.728593349456787, lr: 5.9999999999999995e-05
Step 2694, training loss: 3.873936653137207, total_norm: 2.7540993690490723, lr: 5.9999999999999995e-05
Step 2695, training loss: 3.957024574279785, total_norm: 2.658306360244751, lr: 5.9999999999999995e-05
Step 2696, training loss: 3.7453789710998535, total_norm: 2.648136615753174, lr: 5.9999999999999995e-05
Step 2697, training loss: 4.411484718322754, total_norm: 2.9234490394592285, lr: 5.9999999999999995e-05
Step 2698, training loss: 4.572870254516602, total_norm: 2.72721791267395, lr: 5.9999999999999995e-05
Step 2699, training loss: 3.5487778186798096, total_norm: 2.7510626316070557, lr: 5.9999999999999995e-05
Step 2700, training loss: 3.6508898735046387, total_norm: 2.714359998703003, lr: 5.9999999999999995e-05
Step 2700, validation loss: 5.421285152435303
Step 2701, training loss: 4.919086933135986, total_norm: 4.034360885620117, lr: 5.9999999999999995e-05
Step 2702, training loss: 4.519869804382324, total_norm: 3.176875352859497, lr: 5.9999999999999995e-05
Step 2703, training loss: 3.5987939834594727, total_norm: 3.5239715576171875, lr: 5.9999999999999995e-05
Step 2704, training loss: 3.73201584815979, total_norm: 3.319010019302368, lr: 5.9999999999999995e-05
Step 2705, training loss: 5.074362754821777, total_norm: 3.4080007076263428, lr: 5.9999999999999995e-05
Step 2706, training loss: 4.78538179397583, total_norm: 3.087763786315918, lr: 5.9999999999999995e-05
Step 2707, training loss: 3.89589524269104, total_norm: 3.4067583084106445, lr: 5.9999999999999995e-05
Step 2708, training loss: 4.676990032196045, total_norm: 3.4638381004333496, lr: 5.9999999999999995e-05
Step 2709, training loss: 4.0995564460754395, total_norm: 3.026062488555908, lr: 5.9999999999999995e-05
Step 2710, training loss: 3.998356819152832, total_norm: 3.551452875137329, lr: 5.9999999999999995e-05
Step 2711, training loss: 3.8612422943115234, total_norm: 3.306796073913574, lr: 5.9999999999999995e-05
Step 2712, training loss: 4.276419162750244, total_norm: 3.1041927337646484, lr: 5.9999999999999995e-05
Step 2713, training loss: 4.035052299499512, total_norm: 3.1423604488372803, lr: 5.9999999999999995e-05
Step 2714, training loss: 4.546133041381836, total_norm: 3.2064220905303955, lr: 5.9999999999999995e-05
Step 2715, training loss: 4.472667217254639, total_norm: 3.2640671730041504, lr: 5.9999999999999995e-05
Step 2716, training loss: 4.152002334594727, total_norm: 2.678107261657715, lr: 5.9999999999999995e-05
Step 2717, training loss: 3.6766703128814697, total_norm: 2.8492801189422607, lr: 5.9999999999999995e-05
Step 2718, training loss: 4.722817420959473, total_norm: 3.7521963119506836, lr: 5.9999999999999995e-05
Step 2719, training loss: 4.83554744720459, total_norm: 3.7179007530212402, lr: 5.9999999999999995e-05
Step 2720, training loss: 4.329117774963379, total_norm: 2.7692925930023193, lr: 5.9999999999999995e-05
Step 2721, training loss: 4.846015453338623, total_norm: 3.2566959857940674, lr: 5.9999999999999995e-05
Step 2722, training loss: 4.294783592224121, total_norm: 3.1017515659332275, lr: 5.9999999999999995e-05
Step 2723, training loss: 4.912005424499512, total_norm: 3.312495708465576, lr: 5.9999999999999995e-05
Step 2724, training loss: 5.022400856018066, total_norm: 3.2910873889923096, lr: 5.9999999999999995e-05
Step 2725, training loss: 4.092162132263184, total_norm: 3.059863805770874, lr: 5.9999999999999995e-05
Step 2726, training loss: 4.6649956703186035, total_norm: 3.3261547088623047, lr: 5.9999999999999995e-05
Step 2727, training loss: 4.579158306121826, total_norm: 3.092259168624878, lr: 5.9999999999999995e-05
Step 2728, training loss: 4.422405242919922, total_norm: 3.0480666160583496, lr: 5.9999999999999995e-05
Step 2729, training loss: 4.415136337280273, total_norm: 3.0321362018585205, lr: 5.9999999999999995e-05
Step 2730, training loss: 4.118780612945557, total_norm: 3.210139751434326, lr: 5.9999999999999995e-05
Step 2731, training loss: 4.39042329788208, total_norm: 3.1873040199279785, lr: 5.9999999999999995e-05
Step 2732, training loss: 5.49277925491333, total_norm: 4.012319087982178, lr: 5.9999999999999995e-05
Step 2733, training loss: 4.797199726104736, total_norm: 4.104038715362549, lr: 5.9999999999999995e-05
Step 2734, training loss: 4.649496078491211, total_norm: 4.833760738372803, lr: 5.9999999999999995e-05
Step 2735, training loss: 4.452502250671387, total_norm: 3.9604990482330322, lr: 5.9999999999999995e-05
Step 2736, training loss: 5.009617805480957, total_norm: 3.317600965499878, lr: 5.9999999999999995e-05
Step 2737, training loss: 4.8034162521362305, total_norm: 3.5405728816986084, lr: 5.9999999999999995e-05
Step 2738, training loss: 4.0391998291015625, total_norm: 3.7620856761932373, lr: 5.9999999999999995e-05
Step 2739, training loss: 3.7457706928253174, total_norm: 3.6676197052001953, lr: 5.9999999999999995e-05
Step 2740, training loss: 3.9442477226257324, total_norm: 3.6307482719421387, lr: 5.9999999999999995e-05
Step 2741, training loss: 3.7476541996002197, total_norm: 3.4193530082702637, lr: 5.9999999999999995e-05
Step 2742, training loss: 4.288887023925781, total_norm: 2.8739161491394043, lr: 5.9999999999999995e-05
Step 2743, training loss: 4.560192108154297, total_norm: 3.0287721157073975, lr: 5.9999999999999995e-05
Step 2744, training loss: 4.395796775817871, total_norm: 3.3785791397094727, lr: 5.9999999999999995e-05
Step 2745, training loss: 4.639242172241211, total_norm: 2.9454989433288574, lr: 5.9999999999999995e-05
Step 2746, training loss: 4.19145393371582, total_norm: 2.8241944313049316, lr: 5.9999999999999995e-05
Step 2747, training loss: 4.028651714324951, total_norm: 2.993507146835327, lr: 5.9999999999999995e-05
Step 2748, training loss: 4.333516597747803, total_norm: 2.8247716426849365, lr: 5.9999999999999995e-05
Step 2749, training loss: 4.3507399559021, total_norm: 3.2502286434173584, lr: 5.9999999999999995e-05
Step 2750, training loss: 4.002751350402832, total_norm: 3.071195363998413, lr: 5.9999999999999995e-05
Step 2750, validation loss: 5.493715286254883
Step 2751, training loss: 4.155941963195801, total_norm: 2.8255467414855957, lr: 5.9999999999999995e-05
Step 2752, training loss: 4.0908660888671875, total_norm: 2.7839250564575195, lr: 5.9999999999999995e-05
Step 2753, training loss: 4.435608863830566, total_norm: 3.3193247318267822, lr: 5.9999999999999995e-05
Step 2754, training loss: 5.022828578948975, total_norm: 3.103844165802002, lr: 5.9999999999999995e-05
Step 2755, training loss: 4.343249797821045, total_norm: 3.343066930770874, lr: 5.9999999999999995e-05
Step 2756, training loss: 4.4579997062683105, total_norm: 4.570708274841309, lr: 5.9999999999999995e-05
Step 2757, training loss: 3.9079039096832275, total_norm: 3.703472852706909, lr: 5.9999999999999995e-05
Step 2758, training loss: 4.369406700134277, total_norm: 2.8899078369140625, lr: 5.9999999999999995e-05
Step 2759, training loss: 3.8977513313293457, total_norm: 3.419131278991699, lr: 5.9999999999999995e-05
Step 2760, training loss: 4.2110595703125, total_norm: 3.4183900356292725, lr: 5.9999999999999995e-05
Step 2761, training loss: 4.263444900512695, total_norm: 4.285025119781494, lr: 5.9999999999999995e-05
Step 2762, training loss: 4.1800217628479, total_norm: 3.161669969558716, lr: 5.9999999999999995e-05
Step 2763, training loss: 4.557458400726318, total_norm: 3.271503210067749, lr: 5.9999999999999995e-05
Step 2764, training loss: 4.286971569061279, total_norm: 3.1618332862854004, lr: 5.9999999999999995e-05
Step 2765, training loss: 4.410855293273926, total_norm: 3.3544116020202637, lr: 5.9999999999999995e-05
Step 2766, training loss: 4.484016418457031, total_norm: 3.431767702102661, lr: 5.9999999999999995e-05
Step 2767, training loss: 4.436521053314209, total_norm: 3.1636717319488525, lr: 5.9999999999999995e-05
Step 2768, training loss: 4.068643569946289, total_norm: 3.2560460567474365, lr: 5.9999999999999995e-05
Step 2769, training loss: 4.407961845397949, total_norm: 3.1777443885803223, lr: 5.9999999999999995e-05
Step 2770, training loss: 3.890216827392578, total_norm: 3.366513967514038, lr: 5.9999999999999995e-05
Step 2771, training loss: 3.8786184787750244, total_norm: 3.3367221355438232, lr: 5.9999999999999995e-05
Step 2772, training loss: 4.05997896194458, total_norm: 3.14363956451416, lr: 5.9999999999999995e-05
Step 2773, training loss: 4.358739376068115, total_norm: 3.2315468788146973, lr: 5.9999999999999995e-05
Step 2774, training loss: 3.535480260848999, total_norm: 3.073411703109741, lr: 5.9999999999999995e-05
Step 2775, training loss: 3.4746310710906982, total_norm: 2.859290838241577, lr: 5.9999999999999995e-05
Step 2776, training loss: 4.081589221954346, total_norm: 2.926135778427124, lr: 5.9999999999999995e-05
Step 2777, training loss: 3.8375813961029053, total_norm: 2.7166810035705566, lr: 5.9999999999999995e-05
Step 2778, training loss: 4.295599937438965, total_norm: 2.9894373416900635, lr: 5.9999999999999995e-05
Step 2779, training loss: 4.124978542327881, total_norm: 3.256507396697998, lr: 5.9999999999999995e-05
Step 2780, training loss: 3.917555332183838, total_norm: 3.0906474590301514, lr: 5.9999999999999995e-05
Step 2781, training loss: 4.185911655426025, total_norm: 3.178790330886841, lr: 5.9999999999999995e-05
Step 2782, training loss: 3.7370779514312744, total_norm: 2.8707573413848877, lr: 5.9999999999999995e-05
Step 2783, training loss: 4.333231449127197, total_norm: 3.0991218090057373, lr: 5.9999999999999995e-05
Step 2784, training loss: 4.432880878448486, total_norm: 3.168764352798462, lr: 5.9999999999999995e-05
Step 2785, training loss: 4.213869571685791, total_norm: 2.802426815032959, lr: 5.9999999999999995e-05
Step 2786, training loss: 4.578466415405273, total_norm: 2.8693127632141113, lr: 5.9999999999999995e-05
Step 2787, training loss: 4.678265571594238, total_norm: 2.858631134033203, lr: 5.9999999999999995e-05
Step 2788, training loss: 4.551624774932861, total_norm: 2.9938433170318604, lr: 5.9999999999999995e-05
Step 2789, training loss: 4.1816487312316895, total_norm: 3.266420364379883, lr: 5.9999999999999995e-05
Step 2790, training loss: 4.4935479164123535, total_norm: 3.1160762310028076, lr: 5.9999999999999995e-05
Step 2791, training loss: 4.773280143737793, total_norm: 3.0839502811431885, lr: 5.9999999999999995e-05
Step 2792, training loss: 4.457522392272949, total_norm: 3.316295862197876, lr: 5.9999999999999995e-05
Step 2793, training loss: 3.9055557250976562, total_norm: 2.9621989727020264, lr: 5.9999999999999995e-05
Step 2794, training loss: 3.931969165802002, total_norm: 4.153357028961182, lr: 5.9999999999999995e-05
Step 2795, training loss: 4.513619422912598, total_norm: 3.2417263984680176, lr: 5.9999999999999995e-05
Step 2796, training loss: 3.915435314178467, total_norm: 3.17586612701416, lr: 5.9999999999999995e-05
Step 2797, training loss: 4.3119797706604, total_norm: 3.486828327178955, lr: 5.9999999999999995e-05
Step 2798, training loss: 4.0356292724609375, total_norm: 3.305238962173462, lr: 5.9999999999999995e-05
Step 2799, training loss: 3.5856523513793945, total_norm: 3.170567274093628, lr: 5.9999999999999995e-05
Step 2800, training loss: 4.802099704742432, total_norm: 3.0012714862823486, lr: 5.9999999999999995e-05
Step 2800, validation loss: 5.549194812774658
Step 2801, training loss: 4.613523483276367, total_norm: 3.0742833614349365, lr: 5.9999999999999995e-05
Step 2802, training loss: 4.392787933349609, total_norm: 3.498817205429077, lr: 5.9999999999999995e-05
Step 2803, training loss: 4.706509113311768, total_norm: 3.604992628097534, lr: 5.9999999999999995e-05
Step 2804, training loss: 4.542246341705322, total_norm: 3.084663152694702, lr: 5.9999999999999995e-05
Step 2805, training loss: 3.6981372833251953, total_norm: 3.3269617557525635, lr: 5.9999999999999995e-05
Step 2806, training loss: 4.0918989181518555, total_norm: 2.908236503601074, lr: 5.9999999999999995e-05
Step 2807, training loss: 3.921077013015747, total_norm: 2.690732479095459, lr: 5.9999999999999995e-05
Step 2808, training loss: 3.503690242767334, total_norm: 2.772836208343506, lr: 5.9999999999999995e-05
Step 2809, training loss: 3.803969621658325, total_norm: 2.7928249835968018, lr: 5.9999999999999995e-05
Step 2810, training loss: 4.907334804534912, total_norm: 3.11623215675354, lr: 5.9999999999999995e-05
Step 2811, training loss: 3.024657964706421, total_norm: 2.7957441806793213, lr: 5.9999999999999995e-05
Step 2812, training loss: 3.8703339099884033, total_norm: 2.9598405361175537, lr: 5.9999999999999995e-05
Step 2813, training loss: 4.010472297668457, total_norm: 2.8422279357910156, lr: 5.9999999999999995e-05
Step 2814, training loss: 3.829378128051758, total_norm: 3.257209539413452, lr: 5.9999999999999995e-05
Step 2815, training loss: 4.071206092834473, total_norm: 3.0396242141723633, lr: 5.9999999999999995e-05
Step 2816, training loss: 4.6596760749816895, total_norm: 3.1349639892578125, lr: 5.9999999999999995e-05
Step 2817, training loss: 4.8735833168029785, total_norm: 3.4970862865448, lr: 5.9999999999999995e-05
Step 2818, training loss: 4.638002395629883, total_norm: 3.7399544715881348, lr: 5.9999999999999995e-05
Step 2819, training loss: 4.614272117614746, total_norm: 3.4431071281433105, lr: 5.9999999999999995e-05
Step 2820, training loss: 3.9811031818389893, total_norm: 3.460019588470459, lr: 5.9999999999999995e-05
Step 2821, training loss: 4.876755237579346, total_norm: 3.952678918838501, lr: 5.9999999999999995e-05
Step 2822, training loss: 4.557627201080322, total_norm: 3.35219407081604, lr: 5.9999999999999995e-05
Step 2823, training loss: 4.689935684204102, total_norm: 3.3258519172668457, lr: 5.9999999999999995e-05
Step 2824, training loss: 4.1566081047058105, total_norm: 3.150693893432617, lr: 5.9999999999999995e-05
Step 2825, training loss: 4.3353400230407715, total_norm: 3.4531853199005127, lr: 5.9999999999999995e-05
Step 2826, training loss: 4.819670677185059, total_norm: 3.8453118801116943, lr: 5.9999999999999995e-05
Step 2827, training loss: 4.714343547821045, total_norm: 4.781885147094727, lr: 5.9999999999999995e-05
Step 2828, training loss: 4.839436054229736, total_norm: 3.557429552078247, lr: 5.9999999999999995e-05
Step 2829, training loss: 4.826651096343994, total_norm: 3.4305830001831055, lr: 5.9999999999999995e-05
Step 2830, training loss: 4.810550689697266, total_norm: 3.408412218093872, lr: 5.9999999999999995e-05
Step 2831, training loss: 4.857810020446777, total_norm: 3.945244073867798, lr: 5.9999999999999995e-05
Step 2832, training loss: 4.957746505737305, total_norm: 3.8078668117523193, lr: 5.9999999999999995e-05
Step 2833, training loss: 4.77556037902832, total_norm: 3.582716226577759, lr: 5.9999999999999995e-05
Step 2834, training loss: 4.5690741539001465, total_norm: 3.8205208778381348, lr: 5.9999999999999995e-05
Step 2835, training loss: 4.53155517578125, total_norm: 2.732391357421875, lr: 5.9999999999999995e-05
Step 2836, training loss: 4.763126850128174, total_norm: 3.2451839447021484, lr: 5.9999999999999995e-05
Step 2837, training loss: 5.0267181396484375, total_norm: 3.6732144355773926, lr: 5.9999999999999995e-05
Step 2838, training loss: 4.50186824798584, total_norm: 3.4553980827331543, lr: 5.9999999999999995e-05
Step 2839, training loss: 4.483242511749268, total_norm: 3.0289292335510254, lr: 5.9999999999999995e-05
Step 2840, training loss: 3.9787392616271973, total_norm: 3.3867993354797363, lr: 5.9999999999999995e-05
Step 2841, training loss: 4.4486284255981445, total_norm: 2.8806722164154053, lr: 5.9999999999999995e-05
Step 2842, training loss: 4.832353591918945, total_norm: 3.5102665424346924, lr: 5.9999999999999995e-05
Step 2843, training loss: 4.724992752075195, total_norm: 3.097313165664673, lr: 5.9999999999999995e-05
Step 2844, training loss: 5.260167121887207, total_norm: 3.417482614517212, lr: 5.9999999999999995e-05
Step 2845, training loss: 4.420832633972168, total_norm: 3.3897392749786377, lr: 5.9999999999999995e-05
Step 2846, training loss: 4.611676216125488, total_norm: 2.8076887130737305, lr: 5.9999999999999995e-05
Step 2847, training loss: 4.500423908233643, total_norm: 3.4953179359436035, lr: 5.9999999999999995e-05
Step 2848, training loss: 4.553067684173584, total_norm: 3.3361921310424805, lr: 5.9999999999999995e-05
Step 2849, training loss: 4.556729316711426, total_norm: 4.616729736328125, lr: 5.9999999999999995e-05
Step 2850, training loss: 4.844570159912109, total_norm: 3.6595890522003174, lr: 5.9999999999999995e-05
Step 2850, validation loss: 5.61170768737793
Step 2851, training loss: 4.819911479949951, total_norm: 3.227130651473999, lr: 5.9999999999999995e-05
Step 2852, training loss: 4.404086112976074, total_norm: 4.031045436859131, lr: 5.9999999999999995e-05
Step 2853, training loss: 4.472372055053711, total_norm: 3.1370136737823486, lr: 5.9999999999999995e-05
Step 2854, training loss: 4.450771808624268, total_norm: 3.6022794246673584, lr: 5.9999999999999995e-05
Step 2855, training loss: 4.140586853027344, total_norm: 3.8440165519714355, lr: 5.9999999999999995e-05
Step 2856, training loss: 4.0986857414245605, total_norm: 3.745832681655884, lr: 5.9999999999999995e-05
Step 2857, training loss: 4.245442867279053, total_norm: 3.2401013374328613, lr: 5.9999999999999995e-05
Step 2858, training loss: 4.490546703338623, total_norm: 3.6788387298583984, lr: 5.9999999999999995e-05
Step 2859, training loss: 4.665791988372803, total_norm: 3.336784839630127, lr: 5.9999999999999995e-05
Step 2860, training loss: 4.427879333496094, total_norm: 3.272840738296509, lr: 5.9999999999999995e-05
Step 2861, training loss: 4.706205368041992, total_norm: 3.3992645740509033, lr: 5.9999999999999995e-05
Step 2862, training loss: 4.839634895324707, total_norm: 3.595531463623047, lr: 5.9999999999999995e-05
Step 2863, training loss: 4.838890075683594, total_norm: 3.483696222305298, lr: 5.9999999999999995e-05
Step 2864, training loss: 4.745516300201416, total_norm: 3.4494123458862305, lr: 5.9999999999999995e-05
Step 2865, training loss: 4.940373420715332, total_norm: 3.2525885105133057, lr: 5.9999999999999995e-05
Step 2866, training loss: 4.124568939208984, total_norm: 3.148242950439453, lr: 5.9999999999999995e-05
Step 2867, training loss: 4.375602722167969, total_norm: 3.336599588394165, lr: 5.9999999999999995e-05
Step 2868, training loss: 4.972581386566162, total_norm: 3.316175699234009, lr: 5.9999999999999995e-05
Step 2869, training loss: 4.4844865798950195, total_norm: 3.0633175373077393, lr: 5.9999999999999995e-05
Step 2870, training loss: 4.727881908416748, total_norm: 3.194152593612671, lr: 5.9999999999999995e-05
Step 2871, training loss: 3.6606266498565674, total_norm: 2.6997807025909424, lr: 5.9999999999999995e-05
Step 2872, training loss: 4.6799445152282715, total_norm: 3.0822341442108154, lr: 5.9999999999999995e-05
Step 2873, training loss: 4.893664360046387, total_norm: 3.756342887878418, lr: 5.9999999999999995e-05
Step 2874, training loss: 4.401784420013428, total_norm: 3.2135558128356934, lr: 5.9999999999999995e-05
Step 2875, training loss: 3.9165492057800293, total_norm: 3.063495635986328, lr: 5.9999999999999995e-05
Step 2876, training loss: 4.383373260498047, total_norm: 3.6975831985473633, lr: 5.9999999999999995e-05
Step 2877, training loss: 4.457411766052246, total_norm: 3.373802423477173, lr: 5.9999999999999995e-05
Step 2878, training loss: 4.558391571044922, total_norm: 3.15224289894104, lr: 5.9999999999999995e-05
Step 2879, training loss: 4.288608551025391, total_norm: 3.354574680328369, lr: 5.9999999999999995e-05
Step 2880, training loss: 4.639754772186279, total_norm: 3.2629497051239014, lr: 5.9999999999999995e-05
Step 2881, training loss: 4.052276134490967, total_norm: 2.834061861038208, lr: 5.9999999999999995e-05
Step 2882, training loss: 3.650421380996704, total_norm: 2.890956401824951, lr: 5.9999999999999995e-05
Step 2883, training loss: 4.599916934967041, total_norm: 2.7803399562835693, lr: 5.9999999999999995e-05
Step 2884, training loss: 4.688173294067383, total_norm: 3.1682658195495605, lr: 5.9999999999999995e-05
Step 2885, training loss: 4.174032211303711, total_norm: 3.079585552215576, lr: 5.9999999999999995e-05
Step 2886, training loss: 4.678833961486816, total_norm: 3.685997247695923, lr: 5.9999999999999995e-05
Step 2887, training loss: 3.327648401260376, total_norm: 3.473658800125122, lr: 5.9999999999999995e-05
Step 2888, training loss: 3.2171566486358643, total_norm: 3.27280855178833, lr: 5.9999999999999995e-05
Step 2889, training loss: 3.88900089263916, total_norm: 2.839970588684082, lr: 5.9999999999999995e-05
Step 2890, training loss: 3.259908676147461, total_norm: 3.011445999145508, lr: 5.9999999999999995e-05
Step 2891, training loss: 4.369813919067383, total_norm: 3.9547033309936523, lr: 5.9999999999999995e-05
Step 2892, training loss: 4.058462142944336, total_norm: 3.577915668487549, lr: 5.9999999999999995e-05
Step 2893, training loss: 3.986323833465576, total_norm: 3.499326467514038, lr: 5.9999999999999995e-05
Step 2894, training loss: 4.944161891937256, total_norm: 4.420208930969238, lr: 5.9999999999999995e-05
Step 2895, training loss: 5.039698123931885, total_norm: 4.378531455993652, lr: 5.9999999999999995e-05
Step 2896, training loss: 4.366979122161865, total_norm: 3.7293717861175537, lr: 5.9999999999999995e-05
Step 2897, training loss: 4.451990127563477, total_norm: 3.291933059692383, lr: 5.9999999999999995e-05
Step 2898, training loss: 4.736876487731934, total_norm: 4.211080551147461, lr: 5.9999999999999995e-05
Step 2899, training loss: 4.329509258270264, total_norm: 4.91353178024292, lr: 5.9999999999999995e-05
Step 2900, training loss: 4.663675308227539, total_norm: 3.777329206466675, lr: 5.9999999999999995e-05
Step 2900, validation loss: 5.585862159729004
Step 2901, training loss: 5.0421462059021, total_norm: 4.024453639984131, lr: 5.9999999999999995e-05
Step 2902, training loss: 4.407862186431885, total_norm: 4.297928333282471, lr: 5.9999999999999995e-05
Step 2903, training loss: 4.476700305938721, total_norm: 4.407220363616943, lr: 5.9999999999999995e-05
Step 2904, training loss: 4.61573600769043, total_norm: 3.640120029449463, lr: 5.9999999999999995e-05
Step 2905, training loss: 5.269847869873047, total_norm: 3.846705436706543, lr: 5.9999999999999995e-05
Step 2906, training loss: 4.55236291885376, total_norm: 3.500494956970215, lr: 5.9999999999999995e-05
Step 2907, training loss: 4.3237810134887695, total_norm: 3.4720206260681152, lr: 5.9999999999999995e-05
Step 2908, training loss: 4.870359420776367, total_norm: 4.049138069152832, lr: 5.9999999999999995e-05
Step 2909, training loss: 4.362671852111816, total_norm: 3.4404964447021484, lr: 5.9999999999999995e-05
Step 2910, training loss: 4.406287670135498, total_norm: 3.5949816703796387, lr: 5.9999999999999995e-05
Step 2911, training loss: 4.20008659362793, total_norm: 3.585444927215576, lr: 5.9999999999999995e-05
Step 2912, training loss: 5.533441543579102, total_norm: 3.5072438716888428, lr: 5.9999999999999995e-05
Step 2913, training loss: 4.9058709144592285, total_norm: 3.6351423263549805, lr: 5.9999999999999995e-05
Step 2914, training loss: 4.520029067993164, total_norm: 3.5374414920806885, lr: 5.9999999999999995e-05
Step 2915, training loss: 4.305583953857422, total_norm: 3.4068593978881836, lr: 5.9999999999999995e-05
Step 2916, training loss: 3.9197936058044434, total_norm: 3.314971923828125, lr: 5.9999999999999995e-05
Step 2917, training loss: 4.1349263191223145, total_norm: 2.8195905685424805, lr: 5.9999999999999995e-05
Step 2918, training loss: 4.837947845458984, total_norm: 3.2444839477539062, lr: 5.9999999999999995e-05
Step 2919, training loss: 4.441854476928711, total_norm: 2.938070058822632, lr: 5.9999999999999995e-05
Step 2920, training loss: 4.124179840087891, total_norm: 2.7314417362213135, lr: 5.9999999999999995e-05
Step 2921, training loss: 4.300655841827393, total_norm: 2.9384799003601074, lr: 5.9999999999999995e-05
Step 2922, training loss: 3.8950276374816895, total_norm: 2.8908095359802246, lr: 5.9999999999999995e-05
Step 2923, training loss: 4.043698787689209, total_norm: 3.011356830596924, lr: 5.9999999999999995e-05
Step 2924, training loss: 5.466686725616455, total_norm: 4.400425910949707, lr: 5.9999999999999995e-05
Step 2925, training loss: 4.496147155761719, total_norm: 3.720160484313965, lr: 5.9999999999999995e-05
Step 2926, training loss: 3.9612460136413574, total_norm: 3.2361373901367188, lr: 5.9999999999999995e-05
Step 2927, training loss: 5.3065314292907715, total_norm: 4.393691539764404, lr: 5.9999999999999995e-05
Step 2928, training loss: 4.17501163482666, total_norm: 3.9887001514434814, lr: 5.9999999999999995e-05
Step 2929, training loss: 4.006363391876221, total_norm: 3.863905668258667, lr: 5.9999999999999995e-05
Step 2930, training loss: 4.41099739074707, total_norm: 3.6312270164489746, lr: 5.9999999999999995e-05
Step 2931, training loss: 4.175213813781738, total_norm: 3.1331300735473633, lr: 5.9999999999999995e-05
Step 2932, training loss: 4.746701717376709, total_norm: 3.3912532329559326, lr: 5.9999999999999995e-05
Step 2933, training loss: 4.170236110687256, total_norm: 2.8921926021575928, lr: 5.9999999999999995e-05
Step 2934, training loss: 4.618608474731445, total_norm: 3.6362926959991455, lr: 5.9999999999999995e-05
Step 2935, training loss: 4.306580543518066, total_norm: 3.6263275146484375, lr: 5.9999999999999995e-05
Step 2936, training loss: 4.805342197418213, total_norm: 4.638380527496338, lr: 5.9999999999999995e-05
Step 2937, training loss: 4.231499195098877, total_norm: 3.7370407581329346, lr: 5.9999999999999995e-05
Step 2938, training loss: 4.362639427185059, total_norm: 3.847872734069824, lr: 5.9999999999999995e-05
Step 2939, training loss: 3.995115041732788, total_norm: 3.310971260070801, lr: 5.9999999999999995e-05
Step 2940, training loss: 5.036346435546875, total_norm: 3.491994857788086, lr: 5.9999999999999995e-05
Step 2941, training loss: 4.833821773529053, total_norm: 4.413449287414551, lr: 5.9999999999999995e-05
Step 2942, training loss: 4.366161346435547, total_norm: 3.8531720638275146, lr: 5.9999999999999995e-05
Step 2943, training loss: 4.824854850769043, total_norm: 4.00243616104126, lr: 5.9999999999999995e-05
Step 2944, training loss: 4.461601257324219, total_norm: 3.8014421463012695, lr: 5.9999999999999995e-05
Step 2945, training loss: 4.518322944641113, total_norm: 3.3594493865966797, lr: 5.9999999999999995e-05
Step 2946, training loss: 4.294339179992676, total_norm: 3.145003080368042, lr: 5.9999999999999995e-05
Step 2947, training loss: 4.189208507537842, total_norm: 3.2372803688049316, lr: 5.9999999999999995e-05
Step 2948, training loss: 4.844649791717529, total_norm: 3.6723082065582275, lr: 5.9999999999999995e-05
Step 2949, training loss: 4.099517822265625, total_norm: 3.3116085529327393, lr: 5.9999999999999995e-05
Step 2950, training loss: 4.3717169761657715, total_norm: 3.267665147781372, lr: 5.9999999999999995e-05
Step 2950, validation loss: 5.627866268157959
Step 2951, training loss: 4.134493827819824, total_norm: 2.9405012130737305, lr: 5.9999999999999995e-05
Step 2952, training loss: 3.4867823123931885, total_norm: 3.0665645599365234, lr: 5.9999999999999995e-05
Step 2953, training loss: 3.991886615753174, total_norm: 2.685421943664551, lr: 5.9999999999999995e-05
Step 2954, training loss: 4.3586344718933105, total_norm: 3.0701394081115723, lr: 5.9999999999999995e-05
Step 2955, training loss: 4.354565143585205, total_norm: 2.982428550720215, lr: 5.9999999999999995e-05
Step 2956, training loss: 3.9500560760498047, total_norm: 2.660614013671875, lr: 5.9999999999999995e-05
Step 2957, training loss: 3.81099009513855, total_norm: 2.886216402053833, lr: 5.9999999999999995e-05
Step 2958, training loss: 3.870636463165283, total_norm: 2.664273500442505, lr: 5.9999999999999995e-05
Step 2959, training loss: 4.95867919921875, total_norm: 3.1965386867523193, lr: 5.9999999999999995e-05
Step 2960, training loss: 4.201988220214844, total_norm: 3.041243553161621, lr: 5.9999999999999995e-05
Step 2961, training loss: 3.936192274093628, total_norm: 3.003725290298462, lr: 5.9999999999999995e-05
Step 2962, training loss: 4.349063396453857, total_norm: 3.150869369506836, lr: 5.9999999999999995e-05
Step 2963, training loss: 4.257440090179443, total_norm: 3.090432643890381, lr: 5.9999999999999995e-05
Step 2964, training loss: 4.06089448928833, total_norm: 3.6541144847869873, lr: 5.9999999999999995e-05
Step 2965, training loss: 4.310615539550781, total_norm: 3.9286351203918457, lr: 5.9999999999999995e-05
Step 2966, training loss: 4.656844615936279, total_norm: 3.1244943141937256, lr: 5.9999999999999995e-05
Step 2967, training loss: 4.794404983520508, total_norm: 4.47791051864624, lr: 5.9999999999999995e-05
Step 2968, training loss: 4.5698065757751465, total_norm: 3.2874362468719482, lr: 5.9999999999999995e-05
Step 2969, training loss: 5.17862606048584, total_norm: 3.513554096221924, lr: 5.9999999999999995e-05
Step 2970, training loss: 4.3816447257995605, total_norm: 3.325026273727417, lr: 5.9999999999999995e-05
Step 2971, training loss: 4.828581809997559, total_norm: 2.8954474925994873, lr: 5.9999999999999995e-05
Step 2972, training loss: 4.317755699157715, total_norm: 3.3592541217803955, lr: 5.9999999999999995e-05
Step 2973, training loss: 4.944518566131592, total_norm: 3.1872663497924805, lr: 5.9999999999999995e-05
Step 2974, training loss: 3.9217231273651123, total_norm: 3.8125898838043213, lr: 5.9999999999999995e-05
Step 2975, training loss: 4.315816879272461, total_norm: 4.02297306060791, lr: 5.9999999999999995e-05
Step 2976, training loss: 4.175920009613037, total_norm: 3.2373740673065186, lr: 5.9999999999999995e-05
Step 2977, training loss: 4.698522090911865, total_norm: 3.5360212326049805, lr: 5.9999999999999995e-05
Step 2978, training loss: 4.266857147216797, total_norm: 2.9954309463500977, lr: 5.9999999999999995e-05
Step 2979, training loss: 4.536404609680176, total_norm: 3.500574827194214, lr: 5.9999999999999995e-05
Step 2980, training loss: 4.467661380767822, total_norm: 3.7866227626800537, lr: 5.9999999999999995e-05
Step 2981, training loss: 4.13820743560791, total_norm: 3.803870439529419, lr: 5.9999999999999995e-05
Step 2982, training loss: 4.101377010345459, total_norm: 3.366380214691162, lr: 5.9999999999999995e-05
Step 2983, training loss: 4.0129780769348145, total_norm: 3.402384042739868, lr: 5.9999999999999995e-05
Step 2984, training loss: 3.953740119934082, total_norm: 3.877772092819214, lr: 5.9999999999999995e-05
Step 2985, training loss: 4.105405330657959, total_norm: 3.124562978744507, lr: 5.9999999999999995e-05
Step 2986, training loss: 3.848196268081665, total_norm: 3.1944947242736816, lr: 5.9999999999999995e-05
Step 2987, training loss: 4.2598419189453125, total_norm: 3.198275089263916, lr: 5.9999999999999995e-05
Step 2988, training loss: 4.274235248565674, total_norm: 2.9950854778289795, lr: 5.9999999999999995e-05
Step 2989, training loss: 4.225327014923096, total_norm: 3.240454912185669, lr: 5.9999999999999995e-05
Step 2990, training loss: 4.9032440185546875, total_norm: 3.6052138805389404, lr: 5.9999999999999995e-05
Step 2991, training loss: 4.482224464416504, total_norm: 3.2033135890960693, lr: 5.9999999999999995e-05
Step 2992, training loss: 4.535392761230469, total_norm: 3.678057909011841, lr: 5.9999999999999995e-05
Step 2993, training loss: 4.905643939971924, total_norm: 3.049912214279175, lr: 5.9999999999999995e-05
Step 2994, training loss: 4.262596130371094, total_norm: 3.0275049209594727, lr: 5.9999999999999995e-05
Step 2995, training loss: 4.832683086395264, total_norm: 3.694150686264038, lr: 5.9999999999999995e-05
Step 2996, training loss: 4.714537620544434, total_norm: 3.6107177734375, lr: 5.9999999999999995e-05
Step 2997, training loss: 4.709738254547119, total_norm: 3.6238932609558105, lr: 5.9999999999999995e-05
Step 2998, training loss: 4.47238826751709, total_norm: 2.8823986053466797, lr: 5.9999999999999995e-05
Step 2999, training loss: 4.553817272186279, total_norm: 3.1344189643859863, lr: 5.9999999999999995e-05
Step 3000, training loss: 4.708567142486572, total_norm: 2.9901013374328613, lr: 5.9999999999999995e-05
Step 3000, validation loss: 5.730901718139648
Step 3001, training loss: 4.070944309234619, total_norm: 3.3225250244140625, lr: 5.9999999999999995e-05
Step 3002, training loss: 3.7681686878204346, total_norm: 3.7064990997314453, lr: 5.9999999999999995e-05
Step 3003, training loss: 3.9965546131134033, total_norm: 3.6122074127197266, lr: 5.9999999999999995e-05
Step 3004, training loss: 4.497114181518555, total_norm: 3.827639102935791, lr: 5.9999999999999995e-05
Step 3005, training loss: 4.489415645599365, total_norm: 3.6261661052703857, lr: 5.9999999999999995e-05
Step 3006, training loss: 4.526341915130615, total_norm: 3.416299343109131, lr: 5.9999999999999995e-05
Step 3007, training loss: 5.127099514007568, total_norm: 3.5214130878448486, lr: 5.9999999999999995e-05
Step 3008, training loss: 4.503660678863525, total_norm: 3.49059796333313, lr: 5.9999999999999995e-05
Step 3009, training loss: 4.164919853210449, total_norm: 3.558652877807617, lr: 5.9999999999999995e-05
Step 3010, training loss: 4.7540740966796875, total_norm: 3.2877261638641357, lr: 5.9999999999999995e-05
Step 3011, training loss: 4.2869720458984375, total_norm: 3.266105890274048, lr: 5.9999999999999995e-05
Step 3012, training loss: 4.485142230987549, total_norm: 3.4899890422821045, lr: 5.9999999999999995e-05
Step 3013, training loss: 4.463685512542725, total_norm: 3.18217134475708, lr: 5.9999999999999995e-05
Step 3014, training loss: 3.9544522762298584, total_norm: 3.3571341037750244, lr: 5.9999999999999995e-05
Step 3015, training loss: 3.8971617221832275, total_norm: 3.4490511417388916, lr: 5.9999999999999995e-05
Step 3016, training loss: 3.131455659866333, total_norm: 8.420804977416992, lr: 5.9999999999999995e-05
Step 3017, training loss: 3.5562961101531982, total_norm: 4.03264045715332, lr: 5.9999999999999995e-05
Step 3018, training loss: 3.681454658508301, total_norm: 3.8652970790863037, lr: 5.9999999999999995e-05
Step 3019, training loss: 4.875764846801758, total_norm: 3.465484619140625, lr: 5.9999999999999995e-05
Step 3020, training loss: 4.844851493835449, total_norm: 4.193392276763916, lr: 5.9999999999999995e-05
Step 3021, training loss: 4.307026386260986, total_norm: 4.038819313049316, lr: 5.9999999999999995e-05
Step 3022, training loss: 4.718760967254639, total_norm: 4.041362762451172, lr: 5.9999999999999995e-05
Step 3023, training loss: 4.332072734832764, total_norm: 3.990043878555298, lr: 5.9999999999999995e-05
Step 3024, training loss: 4.260589122772217, total_norm: 3.871286153793335, lr: 5.9999999999999995e-05
Step 3025, training loss: 4.5514912605285645, total_norm: 4.293813228607178, lr: 5.9999999999999995e-05
Step 3026, training loss: 4.046133518218994, total_norm: 3.4087791442871094, lr: 5.9999999999999995e-05
Step 3027, training loss: 3.9937071800231934, total_norm: 3.449646472930908, lr: 5.9999999999999995e-05
Step 3028, training loss: 4.287816524505615, total_norm: 3.5817134380340576, lr: 5.9999999999999995e-05
Step 3029, training loss: 4.131812572479248, total_norm: 2.9555163383483887, lr: 5.9999999999999995e-05
Step 3030, training loss: 4.412912368774414, total_norm: 3.1374402046203613, lr: 5.9999999999999995e-05
Step 3031, training loss: 4.299646377563477, total_norm: 3.8666372299194336, lr: 5.9999999999999995e-05
Step 3032, training loss: 4.150426387786865, total_norm: 4.625181674957275, lr: 5.9999999999999995e-05
Step 3033, training loss: 4.0097808837890625, total_norm: 3.1669094562530518, lr: 5.9999999999999995e-05
Step 3034, training loss: 4.379327774047852, total_norm: 3.5577549934387207, lr: 5.9999999999999995e-05
Step 3035, training loss: 4.167027950286865, total_norm: 3.662306547164917, lr: 5.9999999999999995e-05
Step 3036, training loss: 4.114588737487793, total_norm: 3.4088733196258545, lr: 5.9999999999999995e-05
Step 3037, training loss: 4.364835262298584, total_norm: 3.331859827041626, lr: 5.9999999999999995e-05
Step 3038, training loss: 4.067273139953613, total_norm: 3.4082212448120117, lr: 5.9999999999999995e-05
Step 3039, training loss: 4.037265777587891, total_norm: 3.491792917251587, lr: 5.9999999999999995e-05
Step 3040, training loss: 4.414549350738525, total_norm: 3.7504122257232666, lr: 5.9999999999999995e-05
Step 3041, training loss: 4.554168701171875, total_norm: 3.239027738571167, lr: 5.9999999999999995e-05
Step 3042, training loss: 3.8872828483581543, total_norm: 2.905258893966675, lr: 5.9999999999999995e-05
Step 3043, training loss: 3.897738218307495, total_norm: 2.8538646697998047, lr: 5.9999999999999995e-05
Step 3044, training loss: 4.06080436706543, total_norm: 2.9584317207336426, lr: 5.9999999999999995e-05
Step 3045, training loss: 4.370486736297607, total_norm: 3.144474744796753, lr: 5.9999999999999995e-05
Step 3046, training loss: 4.307450294494629, total_norm: 3.1132595539093018, lr: 5.9999999999999995e-05
Step 3047, training loss: 4.923007965087891, total_norm: 3.3900256156921387, lr: 5.9999999999999995e-05
Step 3048, training loss: 4.210339546203613, total_norm: 2.9969429969787598, lr: 5.9999999999999995e-05
Step 3049, training loss: 3.9382030963897705, total_norm: 2.8902063369750977, lr: 5.9999999999999995e-05
Step 3050, training loss: 3.521646022796631, total_norm: 2.9376771450042725, lr: 5.9999999999999995e-05
Step 3050, validation loss: 5.6267290115356445
Step 3051, training loss: 3.699355125427246, total_norm: 2.990239143371582, lr: 5.9999999999999995e-05
Step 3052, training loss: 4.183722972869873, total_norm: 3.159590721130371, lr: 5.9999999999999995e-05
Step 3053, training loss: 4.492977619171143, total_norm: 3.2564642429351807, lr: 5.9999999999999995e-05
Step 3054, training loss: 4.795661449432373, total_norm: 3.633828639984131, lr: 5.9999999999999995e-05
Step 3055, training loss: 4.553581237792969, total_norm: 3.9575135707855225, lr: 5.9999999999999995e-05
Step 3056, training loss: 5.387125015258789, total_norm: 4.308174133300781, lr: 5.9999999999999995e-05
Step 3057, training loss: 4.683003902435303, total_norm: 3.7114381790161133, lr: 5.9999999999999995e-05
Step 3058, training loss: 4.8945159912109375, total_norm: 4.567877292633057, lr: 5.9999999999999995e-05
Step 3059, training loss: 4.611900329589844, total_norm: 3.744657039642334, lr: 5.9999999999999995e-05
Step 3060, training loss: 4.92944860458374, total_norm: 3.987760543823242, lr: 5.9999999999999995e-05
Step 3061, training loss: 4.767907619476318, total_norm: 4.442997932434082, lr: 5.9999999999999995e-05
Step 3062, training loss: 4.634235858917236, total_norm: 3.998253107070923, lr: 5.9999999999999995e-05
Step 3063, training loss: 4.516400337219238, total_norm: 3.544137954711914, lr: 5.9999999999999995e-05
Step 3064, training loss: 4.917588233947754, total_norm: 3.447836399078369, lr: 5.9999999999999995e-05
Step 3065, training loss: 4.743001937866211, total_norm: 3.149869203567505, lr: 5.9999999999999995e-05
Step 3066, training loss: 4.276313304901123, total_norm: 3.0319201946258545, lr: 5.9999999999999995e-05
Step 3067, training loss: 4.245607376098633, total_norm: 3.256601572036743, lr: 5.9999999999999995e-05
Step 3068, training loss: 4.704324245452881, total_norm: 3.296503782272339, lr: 5.9999999999999995e-05
Step 3069, training loss: 4.022854328155518, total_norm: 3.5242702960968018, lr: 5.9999999999999995e-05
Step 3070, training loss: 4.319244384765625, total_norm: 3.4262518882751465, lr: 5.9999999999999995e-05
Step 3071, training loss: 4.702107906341553, total_norm: 3.5810279846191406, lr: 5.9999999999999995e-05
Step 3072, training loss: 4.344537734985352, total_norm: 3.107252359390259, lr: 5.9999999999999995e-05
Step 3073, training loss: 4.529494285583496, total_norm: 3.2179834842681885, lr: 5.9999999999999995e-05
Step 3074, training loss: 4.402491092681885, total_norm: 3.435711145401001, lr: 5.9999999999999995e-05
Step 3075, training loss: 4.686033725738525, total_norm: 3.69730544090271, lr: 5.9999999999999995e-05
Step 3076, training loss: 4.3390092849731445, total_norm: 3.200808048248291, lr: 5.9999999999999995e-05
Step 3077, training loss: 4.015945911407471, total_norm: 3.3301329612731934, lr: 5.9999999999999995e-05
Step 3078, training loss: 4.0831170082092285, total_norm: 3.1066436767578125, lr: 5.9999999999999995e-05
Step 3079, training loss: 4.403382778167725, total_norm: 3.3676304817199707, lr: 5.9999999999999995e-05
Step 3080, training loss: 4.2706146240234375, total_norm: 3.2221109867095947, lr: 5.9999999999999995e-05
Step 3081, training loss: 4.6496381759643555, total_norm: 3.324648380279541, lr: 5.9999999999999995e-05
Step 3082, training loss: 4.855833053588867, total_norm: 3.9046316146850586, lr: 5.9999999999999995e-05
Step 3083, training loss: 4.605492115020752, total_norm: 3.13611102104187, lr: 5.9999999999999995e-05
Step 3084, training loss: 4.695435047149658, total_norm: 3.088350534439087, lr: 5.9999999999999995e-05
Step 3085, training loss: 4.084137916564941, total_norm: 3.3017783164978027, lr: 5.9999999999999995e-05
Step 3086, training loss: 4.993682384490967, total_norm: 3.482053518295288, lr: 5.9999999999999995e-05
Step 3087, training loss: 4.576969623565674, total_norm: 3.061594009399414, lr: 5.9999999999999995e-05
Step 3088, training loss: 4.362493991851807, total_norm: 3.0644028186798096, lr: 5.9999999999999995e-05
Step 3089, training loss: 5.285120010375977, total_norm: 3.066741704940796, lr: 5.9999999999999995e-05
Step 3090, training loss: 5.47218132019043, total_norm: 4.72821044921875, lr: 5.9999999999999995e-05
Step 3091, training loss: 4.832790851593018, total_norm: 4.60599946975708, lr: 5.9999999999999995e-05
Step 3092, training loss: 4.976802825927734, total_norm: 3.178306818008423, lr: 5.9999999999999995e-05
Step 3093, training loss: 5.041064262390137, total_norm: 3.543930768966675, lr: 5.9999999999999995e-05
Step 3094, training loss: 5.298031806945801, total_norm: 3.9766335487365723, lr: 5.9999999999999995e-05
Step 3095, training loss: 4.875834941864014, total_norm: 4.116710662841797, lr: 5.9999999999999995e-05
Step 3096, training loss: 4.379086017608643, total_norm: 3.886166572570801, lr: 5.9999999999999995e-05
Step 3097, training loss: 4.506985664367676, total_norm: 3.175715923309326, lr: 5.9999999999999995e-05
Step 3098, training loss: 4.860055446624756, total_norm: 3.579171895980835, lr: 5.9999999999999995e-05
Step 3099, training loss: 4.827339172363281, total_norm: 3.921992063522339, lr: 5.9999999999999995e-05
Step 3100, training loss: 5.014775276184082, total_norm: 4.163418292999268, lr: 5.9999999999999995e-05
Step 3100, validation loss: 5.46950626373291
Step 3101, training loss: 4.392416954040527, total_norm: 3.8187296390533447, lr: 5.9999999999999995e-05
Step 3102, training loss: 5.124289512634277, total_norm: 3.5060112476348877, lr: 5.9999999999999995e-05
Step 3103, training loss: 4.9895243644714355, total_norm: 3.425189971923828, lr: 5.9999999999999995e-05
Step 3104, training loss: 4.543508052825928, total_norm: 3.862609386444092, lr: 5.9999999999999995e-05
Step 3105, training loss: 4.180084228515625, total_norm: 3.3682589530944824, lr: 5.9999999999999995e-05
Step 3106, training loss: 4.844547271728516, total_norm: 3.518615484237671, lr: 5.9999999999999995e-05
Step 3107, training loss: 4.024929046630859, total_norm: 2.925903558731079, lr: 5.9999999999999995e-05
Step 3108, training loss: 4.056577205657959, total_norm: 3.177272081375122, lr: 5.9999999999999995e-05
Step 3109, training loss: 4.73322057723999, total_norm: 3.259390115737915, lr: 5.9999999999999995e-05
Step 3110, training loss: 4.301075458526611, total_norm: 2.885128974914551, lr: 5.9999999999999995e-05
Step 3111, training loss: 4.302669525146484, total_norm: 3.389622211456299, lr: 5.9999999999999995e-05
Step 3112, training loss: 4.412883281707764, total_norm: 2.9983150959014893, lr: 5.9999999999999995e-05
Step 3113, training loss: 4.977716445922852, total_norm: 3.4642748832702637, lr: 5.9999999999999995e-05
Step 3114, training loss: 4.014171600341797, total_norm: 3.302621841430664, lr: 5.9999999999999995e-05
Step 3115, training loss: 4.224597454071045, total_norm: 2.954559087753296, lr: 5.9999999999999995e-05
Step 3116, training loss: 4.202646732330322, total_norm: 3.2514352798461914, lr: 5.9999999999999995e-05
Step 3117, training loss: 4.225921630859375, total_norm: 2.9507639408111572, lr: 5.9999999999999995e-05
Step 3118, training loss: 5.1051344871521, total_norm: 3.504718780517578, lr: 5.9999999999999995e-05
Step 3119, training loss: 4.221480369567871, total_norm: 2.975224256515503, lr: 5.9999999999999995e-05
Step 3120, training loss: 4.666512489318848, total_norm: 3.5374882221221924, lr: 5.9999999999999995e-05
Step 3121, training loss: 4.390265941619873, total_norm: 4.452955722808838, lr: 5.9999999999999995e-05
Step 3122, training loss: 4.494734764099121, total_norm: 3.585545301437378, lr: 5.9999999999999995e-05
Step 3123, training loss: 4.772470474243164, total_norm: 3.4384214878082275, lr: 5.9999999999999995e-05
Step 3124, training loss: 4.690882205963135, total_norm: 3.5112557411193848, lr: 5.9999999999999995e-05
Step 3125, training loss: 4.1335272789001465, total_norm: 2.8662924766540527, lr: 5.9999999999999995e-05
Step 3126, training loss: 4.9641594886779785, total_norm: 3.5270369052886963, lr: 5.9999999999999995e-05
Step 3127, training loss: 5.278851509094238, total_norm: 4.423702239990234, lr: 5.9999999999999995e-05
Step 3128, training loss: 5.232872486114502, total_norm: 3.947716474533081, lr: 5.9999999999999995e-05
Step 3129, training loss: 4.423148155212402, total_norm: 4.130033016204834, lr: 5.9999999999999995e-05
Step 3130, training loss: 4.005488395690918, total_norm: 3.7443721294403076, lr: 5.9999999999999995e-05
Step 3131, training loss: 4.55371618270874, total_norm: 3.647181272506714, lr: 5.9999999999999995e-05
Step 3132, training loss: 4.142177104949951, total_norm: 3.2321324348449707, lr: 5.9999999999999995e-05
Step 3133, training loss: 4.342988967895508, total_norm: 3.354928493499756, lr: 5.9999999999999995e-05
Step 3134, training loss: 4.7980499267578125, total_norm: 4.457050800323486, lr: 5.9999999999999995e-05
Step 3135, training loss: 4.982851028442383, total_norm: 4.868313312530518, lr: 5.9999999999999995e-05
Step 3136, training loss: 4.487936496734619, total_norm: 4.577507495880127, lr: 5.9999999999999995e-05
Step 3137, training loss: 4.4353718757629395, total_norm: 6.810482978820801, lr: 5.9999999999999995e-05
Step 3138, training loss: 4.387500286102295, total_norm: 7.645045757293701, lr: 5.9999999999999995e-05
Step 3139, training loss: 4.579523086547852, total_norm: 6.033055305480957, lr: 5.9999999999999995e-05
Step 3140, training loss: 4.734043121337891, total_norm: 3.932243824005127, lr: 5.9999999999999995e-05
Step 3141, training loss: 4.860438346862793, total_norm: 4.7517852783203125, lr: 5.9999999999999995e-05
Step 3142, training loss: 5.122534275054932, total_norm: 4.9966301918029785, lr: 5.9999999999999995e-05
Step 3143, training loss: 4.402117729187012, total_norm: 5.112231254577637, lr: 5.9999999999999995e-05
Step 3144, training loss: 4.7163190841674805, total_norm: 4.0323286056518555, lr: 5.9999999999999995e-05
Step 3145, training loss: 4.316020488739014, total_norm: 3.0868046283721924, lr: 5.9999999999999995e-05
Step 3146, training loss: 4.370912551879883, total_norm: 4.177361965179443, lr: 5.9999999999999995e-05
Step 3147, training loss: 4.15342378616333, total_norm: 4.733184337615967, lr: 5.9999999999999995e-05
Step 3148, training loss: 4.401875019073486, total_norm: 4.697961807250977, lr: 5.9999999999999995e-05
Step 3149, training loss: 3.6608963012695312, total_norm: 3.698763608932495, lr: 5.9999999999999995e-05
Step 3150, training loss: 4.284742832183838, total_norm: 4.317195892333984, lr: 5.9999999999999995e-05
Step 3150, validation loss: 5.18799352645874
Step 3151, training loss: 4.167024612426758, total_norm: 4.241852283477783, lr: 5.9999999999999995e-05
Step 3152, training loss: 4.153830051422119, total_norm: 3.417266607284546, lr: 5.9999999999999995e-05
Step 3153, training loss: 4.012070178985596, total_norm: 3.482208728790283, lr: 5.9999999999999995e-05
Step 3154, training loss: 3.6027910709381104, total_norm: 3.3998172283172607, lr: 5.9999999999999995e-05
Step 3155, training loss: 4.181222915649414, total_norm: 3.555565595626831, lr: 5.9999999999999995e-05
Step 3156, training loss: 4.445882320404053, total_norm: 4.396701812744141, lr: 5.9999999999999995e-05
Step 3157, training loss: 4.313483715057373, total_norm: 3.5774190425872803, lr: 5.9999999999999995e-05
Step 3158, training loss: 3.9725148677825928, total_norm: 4.1732587814331055, lr: 5.9999999999999995e-05
Step 3159, training loss: 4.556233882904053, total_norm: 3.4384992122650146, lr: 5.9999999999999995e-05
Step 3160, training loss: 4.06121301651001, total_norm: 2.775498390197754, lr: 5.9999999999999995e-05
Step 3161, training loss: 4.005552768707275, total_norm: 2.8895344734191895, lr: 5.9999999999999995e-05
Step 3162, training loss: 4.072508811950684, total_norm: 3.0436108112335205, lr: 5.9999999999999995e-05
Step 3163, training loss: 4.4477081298828125, total_norm: 2.928619146347046, lr: 5.9999999999999995e-05
Step 3164, training loss: 4.826014518737793, total_norm: 4.2838029861450195, lr: 5.9999999999999995e-05
Step 3165, training loss: 3.952024459838867, total_norm: 4.429630756378174, lr: 5.9999999999999995e-05
Step 3166, training loss: 4.258444786071777, total_norm: 4.116741180419922, lr: 5.9999999999999995e-05
Step 3167, training loss: 4.337823867797852, total_norm: 3.45938777923584, lr: 5.9999999999999995e-05
Step 3168, training loss: 4.766445636749268, total_norm: 4.31242036819458, lr: 5.9999999999999995e-05
Step 3169, training loss: 4.903366565704346, total_norm: 3.480656862258911, lr: 5.9999999999999995e-05
Step 3170, training loss: 4.526766300201416, total_norm: 3.7401411533355713, lr: 5.9999999999999995e-05
Step 3171, training loss: 4.451772212982178, total_norm: 3.257408857345581, lr: 5.9999999999999995e-05
Step 3172, training loss: 4.945602893829346, total_norm: 3.4567453861236572, lr: 5.9999999999999995e-05
Step 3173, training loss: 4.455881595611572, total_norm: 3.744917631149292, lr: 5.9999999999999995e-05
Step 3174, training loss: 4.148295879364014, total_norm: 4.7932233810424805, lr: 5.9999999999999995e-05
Step 3175, training loss: 4.544780731201172, total_norm: 3.757765293121338, lr: 5.9999999999999995e-05
Step 3176, training loss: 4.790999412536621, total_norm: 3.7365620136260986, lr: 5.9999999999999995e-05
Step 3177, training loss: 4.498405456542969, total_norm: 5.135998725891113, lr: 5.9999999999999995e-05
Step 3178, training loss: 4.308719635009766, total_norm: 4.426872253417969, lr: 5.9999999999999995e-05
Step 3179, training loss: 3.942502021789551, total_norm: 3.603670120239258, lr: 5.9999999999999995e-05
Step 3180, training loss: 4.642037868499756, total_norm: 3.2782557010650635, lr: 5.9999999999999995e-05
Step 3181, training loss: 4.285676956176758, total_norm: 3.4159483909606934, lr: 5.9999999999999995e-05
Step 3182, training loss: 4.330766201019287, total_norm: 3.173112392425537, lr: 5.9999999999999995e-05
Step 3183, training loss: 3.932769775390625, total_norm: 3.505157709121704, lr: 5.9999999999999995e-05
Step 3184, training loss: 4.455958366394043, total_norm: 2.968536138534546, lr: 5.9999999999999995e-05
Step 3185, training loss: 4.321728706359863, total_norm: 3.2621681690216064, lr: 5.9999999999999995e-05
Step 3186, training loss: 4.578817367553711, total_norm: 3.270679473876953, lr: 5.9999999999999995e-05
Step 3187, training loss: 3.8579041957855225, total_norm: 3.4022789001464844, lr: 5.9999999999999995e-05
Step 3188, training loss: 4.148484706878662, total_norm: 2.9721856117248535, lr: 5.9999999999999995e-05
Step 3189, training loss: 3.9814071655273438, total_norm: 3.408271074295044, lr: 5.9999999999999995e-05
Step 3190, training loss: 5.250267028808594, total_norm: 3.9174931049346924, lr: 5.9999999999999995e-05
Step 3191, training loss: 4.525599956512451, total_norm: 3.867088556289673, lr: 5.9999999999999995e-05
Step 3192, training loss: 4.060324192047119, total_norm: 3.5658445358276367, lr: 5.9999999999999995e-05
Step 3193, training loss: 4.081454277038574, total_norm: 3.6140928268432617, lr: 5.9999999999999995e-05
Step 3194, training loss: 4.662036895751953, total_norm: 3.832735300064087, lr: 5.9999999999999995e-05
Step 3195, training loss: 4.301609992980957, total_norm: 3.1755423545837402, lr: 5.9999999999999995e-05
Step 3196, training loss: 4.71738338470459, total_norm: 3.167070150375366, lr: 5.9999999999999995e-05
Step 3197, training loss: 4.186954498291016, total_norm: 3.4065282344818115, lr: 5.9999999999999995e-05
Step 3198, training loss: 4.843508243560791, total_norm: 3.688981056213379, lr: 5.9999999999999995e-05
Step 3199, training loss: 3.983187198638916, total_norm: 3.3196816444396973, lr: 5.9999999999999995e-05
Step 3200, training loss: 4.505026340484619, total_norm: 4.144286632537842, lr: 5.9999999999999995e-05
Step 3200, validation loss: 5.430325984954834
Step 3201, training loss: 4.223435878753662, total_norm: 3.349069595336914, lr: 5.9999999999999995e-05
Step 3202, training loss: 3.614445686340332, total_norm: 3.452634811401367, lr: 5.9999999999999995e-05
Step 3203, training loss: 4.604030132293701, total_norm: 3.4050936698913574, lr: 5.9999999999999995e-05
Step 3204, training loss: 3.403362989425659, total_norm: 2.6851863861083984, lr: 5.9999999999999995e-05
Step 3205, training loss: 4.4780049324035645, total_norm: 3.443351984024048, lr: 5.9999999999999995e-05
Step 3206, training loss: 4.7641425132751465, total_norm: 3.4874916076660156, lr: 5.9999999999999995e-05
Step 3207, training loss: 3.7038705348968506, total_norm: 3.6461730003356934, lr: 5.9999999999999995e-05
Step 3208, training loss: 3.297912836074829, total_norm: 3.1518237590789795, lr: 5.9999999999999995e-05
Step 3209, training loss: 3.8427844047546387, total_norm: 3.1932425498962402, lr: 5.9999999999999995e-05
Step 3210, training loss: 4.218684673309326, total_norm: 3.2414231300354004, lr: 5.9999999999999995e-05
Step 3211, training loss: 4.672239780426025, total_norm: 3.2157604694366455, lr: 5.9999999999999995e-05
Step 3212, training loss: 3.897627830505371, total_norm: 3.2737441062927246, lr: 5.9999999999999995e-05
Step 3213, training loss: 3.45491623878479, total_norm: 3.4473400115966797, lr: 5.9999999999999995e-05
Step 3214, training loss: 3.603344440460205, total_norm: 3.207944869995117, lr: 5.9999999999999995e-05
Step 3215, training loss: 3.4874155521392822, total_norm: 3.152467727661133, lr: 5.9999999999999995e-05
Step 3216, training loss: 4.170561790466309, total_norm: 3.320791482925415, lr: 5.9999999999999995e-05
Step 3217, training loss: 3.9882798194885254, total_norm: 2.972212791442871, lr: 5.9999999999999995e-05
Step 3218, training loss: 3.549257516860962, total_norm: 2.755411148071289, lr: 5.9999999999999995e-05
Step 3219, training loss: 4.521340370178223, total_norm: 2.9763498306274414, lr: 5.9999999999999995e-05
Step 3220, training loss: 4.228821277618408, total_norm: 3.0296945571899414, lr: 5.9999999999999995e-05
Step 3221, training loss: 3.819073438644409, total_norm: 3.1176626682281494, lr: 5.9999999999999995e-05
Step 3222, training loss: 3.7365882396698, total_norm: 3.1528069972991943, lr: 5.9999999999999995e-05
Step 3223, training loss: 3.8508758544921875, total_norm: 2.9656403064727783, lr: 5.9999999999999995e-05
Step 3224, training loss: 3.635345935821533, total_norm: 3.081984281539917, lr: 5.9999999999999995e-05
Step 3225, training loss: 4.29307222366333, total_norm: 3.5220069885253906, lr: 5.9999999999999995e-05
Step 3226, training loss: 4.431770324707031, total_norm: 3.212811231613159, lr: 5.9999999999999995e-05
Step 3227, training loss: 3.418450117111206, total_norm: 2.965956449508667, lr: 5.9999999999999995e-05
Step 3228, training loss: 3.5287535190582275, total_norm: 3.08528208732605, lr: 5.9999999999999995e-05
Step 3229, training loss: 4.78864049911499, total_norm: 5.1593804359436035, lr: 5.9999999999999995e-05
Step 3230, training loss: 4.3895158767700195, total_norm: 3.4256725311279297, lr: 5.9999999999999995e-05
Step 3231, training loss: 3.437110185623169, total_norm: 3.544922351837158, lr: 5.9999999999999995e-05
Step 3232, training loss: 3.5563502311706543, total_norm: 3.3297078609466553, lr: 5.9999999999999995e-05
Step 3233, training loss: 4.946808815002441, total_norm: 3.61434268951416, lr: 5.9999999999999995e-05
Step 3234, training loss: 4.6159539222717285, total_norm: 3.0191566944122314, lr: 5.9999999999999995e-05
Step 3235, training loss: 3.7334911823272705, total_norm: 3.6334025859832764, lr: 5.9999999999999995e-05
Step 3236, training loss: 4.529959678649902, total_norm: 3.982354164123535, lr: 5.9999999999999995e-05
Step 3237, training loss: 3.980212450027466, total_norm: 3.2930409908294678, lr: 5.9999999999999995e-05
Step 3238, training loss: 3.8606173992156982, total_norm: 3.676748752593994, lr: 5.9999999999999995e-05
Step 3239, training loss: 3.712498426437378, total_norm: 3.1432065963745117, lr: 5.9999999999999995e-05
Step 3240, training loss: 4.149510383605957, total_norm: 3.1213274002075195, lr: 5.9999999999999995e-05
Step 3241, training loss: 3.9167838096618652, total_norm: 3.2624881267547607, lr: 5.9999999999999995e-05
Step 3242, training loss: 4.410048484802246, total_norm: 3.41264009475708, lr: 5.9999999999999995e-05
Step 3243, training loss: 4.324627876281738, total_norm: 3.344440460205078, lr: 5.9999999999999995e-05
Step 3244, training loss: 4.017246723175049, total_norm: 2.949180841445923, lr: 5.9999999999999995e-05
Step 3245, training loss: 3.553154945373535, total_norm: 3.0542685985565186, lr: 5.9999999999999995e-05
Step 3246, training loss: 4.615354061126709, total_norm: 3.9065017700195312, lr: 5.9999999999999995e-05
Step 3247, training loss: 4.700177192687988, total_norm: 3.8385965824127197, lr: 5.9999999999999995e-05
Step 3248, training loss: 4.2022318840026855, total_norm: 2.8613991737365723, lr: 5.9999999999999995e-05
Step 3249, training loss: 4.726458549499512, total_norm: 3.5298984050750732, lr: 5.9999999999999995e-05
Step 3250, training loss: 4.166959285736084, total_norm: 3.328843116760254, lr: 5.9999999999999995e-05
Step 3250, validation loss: 5.3638153076171875
Step 3251, training loss: 4.7632951736450195, total_norm: 3.6036930084228516, lr: 5.9999999999999995e-05
Step 3252, training loss: 4.874680995941162, total_norm: 3.436008930206299, lr: 5.9999999999999995e-05
Step 3253, training loss: 3.953782081604004, total_norm: 3.314954996109009, lr: 5.9999999999999995e-05
Step 3254, training loss: 4.53364896774292, total_norm: 3.499573230743408, lr: 5.9999999999999995e-05
Step 3255, training loss: 4.42569637298584, total_norm: 3.2843563556671143, lr: 5.9999999999999995e-05
Step 3256, training loss: 4.281599044799805, total_norm: 3.479308843612671, lr: 5.9999999999999995e-05
Step 3257, training loss: 4.266693592071533, total_norm: 3.250068426132202, lr: 5.9999999999999995e-05
Step 3258, training loss: 3.9904487133026123, total_norm: 3.4967217445373535, lr: 5.9999999999999995e-05
Step 3259, training loss: 4.2482008934021, total_norm: 3.6129605770111084, lr: 5.9999999999999995e-05
Step 3260, training loss: 5.358761787414551, total_norm: 4.483868598937988, lr: 5.9999999999999995e-05
Step 3261, training loss: 4.616370677947998, total_norm: 4.628154754638672, lr: 5.9999999999999995e-05
Step 3262, training loss: 4.488390922546387, total_norm: 6.294935703277588, lr: 5.9999999999999995e-05
Step 3263, training loss: 4.275750637054443, total_norm: 4.518814563751221, lr: 5.9999999999999995e-05
Step 3264, training loss: 4.85535192489624, total_norm: 3.884355068206787, lr: 5.9999999999999995e-05
Step 3265, training loss: 4.6049275398254395, total_norm: 3.6300837993621826, lr: 5.9999999999999995e-05
Step 3266, training loss: 3.8170878887176514, total_norm: 3.345768690109253, lr: 5.9999999999999995e-05
Step 3267, training loss: 3.577915906906128, total_norm: 3.9446141719818115, lr: 5.9999999999999995e-05
Step 3268, training loss: 3.806453227996826, total_norm: 4.0762739181518555, lr: 5.9999999999999995e-05
Step 3269, training loss: 3.609759569168091, total_norm: 3.668004035949707, lr: 5.9999999999999995e-05
Step 3270, training loss: 4.156388759613037, total_norm: 3.215912342071533, lr: 5.9999999999999995e-05
Step 3271, training loss: 4.42671537399292, total_norm: 3.491366147994995, lr: 5.9999999999999995e-05
Step 3272, training loss: 4.242122173309326, total_norm: 3.684614658355713, lr: 5.9999999999999995e-05
Step 3273, training loss: 4.519299507141113, total_norm: 3.2112574577331543, lr: 5.9999999999999995e-05
Step 3274, training loss: 4.030632019042969, total_norm: 2.959235668182373, lr: 5.9999999999999995e-05
Step 3275, training loss: 3.9044246673583984, total_norm: 3.020045280456543, lr: 5.9999999999999995e-05
Step 3276, training loss: 4.1913909912109375, total_norm: 3.0362112522125244, lr: 5.9999999999999995e-05
Step 3277, training loss: 4.208722114562988, total_norm: 3.4251842498779297, lr: 5.9999999999999995e-05
Step 3278, training loss: 3.8499534130096436, total_norm: 3.2031826972961426, lr: 5.9999999999999995e-05
Step 3279, training loss: 4.021198272705078, total_norm: 3.0017035007476807, lr: 5.9999999999999995e-05
Step 3280, training loss: 3.959176778793335, total_norm: 3.0640885829925537, lr: 5.9999999999999995e-05
Step 3281, training loss: 4.271752834320068, total_norm: 3.5377180576324463, lr: 5.9999999999999995e-05
Step 3282, training loss: 4.885311126708984, total_norm: 3.5728061199188232, lr: 5.9999999999999995e-05
Step 3283, training loss: 4.157180309295654, total_norm: 3.22165846824646, lr: 5.9999999999999995e-05
Step 3284, training loss: 4.2475266456604, total_norm: 4.299213409423828, lr: 5.9999999999999995e-05
Step 3285, training loss: 3.7335760593414307, total_norm: 3.833141326904297, lr: 5.9999999999999995e-05
Step 3286, training loss: 4.228707790374756, total_norm: 3.2134625911712646, lr: 5.9999999999999995e-05
Step 3287, training loss: 3.7602245807647705, total_norm: 3.369659900665283, lr: 5.9999999999999995e-05
Step 3288, training loss: 4.0535888671875, total_norm: 2.9358882904052734, lr: 5.9999999999999995e-05
Step 3289, training loss: 4.123154640197754, total_norm: 3.6823301315307617, lr: 5.9999999999999995e-05
Step 3290, training loss: 4.039367198944092, total_norm: 3.1442337036132812, lr: 5.9999999999999995e-05
Step 3291, training loss: 4.433082103729248, total_norm: 3.525540828704834, lr: 5.9999999999999995e-05
Step 3292, training loss: 4.134570598602295, total_norm: 3.4330270290374756, lr: 5.9999999999999995e-05
Step 3293, training loss: 4.247129917144775, total_norm: 3.1247527599334717, lr: 5.9999999999999995e-05
Step 3294, training loss: 4.353029727935791, total_norm: 3.4308764934539795, lr: 5.9999999999999995e-05
Step 3295, training loss: 4.314581394195557, total_norm: 3.470696449279785, lr: 5.9999999999999995e-05
Step 3296, training loss: 3.9455201625823975, total_norm: 3.627603530883789, lr: 5.9999999999999995e-05
Step 3297, training loss: 4.254584789276123, total_norm: 3.4820523262023926, lr: 5.9999999999999995e-05
Step 3298, training loss: 3.7088069915771484, total_norm: 3.3316192626953125, lr: 5.9999999999999995e-05
Step 3299, training loss: 3.7170114517211914, total_norm: 3.665757894515991, lr: 5.9999999999999995e-05
Step 3300, training loss: 3.91345477104187, total_norm: 3.0938873291015625, lr: 5.9999999999999995e-05
Step 3300, validation loss: 5.388883113861084
Step 3301, training loss: 4.192902565002441, total_norm: 3.242860794067383, lr: 5.9999999999999995e-05
Step 3302, training loss: 3.3834540843963623, total_norm: 3.312049150466919, lr: 5.9999999999999995e-05
Step 3303, training loss: 3.340519905090332, total_norm: 2.865952491760254, lr: 5.9999999999999995e-05
Step 3304, training loss: 3.931086301803589, total_norm: 3.3132545948028564, lr: 5.9999999999999995e-05
Step 3305, training loss: 3.695769786834717, total_norm: 3.118098258972168, lr: 5.9999999999999995e-05
Step 3306, training loss: 4.137626647949219, total_norm: 3.4592978954315186, lr: 5.9999999999999995e-05
Step 3307, training loss: 3.96207857131958, total_norm: 3.489964723587036, lr: 5.9999999999999995e-05
Step 3308, training loss: 3.7610971927642822, total_norm: 3.2574362754821777, lr: 5.9999999999999995e-05
Step 3309, training loss: 4.034324645996094, total_norm: 3.6246442794799805, lr: 5.9999999999999995e-05
Step 3310, training loss: 3.5682199001312256, total_norm: 3.1239020824432373, lr: 5.9999999999999995e-05
Step 3311, training loss: 4.189610004425049, total_norm: 3.1411688327789307, lr: 5.9999999999999995e-05
Step 3312, training loss: 4.305029392242432, total_norm: 3.1787984371185303, lr: 5.9999999999999995e-05
Step 3313, training loss: 4.101113796234131, total_norm: 2.982956886291504, lr: 5.9999999999999995e-05
Step 3314, training loss: 4.4718804359436035, total_norm: 3.52022647857666, lr: 5.9999999999999995e-05
Step 3315, training loss: 4.543707847595215, total_norm: 3.5554373264312744, lr: 5.9999999999999995e-05
Step 3316, training loss: 4.418808937072754, total_norm: 2.9879045486450195, lr: 5.9999999999999995e-05
Step 3317, training loss: 4.069859027862549, total_norm: 3.55527663230896, lr: 5.9999999999999995e-05
Step 3318, training loss: 4.370645999908447, total_norm: 3.6788554191589355, lr: 5.9999999999999995e-05
Step 3319, training loss: 4.670654773712158, total_norm: 3.8868460655212402, lr: 5.9999999999999995e-05
Step 3320, training loss: 4.336549282073975, total_norm: 3.158761739730835, lr: 5.9999999999999995e-05
Step 3321, training loss: 3.74389910697937, total_norm: 2.950080394744873, lr: 5.9999999999999995e-05
Step 3322, training loss: 3.7977237701416016, total_norm: 3.9236221313476562, lr: 5.9999999999999995e-05
Step 3323, training loss: 4.363624572753906, total_norm: 3.687598943710327, lr: 5.9999999999999995e-05
Step 3324, training loss: 3.851332664489746, total_norm: 5.170154094696045, lr: 5.9999999999999995e-05
Step 3325, training loss: 4.229855537414551, total_norm: 5.465862274169922, lr: 5.9999999999999995e-05
Step 3326, training loss: 3.8908419609069824, total_norm: 4.1414875984191895, lr: 5.9999999999999995e-05
Step 3327, training loss: 3.458582878112793, total_norm: 3.315577268600464, lr: 5.9999999999999995e-05
Step 3328, training loss: 4.652117729187012, total_norm: 3.568281650543213, lr: 5.9999999999999995e-05
Step 3329, training loss: 4.4767374992370605, total_norm: 3.5528481006622314, lr: 5.9999999999999995e-05
Step 3330, training loss: 4.2391462326049805, total_norm: 3.8318445682525635, lr: 5.9999999999999995e-05
Step 3331, training loss: 4.581436634063721, total_norm: 4.295719623565674, lr: 5.9999999999999995e-05
Step 3332, training loss: 4.407209873199463, total_norm: 3.51601243019104, lr: 5.9999999999999995e-05
Step 3333, training loss: 3.6100826263427734, total_norm: 4.410069465637207, lr: 5.9999999999999995e-05
Step 3334, training loss: 3.9481067657470703, total_norm: 3.1176345348358154, lr: 5.9999999999999995e-05
Step 3335, training loss: 3.7983460426330566, total_norm: 2.9532485008239746, lr: 5.9999999999999995e-05
Step 3336, training loss: 3.378826379776001, total_norm: 2.8988351821899414, lr: 5.9999999999999995e-05
Step 3337, training loss: 3.709357976913452, total_norm: 2.9819867610931396, lr: 5.9999999999999995e-05
Step 3338, training loss: 4.79169225692749, total_norm: 3.7475531101226807, lr: 5.9999999999999995e-05
Step 3339, training loss: 2.9298059940338135, total_norm: 3.129800319671631, lr: 5.9999999999999995e-05
Step 3340, training loss: 3.7474257946014404, total_norm: 3.3503224849700928, lr: 5.9999999999999995e-05
Step 3341, training loss: 3.8717801570892334, total_norm: 2.890139579772949, lr: 5.9999999999999995e-05
Step 3342, training loss: 3.651437282562256, total_norm: 3.2934365272521973, lr: 5.9999999999999995e-05
Step 3343, training loss: 3.905005693435669, total_norm: 3.3151965141296387, lr: 5.9999999999999995e-05
Step 3344, training loss: 4.508087635040283, total_norm: 3.3338284492492676, lr: 5.9999999999999995e-05
Step 3345, training loss: 4.762695789337158, total_norm: 3.8545963764190674, lr: 5.9999999999999995e-05
Step 3346, training loss: 4.476487636566162, total_norm: 3.740960121154785, lr: 5.9999999999999995e-05
Step 3347, training loss: 4.452693462371826, total_norm: 3.7361912727355957, lr: 5.9999999999999995e-05
Step 3348, training loss: 3.8293349742889404, total_norm: 3.592707633972168, lr: 5.9999999999999995e-05
Step 3349, training loss: 4.709839344024658, total_norm: 4.409234046936035, lr: 5.9999999999999995e-05
Step 3350, training loss: 4.389026165008545, total_norm: 3.573007822036743, lr: 5.9999999999999995e-05
Step 3350, validation loss: 5.53538179397583
Step 3351, training loss: 4.5046067237854, total_norm: 3.4006242752075195, lr: 5.9999999999999995e-05
Step 3352, training loss: 3.986959218978882, total_norm: 3.1812355518341064, lr: 5.9999999999999995e-05
Step 3353, training loss: 4.164255142211914, total_norm: 3.545175313949585, lr: 5.9999999999999995e-05
Step 3354, training loss: 4.651636600494385, total_norm: 4.094732761383057, lr: 5.9999999999999995e-05
Step 3355, training loss: 4.556859016418457, total_norm: 5.602972984313965, lr: 5.9999999999999995e-05
Step 3356, training loss: 4.688933849334717, total_norm: 3.9542129039764404, lr: 5.9999999999999995e-05
Step 3357, training loss: 4.621438026428223, total_norm: 3.6516261100769043, lr: 5.9999999999999995e-05
Step 3358, training loss: 4.626956939697266, total_norm: 3.558969736099243, lr: 5.9999999999999995e-05
Step 3359, training loss: 4.71035099029541, total_norm: 3.8761351108551025, lr: 5.9999999999999995e-05
Step 3360, training loss: 4.804197788238525, total_norm: 3.903416395187378, lr: 5.9999999999999995e-05
Step 3361, training loss: 4.59525728225708, total_norm: 4.080523490905762, lr: 5.9999999999999995e-05
Step 3362, training loss: 4.417120933532715, total_norm: 4.712526798248291, lr: 5.9999999999999995e-05
Step 3363, training loss: 4.369266986846924, total_norm: 3.160433530807495, lr: 5.9999999999999995e-05
Step 3364, training loss: 4.591621398925781, total_norm: 3.509798049926758, lr: 5.9999999999999995e-05
Step 3365, training loss: 4.881958961486816, total_norm: 4.105722427368164, lr: 5.9999999999999995e-05
Step 3366, training loss: 4.320276737213135, total_norm: 4.187291145324707, lr: 5.9999999999999995e-05
Step 3367, training loss: 4.349967002868652, total_norm: 4.214625835418701, lr: 5.9999999999999995e-05
Step 3368, training loss: 3.7983579635620117, total_norm: 3.5546798706054688, lr: 5.9999999999999995e-05
Step 3369, training loss: 4.273718357086182, total_norm: 3.068194627761841, lr: 5.9999999999999995e-05
Step 3370, training loss: 4.672050476074219, total_norm: 3.3868026733398438, lr: 5.9999999999999995e-05
Step 3371, training loss: 4.591574668884277, total_norm: 3.657994031906128, lr: 5.9999999999999995e-05
Step 3372, training loss: 5.116751670837402, total_norm: 3.889763116836548, lr: 5.9999999999999995e-05
Step 3373, training loss: 4.321722984313965, total_norm: 4.76779317855835, lr: 5.9999999999999995e-05
Step 3374, training loss: 4.47313117980957, total_norm: 3.2469985485076904, lr: 5.9999999999999995e-05
Step 3375, training loss: 4.3619232177734375, total_norm: 3.741861581802368, lr: 5.9999999999999995e-05
Step 3376, training loss: 4.434142112731934, total_norm: 3.9814796447753906, lr: 5.9999999999999995e-05
Step 3377, training loss: 4.3872833251953125, total_norm: 4.534132480621338, lr: 5.9999999999999995e-05
Step 3378, training loss: 4.710414409637451, total_norm: 4.8382649421691895, lr: 5.9999999999999995e-05
Step 3379, training loss: 4.700281143188477, total_norm: 4.001122951507568, lr: 5.9999999999999995e-05
Step 3380, training loss: 4.262441635131836, total_norm: 3.8596737384796143, lr: 5.9999999999999995e-05
Step 3381, training loss: 4.33219051361084, total_norm: 3.4341466426849365, lr: 5.9999999999999995e-05
Step 3382, training loss: 4.339247226715088, total_norm: 5.944640159606934, lr: 5.9999999999999995e-05
Step 3383, training loss: 4.028308391571045, total_norm: 6.543644428253174, lr: 5.9999999999999995e-05
Step 3384, training loss: 3.9940319061279297, total_norm: 4.877355575561523, lr: 5.9999999999999995e-05
Step 3385, training loss: 4.1169538497924805, total_norm: 3.7207932472229004, lr: 5.9999999999999995e-05
Step 3386, training loss: 4.344517230987549, total_norm: 3.7593865394592285, lr: 5.9999999999999995e-05
Step 3387, training loss: 4.528700351715088, total_norm: 4.027652740478516, lr: 5.9999999999999995e-05
Step 3388, training loss: 4.3023529052734375, total_norm: 4.095391273498535, lr: 5.9999999999999995e-05
Step 3389, training loss: 4.584130764007568, total_norm: 4.7267656326293945, lr: 5.9999999999999995e-05
Step 3390, training loss: 4.692101955413818, total_norm: 4.527634143829346, lr: 5.9999999999999995e-05
Step 3391, training loss: 4.700529098510742, total_norm: 4.396553039550781, lr: 5.9999999999999995e-05
Step 3392, training loss: 4.566839694976807, total_norm: 4.536569118499756, lr: 5.9999999999999995e-05
Step 3393, training loss: 4.809074878692627, total_norm: 3.9940755367279053, lr: 5.9999999999999995e-05
Step 3394, training loss: 3.9910573959350586, total_norm: 3.6433534622192383, lr: 5.9999999999999995e-05
Step 3395, training loss: 4.22390604019165, total_norm: 3.867384195327759, lr: 5.9999999999999995e-05
Step 3396, training loss: 4.849425792694092, total_norm: 3.7224254608154297, lr: 5.9999999999999995e-05
Step 3397, training loss: 4.332492351531982, total_norm: 3.445615291595459, lr: 5.9999999999999995e-05
Step 3398, training loss: 4.574706077575684, total_norm: 3.7789337635040283, lr: 5.9999999999999995e-05
Step 3399, training loss: 3.5419132709503174, total_norm: 2.954238176345825, lr: 5.9999999999999995e-05
Step 3400, training loss: 4.50841760635376, total_norm: 3.225996971130371, lr: 5.9999999999999995e-05
Step 3400, validation loss: 5.557401657104492
Step 3401, training loss: 4.725983142852783, total_norm: 3.736900568008423, lr: 5.9999999999999995e-05
Step 3402, training loss: 4.235223770141602, total_norm: 3.391646385192871, lr: 5.9999999999999995e-05
Step 3403, training loss: 3.7497434616088867, total_norm: 3.4325313568115234, lr: 5.9999999999999995e-05
Step 3404, training loss: 4.243245601654053, total_norm: 3.473994255065918, lr: 5.9999999999999995e-05
Step 3405, training loss: 4.300058364868164, total_norm: 3.5974035263061523, lr: 5.9999999999999995e-05
Step 3406, training loss: 4.4173760414123535, total_norm: 3.3736557960510254, lr: 5.9999999999999995e-05
Step 3407, training loss: 4.148010730743408, total_norm: 3.434234142303467, lr: 5.9999999999999995e-05
Step 3408, training loss: 4.492086410522461, total_norm: 3.4568967819213867, lr: 5.9999999999999995e-05
Step 3409, training loss: 3.9210317134857178, total_norm: 3.2809176445007324, lr: 5.9999999999999995e-05
Step 3410, training loss: 3.546926736831665, total_norm: 3.365844964981079, lr: 5.9999999999999995e-05
Step 3411, training loss: 4.432293891906738, total_norm: 2.901171922683716, lr: 5.9999999999999995e-05
Step 3412, training loss: 4.526764869689941, total_norm: 3.1895546913146973, lr: 5.9999999999999995e-05
Step 3413, training loss: 4.035063743591309, total_norm: 3.1013989448547363, lr: 5.9999999999999995e-05
Step 3414, training loss: 4.538403511047363, total_norm: 3.923929452896118, lr: 5.9999999999999995e-05
Step 3415, training loss: 3.2481579780578613, total_norm: 4.397924900054932, lr: 5.9999999999999995e-05
Step 3416, training loss: 3.0973293781280518, total_norm: 3.59789776802063, lr: 5.9999999999999995e-05
Step 3417, training loss: 3.7753100395202637, total_norm: 3.3967785835266113, lr: 5.9999999999999995e-05
Step 3418, training loss: 3.1568377017974854, total_norm: 3.1951069831848145, lr: 5.9999999999999995e-05
Step 3419, training loss: 4.217283248901367, total_norm: 3.4888992309570312, lr: 5.9999999999999995e-05
Step 3420, training loss: 3.9042046070098877, total_norm: 3.4263997077941895, lr: 5.9999999999999995e-05
Step 3421, training loss: 3.857438802719116, total_norm: 3.3122193813323975, lr: 5.9999999999999995e-05
Step 3422, training loss: 4.860742568969727, total_norm: 5.527731895446777, lr: 5.9999999999999995e-05
Step 3423, training loss: 4.930883407592773, total_norm: 5.722103595733643, lr: 5.9999999999999995e-05
Step 3424, training loss: 4.289514064788818, total_norm: 6.112889766693115, lr: 5.9999999999999995e-05
Step 3425, training loss: 4.329593658447266, total_norm: 4.57265043258667, lr: 5.9999999999999995e-05
Step 3426, training loss: 4.616930961608887, total_norm: 5.634152412414551, lr: 5.9999999999999995e-05
Step 3427, training loss: 4.106268405914307, total_norm: 4.454740524291992, lr: 5.9999999999999995e-05
Step 3428, training loss: 4.491705417633057, total_norm: 4.280014991760254, lr: 5.9999999999999995e-05
Step 3429, training loss: 4.920469760894775, total_norm: 5.234678268432617, lr: 5.9999999999999995e-05
Step 3430, training loss: 4.245789051055908, total_norm: 5.013749122619629, lr: 5.9999999999999995e-05
Step 3431, training loss: 4.288327693939209, total_norm: 3.8030450344085693, lr: 5.9999999999999995e-05
Step 3432, training loss: 4.501062870025635, total_norm: 4.037328243255615, lr: 5.9999999999999995e-05
Step 3433, training loss: 5.165794849395752, total_norm: 4.171914577484131, lr: 5.9999999999999995e-05
Step 3434, training loss: 4.343400001525879, total_norm: 3.654402256011963, lr: 5.9999999999999995e-05
Step 3435, training loss: 4.143835067749023, total_norm: 3.6226048469543457, lr: 5.9999999999999995e-05
Step 3436, training loss: 4.725625038146973, total_norm: 3.892151117324829, lr: 5.9999999999999995e-05
Step 3437, training loss: 4.218991279602051, total_norm: 3.5372211933135986, lr: 5.9999999999999995e-05
Step 3438, training loss: 4.2308669090271, total_norm: 3.381652593612671, lr: 5.9999999999999995e-05
Step 3439, training loss: 4.043213367462158, total_norm: 3.4147610664367676, lr: 5.9999999999999995e-05
Step 3440, training loss: 5.408452033996582, total_norm: 5.152682304382324, lr: 5.9999999999999995e-05
Step 3441, training loss: 4.754937171936035, total_norm: 3.795412540435791, lr: 5.9999999999999995e-05
Step 3442, training loss: 4.338458061218262, total_norm: 3.099301815032959, lr: 5.9999999999999995e-05
Step 3443, training loss: 4.16361141204834, total_norm: 3.582475423812866, lr: 5.9999999999999995e-05
Step 3444, training loss: 3.821141242980957, total_norm: 3.6390573978424072, lr: 5.9999999999999995e-05
Step 3445, training loss: 4.020801544189453, total_norm: 3.2956435680389404, lr: 5.9999999999999995e-05
Step 3446, training loss: 4.697641372680664, total_norm: 4.075350284576416, lr: 5.9999999999999995e-05
Step 3447, training loss: 4.313591003417969, total_norm: 3.3699796199798584, lr: 5.9999999999999995e-05
Step 3448, training loss: 4.005087375640869, total_norm: 2.960059642791748, lr: 5.9999999999999995e-05
Step 3449, training loss: 4.1542816162109375, total_norm: 3.182788848876953, lr: 5.9999999999999995e-05
Step 3450, training loss: 3.7637882232666016, total_norm: 2.9454524517059326, lr: 5.9999999999999995e-05
Step 3450, validation loss: 5.536703109741211
Step 3451, training loss: 3.9571619033813477, total_norm: 3.5449442863464355, lr: 5.9999999999999995e-05
Step 3452, training loss: 5.400362014770508, total_norm: 6.087395668029785, lr: 5.9999999999999995e-05
Step 3453, training loss: 4.408878803253174, total_norm: 4.860286712646484, lr: 5.9999999999999995e-05
Step 3454, training loss: 3.848360538482666, total_norm: 3.882019519805908, lr: 5.9999999999999995e-05
Step 3455, training loss: 5.177409648895264, total_norm: 4.521396636962891, lr: 5.9999999999999995e-05
Step 3456, training loss: 4.0574259757995605, total_norm: 3.484123945236206, lr: 5.9999999999999995e-05
Step 3457, training loss: 3.8778676986694336, total_norm: 3.553438425064087, lr: 5.9999999999999995e-05
Step 3458, training loss: 4.288390636444092, total_norm: 4.098901271820068, lr: 5.9999999999999995e-05
Step 3459, training loss: 4.053219318389893, total_norm: 3.4350438117980957, lr: 5.9999999999999995e-05
Step 3460, training loss: 4.621922969818115, total_norm: 3.6966402530670166, lr: 5.9999999999999995e-05
Step 3461, training loss: 4.043334007263184, total_norm: 3.036322593688965, lr: 5.9999999999999995e-05
Step 3462, training loss: 4.5278191566467285, total_norm: 3.5542216300964355, lr: 5.9999999999999995e-05
Step 3463, training loss: 4.2042365074157715, total_norm: 3.7634100914001465, lr: 5.9999999999999995e-05
Step 3464, training loss: 4.688676357269287, total_norm: 4.595978260040283, lr: 5.9999999999999995e-05
Step 3465, training loss: 4.088490962982178, total_norm: 3.699174642562866, lr: 5.9999999999999995e-05
Step 3466, training loss: 4.2199225425720215, total_norm: 4.230759143829346, lr: 5.9999999999999995e-05
Step 3467, training loss: 3.8277411460876465, total_norm: 3.564335823059082, lr: 5.9999999999999995e-05
Step 3468, training loss: 4.853202819824219, total_norm: 3.6383392810821533, lr: 5.9999999999999995e-05
Step 3469, training loss: 4.701223850250244, total_norm: 3.6102566719055176, lr: 5.9999999999999995e-05
Step 3470, training loss: 4.218644618988037, total_norm: 3.814044237136841, lr: 5.9999999999999995e-05
Step 3471, training loss: 4.675931930541992, total_norm: 4.967780590057373, lr: 5.9999999999999995e-05
Step 3472, training loss: 4.3186845779418945, total_norm: 4.500592231750488, lr: 5.9999999999999995e-05
Step 3473, training loss: 4.42509126663208, total_norm: 4.488322734832764, lr: 5.9999999999999995e-05
Step 3474, training loss: 4.184256553649902, total_norm: 3.982553005218506, lr: 5.9999999999999995e-05
Step 3475, training loss: 4.066778182983398, total_norm: 3.467775344848633, lr: 5.9999999999999995e-05
Step 3476, training loss: 4.710860252380371, total_norm: 3.9904236793518066, lr: 5.9999999999999995e-05
Step 3477, training loss: 4.010836124420166, total_norm: 3.739783525466919, lr: 5.9999999999999995e-05
Step 3478, training loss: 4.252817630767822, total_norm: 3.532308578491211, lr: 5.9999999999999995e-05
Step 3479, training loss: 4.023990631103516, total_norm: 3.2843518257141113, lr: 5.9999999999999995e-05
Step 3480, training loss: 3.3578407764434814, total_norm: 3.3687992095947266, lr: 5.9999999999999995e-05
Step 3481, training loss: 3.863291025161743, total_norm: 3.193047046661377, lr: 5.9999999999999995e-05
Step 3482, training loss: 4.230612754821777, total_norm: 3.3747615814208984, lr: 5.9999999999999995e-05
Step 3483, training loss: 4.2468061447143555, total_norm: 3.567960262298584, lr: 5.9999999999999995e-05
Step 3484, training loss: 3.8097076416015625, total_norm: 2.9361112117767334, lr: 5.9999999999999995e-05
Step 3485, training loss: 3.724318742752075, total_norm: 4.957223415374756, lr: 5.9999999999999995e-05
Step 3486, training loss: 3.7524430751800537, total_norm: 3.4394118785858154, lr: 5.9999999999999995e-05
Step 3487, training loss: 4.850677013397217, total_norm: 3.376943588256836, lr: 5.9999999999999995e-05
Step 3488, training loss: 4.062463283538818, total_norm: 3.3056960105895996, lr: 5.9999999999999995e-05
Step 3489, training loss: 3.8258347511291504, total_norm: 3.67484188079834, lr: 5.9999999999999995e-05
Step 3490, training loss: 4.25202751159668, total_norm: 3.7425882816314697, lr: 5.9999999999999995e-05
Step 3491, training loss: 4.107503890991211, total_norm: 3.250246286392212, lr: 5.9999999999999995e-05
Step 3492, training loss: 3.924797296524048, total_norm: 3.913127899169922, lr: 5.9999999999999995e-05
Step 3493, training loss: 4.187016010284424, total_norm: 3.79002046585083, lr: 5.9999999999999995e-05
Step 3494, training loss: 4.50457239151001, total_norm: 3.469896078109741, lr: 5.9999999999999995e-05
Step 3495, training loss: 4.641600131988525, total_norm: 5.086030006408691, lr: 5.9999999999999995e-05
Step 3496, training loss: 4.483917713165283, total_norm: 4.6940999031066895, lr: 5.9999999999999995e-05
Step 3497, training loss: 5.039875507354736, total_norm: 3.8906490802764893, lr: 5.9999999999999995e-05
Step 3498, training loss: 4.244658470153809, total_norm: 3.843735933303833, lr: 5.9999999999999995e-05
Step 3499, training loss: 4.694491863250732, total_norm: 3.4001665115356445, lr: 5.9999999999999995e-05
Step 3500, training loss: 4.180517673492432, total_norm: 3.4820704460144043, lr: 5.9999999999999995e-05
Step 3500, validation loss: 5.566359043121338
Step 3501, training loss: 4.818573474884033, total_norm: 3.4499406814575195, lr: 5.9999999999999995e-05
Step 3502, training loss: 3.7688636779785156, total_norm: 3.247467517852783, lr: 5.9999999999999995e-05
Step 3503, training loss: 4.205357074737549, total_norm: 3.5409302711486816, lr: 5.9999999999999995e-05
Step 3504, training loss: 4.029562473297119, total_norm: 3.2348556518554688, lr: 5.9999999999999995e-05
Step 3505, training loss: 4.575306415557861, total_norm: 3.8033716678619385, lr: 5.9999999999999995e-05
Step 3506, training loss: 4.138582229614258, total_norm: 3.494260549545288, lr: 5.9999999999999995e-05
Step 3507, training loss: 4.397693157196045, total_norm: 4.345822811126709, lr: 5.9999999999999995e-05
Step 3508, training loss: 4.304858207702637, total_norm: 4.529969215393066, lr: 5.9999999999999995e-05
Step 3509, training loss: 3.9452598094940186, total_norm: 4.014914512634277, lr: 5.9999999999999995e-05
Step 3510, training loss: 3.929586887359619, total_norm: 3.7832446098327637, lr: 5.9999999999999995e-05
Step 3511, training loss: 3.8003058433532715, total_norm: 3.7912063598632812, lr: 5.9999999999999995e-05
Step 3512, training loss: 3.7452187538146973, total_norm: 4.362569332122803, lr: 5.9999999999999995e-05
Step 3513, training loss: 3.9292349815368652, total_norm: 3.313206672668457, lr: 5.9999999999999995e-05
Step 3514, training loss: 3.71083402633667, total_norm: 3.036886215209961, lr: 5.9999999999999995e-05
Step 3515, training loss: 4.152834415435791, total_norm: 3.7172152996063232, lr: 5.9999999999999995e-05
Step 3516, training loss: 4.130230903625488, total_norm: 3.5038158893585205, lr: 5.9999999999999995e-05
Step 3517, training loss: 4.051262855529785, total_norm: 3.4380316734313965, lr: 5.9999999999999995e-05
Step 3518, training loss: 4.753427982330322, total_norm: 4.001748561859131, lr: 5.9999999999999995e-05
Step 3519, training loss: 4.342331409454346, total_norm: 3.3859546184539795, lr: 5.9999999999999995e-05
Step 3520, training loss: 4.412805080413818, total_norm: 4.111259460449219, lr: 5.9999999999999995e-05
Step 3521, training loss: 4.738158226013184, total_norm: 3.8433837890625, lr: 5.9999999999999995e-05
Step 3522, training loss: 4.134565830230713, total_norm: 3.4939284324645996, lr: 5.9999999999999995e-05
Step 3523, training loss: 4.683276653289795, total_norm: 3.505037784576416, lr: 5.9999999999999995e-05
Step 3524, training loss: 4.565837383270264, total_norm: 3.927060604095459, lr: 5.9999999999999995e-05
Step 3525, training loss: 4.5612006187438965, total_norm: 3.564182996749878, lr: 5.9999999999999995e-05
Step 3526, training loss: 4.3479790687561035, total_norm: 3.6246190071105957, lr: 5.9999999999999995e-05
Step 3527, training loss: 4.422656059265137, total_norm: 4.063272476196289, lr: 5.9999999999999995e-05
Step 3528, training loss: 4.562547206878662, total_norm: 3.2129108905792236, lr: 5.9999999999999995e-05
Step 3529, training loss: 3.956906318664551, total_norm: 3.5377261638641357, lr: 5.9999999999999995e-05
Step 3530, training loss: 3.61572265625, total_norm: 3.600452184677124, lr: 5.9999999999999995e-05
Step 3531, training loss: 3.855797052383423, total_norm: 3.544584274291992, lr: 5.9999999999999995e-05
Step 3532, training loss: 4.3730244636535645, total_norm: 4.090089797973633, lr: 5.9999999999999995e-05
Step 3533, training loss: 4.385993957519531, total_norm: 4.211956024169922, lr: 5.9999999999999995e-05
Step 3534, training loss: 4.395493030548096, total_norm: 4.316386699676514, lr: 5.9999999999999995e-05
Step 3535, training loss: 4.963058948516846, total_norm: 4.019265174865723, lr: 5.9999999999999995e-05
Step 3536, training loss: 4.396263122558594, total_norm: 4.396656513214111, lr: 5.9999999999999995e-05
Step 3537, training loss: 4.058960914611816, total_norm: 4.457959175109863, lr: 5.9999999999999995e-05
Step 3538, training loss: 4.605833053588867, total_norm: 3.3373842239379883, lr: 5.9999999999999995e-05
Step 3539, training loss: 4.1429009437561035, total_norm: 3.8206584453582764, lr: 5.9999999999999995e-05
Step 3540, training loss: 4.349752426147461, total_norm: 4.0528459548950195, lr: 5.9999999999999995e-05
Step 3541, training loss: 4.331056594848633, total_norm: 3.3912394046783447, lr: 5.9999999999999995e-05
Step 3542, training loss: 3.8116469383239746, total_norm: 3.544778823852539, lr: 5.9999999999999995e-05
Step 3543, training loss: 3.7425360679626465, total_norm: 3.582754373550415, lr: 5.9999999999999995e-05
Step 3544, training loss: 2.954916000366211, total_norm: 7.767958164215088, lr: 5.9999999999999995e-05
Step 3545, training loss: 3.3679206371307373, total_norm: 3.8720285892486572, lr: 5.9999999999999995e-05
Step 3546, training loss: 3.4986653327941895, total_norm: 3.5659093856811523, lr: 5.9999999999999995e-05
Step 3547, training loss: 4.731492042541504, total_norm: 3.781736135482788, lr: 5.9999999999999995e-05
Step 3548, training loss: 4.705617904663086, total_norm: 4.305691242218018, lr: 5.9999999999999995e-05
Step 3549, training loss: 4.173851490020752, total_norm: 4.715347766876221, lr: 5.9999999999999995e-05
Step 3550, training loss: 4.616777420043945, total_norm: 5.116004943847656, lr: 5.9999999999999995e-05
Step 3550, validation loss: 5.613828182220459
Step 3551, training loss: 4.214315891265869, total_norm: 5.5805487632751465, lr: 5.9999999999999995e-05
Step 3552, training loss: 4.133924961090088, total_norm: 4.672442436218262, lr: 5.9999999999999995e-05
Step 3553, training loss: 4.415539264678955, total_norm: 3.8713111877441406, lr: 5.9999999999999995e-05
Step 3554, training loss: 3.900339365005493, total_norm: 3.919687032699585, lr: 5.9999999999999995e-05
Step 3555, training loss: 3.8878378868103027, total_norm: 4.04956579208374, lr: 5.9999999999999995e-05
Step 3556, training loss: 4.153671741485596, total_norm: 3.849490165710449, lr: 5.9999999999999995e-05
Step 3557, training loss: 4.002998352050781, total_norm: 3.249959945678711, lr: 5.9999999999999995e-05
Step 3558, training loss: 4.264034748077393, total_norm: 3.463637590408325, lr: 5.9999999999999995e-05
Step 3559, training loss: 4.182400703430176, total_norm: 3.374864339828491, lr: 5.9999999999999995e-05
Step 3560, training loss: 4.047985076904297, total_norm: 4.372615814208984, lr: 5.9999999999999995e-05
Step 3561, training loss: 3.9229910373687744, total_norm: 3.640272855758667, lr: 5.9999999999999995e-05
Step 3562, training loss: 4.253833770751953, total_norm: 3.9138481616973877, lr: 5.9999999999999995e-05
Step 3563, training loss: 4.045510292053223, total_norm: 4.02876615524292, lr: 5.9999999999999995e-05
Step 3564, training loss: 3.994576930999756, total_norm: 4.314840793609619, lr: 5.9999999999999995e-05
Step 3565, training loss: 4.212357044219971, total_norm: 3.7527027130126953, lr: 5.9999999999999995e-05
Step 3566, training loss: 3.966780662536621, total_norm: 4.052304267883301, lr: 5.9999999999999995e-05
Step 3567, training loss: 3.9483022689819336, total_norm: 3.9764482975006104, lr: 5.9999999999999995e-05
Step 3568, training loss: 4.3219313621521, total_norm: 4.8359246253967285, lr: 5.9999999999999995e-05
Step 3569, training loss: 4.426235675811768, total_norm: 4.457551002502441, lr: 5.9999999999999995e-05
Step 3570, training loss: 3.7694039344787598, total_norm: 3.4981071949005127, lr: 5.9999999999999995e-05
Step 3571, training loss: 3.7978670597076416, total_norm: 3.353139877319336, lr: 5.9999999999999995e-05
Step 3572, training loss: 3.9353909492492676, total_norm: 3.2857322692871094, lr: 5.9999999999999995e-05
Step 3573, training loss: 4.24497652053833, total_norm: 3.652939796447754, lr: 5.9999999999999995e-05
Step 3574, training loss: 4.1694135665893555, total_norm: 3.3891403675079346, lr: 5.9999999999999995e-05
Step 3575, training loss: 4.799313545227051, total_norm: 3.741363286972046, lr: 5.9999999999999995e-05
Step 3576, training loss: 4.073829174041748, total_norm: 3.375804901123047, lr: 5.9999999999999995e-05
Step 3577, training loss: 3.806847333908081, total_norm: 3.470921516418457, lr: 5.9999999999999995e-05
Step 3578, training loss: 3.4132654666900635, total_norm: 3.476879358291626, lr: 5.9999999999999995e-05
Step 3579, training loss: 3.587217092514038, total_norm: 3.69828462600708, lr: 5.9999999999999995e-05
Step 3580, training loss: 4.044175624847412, total_norm: 3.742525339126587, lr: 5.9999999999999995e-05
Step 3581, training loss: 4.3594183921813965, total_norm: 3.913008689880371, lr: 5.9999999999999995e-05
Step 3582, training loss: 4.683784008026123, total_norm: 3.9913570880889893, lr: 5.9999999999999995e-05
Step 3583, training loss: 4.3827667236328125, total_norm: 4.241386413574219, lr: 5.9999999999999995e-05
Step 3584, training loss: 5.2117180824279785, total_norm: 5.218671798706055, lr: 5.9999999999999995e-05
Step 3585, training loss: 4.485525608062744, total_norm: 3.992990255355835, lr: 5.9999999999999995e-05
Step 3586, training loss: 4.7332000732421875, total_norm: 5.679999351501465, lr: 5.9999999999999995e-05
Step 3587, training loss: 4.439671993255615, total_norm: 4.079267501831055, lr: 5.9999999999999995e-05
Step 3588, training loss: 4.771295070648193, total_norm: 3.830177068710327, lr: 5.9999999999999995e-05
Step 3589, training loss: 4.6240081787109375, total_norm: 4.111006736755371, lr: 5.9999999999999995e-05
Step 3590, training loss: 4.489121913909912, total_norm: 4.109594345092773, lr: 5.9999999999999995e-05
Step 3591, training loss: 4.3592352867126465, total_norm: 4.031835556030273, lr: 5.9999999999999995e-05
Step 3592, training loss: 4.773966312408447, total_norm: 4.2340006828308105, lr: 5.9999999999999995e-05
Step 3593, training loss: 4.6104736328125, total_norm: 3.6852238178253174, lr: 5.9999999999999995e-05
Step 3594, training loss: 4.1502604484558105, total_norm: 3.3078794479370117, lr: 5.9999999999999995e-05
Step 3595, training loss: 4.097970962524414, total_norm: 3.610646963119507, lr: 5.9999999999999995e-05
Step 3596, training loss: 4.549594879150391, total_norm: 3.3837087154388428, lr: 5.9999999999999995e-05
Step 3597, training loss: 3.881498336791992, total_norm: 3.758852005004883, lr: 5.9999999999999995e-05
Step 3598, training loss: 4.1906890869140625, total_norm: 3.6110341548919678, lr: 5.9999999999999995e-05
Step 3599, training loss: 4.538432598114014, total_norm: 3.3639395236968994, lr: 5.9999999999999995e-05
Step 3600, training loss: 4.197556495666504, total_norm: 3.569678544998169, lr: 5.9999999999999995e-05
Step 3600, validation loss: 5.473489761352539
Step 3601, training loss: 4.397963047027588, total_norm: 3.6321537494659424, lr: 5.9999999999999995e-05
Step 3602, training loss: 4.240528583526611, total_norm: 3.594451427459717, lr: 5.9999999999999995e-05
Step 3603, training loss: 4.496904373168945, total_norm: 3.6881585121154785, lr: 5.9999999999999995e-05
Step 3604, training loss: 4.187897682189941, total_norm: 3.337632656097412, lr: 5.9999999999999995e-05
Step 3605, training loss: 3.8874001502990723, total_norm: 4.307774543762207, lr: 5.9999999999999995e-05
Step 3606, training loss: 3.907893180847168, total_norm: 3.325709104537964, lr: 5.9999999999999995e-05
Step 3607, training loss: 4.258935928344727, total_norm: 3.6776952743530273, lr: 5.9999999999999995e-05
Step 3608, training loss: 4.127564907073975, total_norm: 3.184739351272583, lr: 5.9999999999999995e-05
Step 3609, training loss: 4.48950719833374, total_norm: 3.426693916320801, lr: 5.9999999999999995e-05
Step 3610, training loss: 4.712864875793457, total_norm: 3.505723476409912, lr: 5.9999999999999995e-05
Step 3611, training loss: 4.47412633895874, total_norm: 3.366899013519287, lr: 5.9999999999999995e-05
Step 3612, training loss: 4.57464599609375, total_norm: 3.391531229019165, lr: 5.9999999999999995e-05
Step 3613, training loss: 3.9428000450134277, total_norm: 3.455047130584717, lr: 5.9999999999999995e-05
Step 3614, training loss: 4.832212924957275, total_norm: 3.869856595993042, lr: 5.9999999999999995e-05
Step 3615, training loss: 4.425868511199951, total_norm: 3.352663278579712, lr: 5.9999999999999995e-05
Step 3616, training loss: 4.202550411224365, total_norm: 3.123168468475342, lr: 5.9999999999999995e-05
Step 3617, training loss: 5.117044925689697, total_norm: 3.7430977821350098, lr: 5.9999999999999995e-05
Step 3618, training loss: 5.296717643737793, total_norm: 4.527337074279785, lr: 5.9999999999999995e-05
Step 3619, training loss: 4.689211368560791, total_norm: 4.516524791717529, lr: 5.9999999999999995e-05
Step 3620, training loss: 4.841867923736572, total_norm: 4.3155107498168945, lr: 5.9999999999999995e-05
Step 3621, training loss: 4.897921085357666, total_norm: 4.270412445068359, lr: 5.9999999999999995e-05
Step 3622, training loss: 5.160371780395508, total_norm: 4.094585418701172, lr: 5.9999999999999995e-05
Step 3623, training loss: 4.729448318481445, total_norm: 4.269606590270996, lr: 5.9999999999999995e-05
Step 3624, training loss: 4.244863986968994, total_norm: 4.5428290367126465, lr: 5.9999999999999995e-05
Step 3625, training loss: 4.3196821212768555, total_norm: 3.7888762950897217, lr: 5.9999999999999995e-05
Step 3626, training loss: 4.715184211730957, total_norm: 4.152464389801025, lr: 5.9999999999999995e-05
Step 3627, training loss: 4.700174331665039, total_norm: 4.152162075042725, lr: 5.9999999999999995e-05
Step 3628, training loss: 4.870398998260498, total_norm: 3.9198286533355713, lr: 5.9999999999999995e-05
Step 3629, training loss: 4.2654924392700195, total_norm: 3.9904844760894775, lr: 5.9999999999999995e-05
Step 3630, training loss: 5.000856876373291, total_norm: 4.784846305847168, lr: 5.9999999999999995e-05
Step 3631, training loss: 4.870017051696777, total_norm: 5.909985065460205, lr: 5.9999999999999995e-05
Step 3632, training loss: 4.387476444244385, total_norm: 4.219875335693359, lr: 5.9999999999999995e-05
Step 3633, training loss: 3.969721555709839, total_norm: 3.9279680252075195, lr: 5.9999999999999995e-05
Step 3634, training loss: 4.714186668395996, total_norm: 3.2839856147766113, lr: 5.9999999999999995e-05
Step 3635, training loss: 3.8958792686462402, total_norm: 3.151560068130493, lr: 5.9999999999999995e-05
Step 3636, training loss: 3.9282965660095215, total_norm: 3.977073907852173, lr: 5.9999999999999995e-05
Step 3637, training loss: 4.6230058670043945, total_norm: 4.075723648071289, lr: 5.9999999999999995e-05
Step 3638, training loss: 4.172494411468506, total_norm: 3.747026205062866, lr: 5.9999999999999995e-05
Step 3639, training loss: 4.168280601501465, total_norm: 3.592505693435669, lr: 5.9999999999999995e-05
Step 3640, training loss: 4.286755561828613, total_norm: 3.6322426795959473, lr: 5.9999999999999995e-05
Step 3641, training loss: 4.805176734924316, total_norm: 3.5391249656677246, lr: 5.9999999999999995e-05
Step 3642, training loss: 3.8917112350463867, total_norm: 4.169064521789551, lr: 5.9999999999999995e-05
Step 3643, training loss: 4.105506420135498, total_norm: 4.0654520988464355, lr: 5.9999999999999995e-05
Step 3644, training loss: 4.0855255126953125, total_norm: 4.30548620223999, lr: 5.9999999999999995e-05
Step 3645, training loss: 4.11574125289917, total_norm: 5.068077087402344, lr: 5.9999999999999995e-05
Step 3646, training loss: 4.99137544631958, total_norm: 4.290595531463623, lr: 5.9999999999999995e-05
Step 3647, training loss: 4.096834659576416, total_norm: 3.3046915531158447, lr: 5.9999999999999995e-05
Step 3648, training loss: 4.527780532836914, total_norm: 3.558408498764038, lr: 5.9999999999999995e-05
Step 3649, training loss: 4.252790927886963, total_norm: 4.128690242767334, lr: 5.9999999999999995e-05
Step 3650, training loss: 4.355510711669922, total_norm: 4.101975917816162, lr: 5.9999999999999995e-05
Step 3650, validation loss: 5.466311454772949
Step 3651, training loss: 4.647030830383301, total_norm: 3.9199891090393066, lr: 5.9999999999999995e-05
Step 3652, training loss: 4.580891132354736, total_norm: 3.7808802127838135, lr: 5.9999999999999995e-05
Step 3653, training loss: 4.004998683929443, total_norm: 3.0726473331451416, lr: 5.9999999999999995e-05
Step 3654, training loss: 4.831142425537109, total_norm: 4.277027130126953, lr: 5.9999999999999995e-05
Step 3655, training loss: 5.13461971282959, total_norm: 4.408635139465332, lr: 5.9999999999999995e-05
Step 3656, training loss: 5.081787109375, total_norm: 3.9734108448028564, lr: 5.9999999999999995e-05
Step 3657, training loss: 4.33674955368042, total_norm: 4.465025901794434, lr: 5.9999999999999995e-05
Step 3658, training loss: 3.8978233337402344, total_norm: 4.950367450714111, lr: 5.9999999999999995e-05
Step 3659, training loss: 4.460079193115234, total_norm: 4.998261451721191, lr: 5.9999999999999995e-05
Step 3660, training loss: 4.050634384155273, total_norm: 4.546705722808838, lr: 5.9999999999999995e-05
Step 3661, training loss: 4.20540189743042, total_norm: 3.8296751976013184, lr: 5.9999999999999995e-05
Step 3662, training loss: 4.675149440765381, total_norm: 4.311212062835693, lr: 5.9999999999999995e-05
Step 3663, training loss: 4.871543884277344, total_norm: 4.373243808746338, lr: 5.9999999999999995e-05
Step 3664, training loss: 4.344479084014893, total_norm: 4.8032121658325195, lr: 5.9999999999999995e-05
Step 3665, training loss: 4.356317520141602, total_norm: 6.4851579666137695, lr: 5.9999999999999995e-05
Step 3666, training loss: 4.29542350769043, total_norm: 10.897510528564453, lr: 5.9999999999999995e-05
Step 3667, training loss: 4.442302703857422, total_norm: 8.440961837768555, lr: 5.9999999999999995e-05
Step 3668, training loss: 4.579719066619873, total_norm: 4.25403356552124, lr: 5.9999999999999995e-05
Step 3669, training loss: 4.686931610107422, total_norm: 4.596551418304443, lr: 5.9999999999999995e-05
Step 3670, training loss: 4.941295623779297, total_norm: 4.697020053863525, lr: 5.9999999999999995e-05
Step 3671, training loss: 4.287243843078613, total_norm: 6.6404032707214355, lr: 5.9999999999999995e-05
Step 3672, training loss: 4.672402381896973, total_norm: 9.606585502624512, lr: 5.9999999999999995e-05
Step 3673, training loss: 4.278783798217773, total_norm: 7.326928615570068, lr: 5.9999999999999995e-05
Step 3674, training loss: 4.275108814239502, total_norm: 7.184824466705322, lr: 5.9999999999999995e-05
Step 3675, training loss: 4.053955078125, total_norm: 7.095487117767334, lr: 5.9999999999999995e-05
Step 3676, training loss: 4.280182838439941, total_norm: 4.836544036865234, lr: 5.9999999999999995e-05
Step 3677, training loss: 3.518411636352539, total_norm: 3.9682648181915283, lr: 5.9999999999999995e-05
Step 3678, training loss: 4.162998199462891, total_norm: 4.254825592041016, lr: 5.9999999999999995e-05
Step 3679, training loss: 4.077432632446289, total_norm: 5.393550395965576, lr: 5.9999999999999995e-05
Step 3680, training loss: 4.033479690551758, total_norm: 4.570818901062012, lr: 5.9999999999999995e-05
Step 3681, training loss: 3.941244602203369, total_norm: 4.9144511222839355, lr: 5.9999999999999995e-05
Step 3682, training loss: 3.557447671890259, total_norm: 5.547533988952637, lr: 5.9999999999999995e-05
Step 3683, training loss: 4.099976539611816, total_norm: 4.51802921295166, lr: 5.9999999999999995e-05
Step 3684, training loss: 4.337543487548828, total_norm: 4.401111602783203, lr: 5.9999999999999995e-05
Step 3685, training loss: 4.180190563201904, total_norm: 3.6002774238586426, lr: 5.9999999999999995e-05
Step 3686, training loss: 3.816600799560547, total_norm: 4.522433280944824, lr: 5.9999999999999995e-05
Step 3687, training loss: 4.426257610321045, total_norm: 3.9024300575256348, lr: 5.9999999999999995e-05
Step 3688, training loss: 3.950821876525879, total_norm: 3.360178232192993, lr: 5.9999999999999995e-05
Step 3689, training loss: 3.920562267303467, total_norm: 3.618368625640869, lr: 5.9999999999999995e-05
Step 3690, training loss: 3.9761152267456055, total_norm: 3.8023879528045654, lr: 5.9999999999999995e-05
Step 3691, training loss: 4.360206604003906, total_norm: 3.7140891551971436, lr: 5.9999999999999995e-05
Step 3692, training loss: 4.7078375816345215, total_norm: 4.658653736114502, lr: 5.9999999999999995e-05
Step 3693, training loss: 3.803537368774414, total_norm: 4.204252243041992, lr: 5.9999999999999995e-05
Step 3694, training loss: 4.0811262130737305, total_norm: 3.2457878589630127, lr: 5.9999999999999995e-05
Step 3695, training loss: 4.214447975158691, total_norm: 3.44448184967041, lr: 5.9999999999999995e-05
Step 3696, training loss: 4.609891414642334, total_norm: 5.010509967803955, lr: 5.9999999999999995e-05
Step 3697, training loss: 4.7298150062561035, total_norm: 3.5394835472106934, lr: 5.9999999999999995e-05
Step 3698, training loss: 4.40507173538208, total_norm: 3.8961546421051025, lr: 5.9999999999999995e-05
Step 3699, training loss: 4.317698955535889, total_norm: 3.445791006088257, lr: 5.9999999999999995e-05
Step 3700, training loss: 4.821282863616943, total_norm: 4.629871845245361, lr: 5.9999999999999995e-05
Step 3700, validation loss: 5.154855251312256
Step 3701, training loss: 4.341984748840332, total_norm: 5.812510013580322, lr: 5.9999999999999995e-05
Step 3702, training loss: 4.002805709838867, total_norm: 5.460109710693359, lr: 5.9999999999999995e-05
Step 3703, training loss: 4.3920111656188965, total_norm: 4.387134552001953, lr: 5.9999999999999995e-05
Step 3704, training loss: 4.632743835449219, total_norm: 3.887357473373413, lr: 5.9999999999999995e-05
Step 3705, training loss: 4.261648178100586, total_norm: 4.364988803863525, lr: 5.9999999999999995e-05
Step 3706, training loss: 4.0904035568237305, total_norm: 4.8132243156433105, lr: 5.9999999999999995e-05
Step 3707, training loss: 3.781773567199707, total_norm: 4.410650730133057, lr: 5.9999999999999995e-05
Step 3708, training loss: 4.5092573165893555, total_norm: 3.552917003631592, lr: 5.9999999999999995e-05
Step 3709, training loss: 4.162981033325195, total_norm: 3.426823377609253, lr: 5.9999999999999995e-05
Step 3710, training loss: 4.206395626068115, total_norm: 3.2288527488708496, lr: 5.9999999999999995e-05
Step 3711, training loss: 3.803649663925171, total_norm: 3.427548885345459, lr: 5.9999999999999995e-05
Step 3712, training loss: 4.317765235900879, total_norm: 3.273021697998047, lr: 5.9999999999999995e-05
Step 3713, training loss: 4.175646781921387, total_norm: 3.495037317276001, lr: 5.9999999999999995e-05
Step 3714, training loss: 4.444568157196045, total_norm: 3.4240541458129883, lr: 5.9999999999999995e-05
Step 3715, training loss: 3.699709177017212, total_norm: 3.777712106704712, lr: 5.9999999999999995e-05
Step 3716, training loss: 4.03370475769043, total_norm: 3.3053641319274902, lr: 5.9999999999999995e-05
Step 3717, training loss: 3.870940923690796, total_norm: 3.980013608932495, lr: 5.9999999999999995e-05
Step 3718, training loss: 5.132548809051514, total_norm: 4.202449321746826, lr: 5.9999999999999995e-05
Step 3719, training loss: 4.3715500831604, total_norm: 4.059103012084961, lr: 5.9999999999999995e-05
Step 3720, training loss: 3.9485199451446533, total_norm: 4.040655612945557, lr: 5.9999999999999995e-05
Step 3721, training loss: 3.9443161487579346, total_norm: 4.001615047454834, lr: 5.9999999999999995e-05
Step 3722, training loss: 4.53558874130249, total_norm: 4.878293037414551, lr: 5.9999999999999995e-05
Step 3723, training loss: 4.183192729949951, total_norm: 3.687964916229248, lr: 5.9999999999999995e-05
Step 3724, training loss: 4.577321529388428, total_norm: 3.7958850860595703, lr: 5.9999999999999995e-05
Step 3725, training loss: 4.054502964019775, total_norm: 3.6929454803466797, lr: 5.9999999999999995e-05
Step 3726, training loss: 4.707040786743164, total_norm: 3.6557793617248535, lr: 5.9999999999999995e-05
Step 3727, training loss: 3.873730182647705, total_norm: 3.7104079723358154, lr: 5.9999999999999995e-05
Step 3728, training loss: 4.358593463897705, total_norm: 4.1400465965271, lr: 5.9999999999999995e-05
Step 3729, training loss: 4.113497257232666, total_norm: 3.490206003189087, lr: 5.9999999999999995e-05
Step 3730, training loss: 3.537976026535034, total_norm: 4.013174533843994, lr: 5.9999999999999995e-05
Step 3731, training loss: 4.472726821899414, total_norm: 3.872001886367798, lr: 5.9999999999999995e-05
Step 3732, training loss: 3.3028128147125244, total_norm: 3.060736894607544, lr: 5.9999999999999995e-05
Step 3733, training loss: 4.308176517486572, total_norm: 3.453378438949585, lr: 5.9999999999999995e-05
Step 3734, training loss: 4.631848335266113, total_norm: 3.883815288543701, lr: 5.9999999999999995e-05
Step 3735, training loss: 3.587313175201416, total_norm: 3.604464054107666, lr: 5.9999999999999995e-05
Step 3736, training loss: 3.205990791320801, total_norm: 3.557082414627075, lr: 5.9999999999999995e-05
Step 3737, training loss: 3.7651426792144775, total_norm: 3.7360572814941406, lr: 5.9999999999999995e-05
Step 3738, training loss: 4.138288974761963, total_norm: 3.95174241065979, lr: 5.9999999999999995e-05
Step 3739, training loss: 4.54890775680542, total_norm: 3.5216073989868164, lr: 5.9999999999999995e-05
Step 3740, training loss: 3.7670648097991943, total_norm: 3.605929374694824, lr: 5.9999999999999995e-05
Step 3741, training loss: 3.311919927597046, total_norm: 3.0318961143493652, lr: 5.9999999999999995e-05
Step 3742, training loss: 3.501567840576172, total_norm: 3.3889243602752686, lr: 5.9999999999999995e-05
Step 3743, training loss: 3.3681318759918213, total_norm: 3.5181822776794434, lr: 5.9999999999999995e-05
Step 3744, training loss: 4.087738513946533, total_norm: 4.061372756958008, lr: 5.9999999999999995e-05
Step 3745, training loss: 3.888442277908325, total_norm: 3.4146082401275635, lr: 5.9999999999999995e-05
Step 3746, training loss: 3.450481653213501, total_norm: 3.098221778869629, lr: 5.9999999999999995e-05
Step 3747, training loss: 4.390259265899658, total_norm: 3.435349702835083, lr: 5.9999999999999995e-05
Step 3748, training loss: 4.101163864135742, total_norm: 3.574587821960449, lr: 5.9999999999999995e-05
Step 3749, training loss: 3.68942928314209, total_norm: 3.10974383354187, lr: 5.9999999999999995e-05
Step 3750, training loss: 3.6081483364105225, total_norm: 3.087223529815674, lr: 5.9999999999999995e-05
Step 3750, validation loss: 5.378538131713867
Step 3751, training loss: 3.7371010780334473, total_norm: 3.2227842807769775, lr: 5.9999999999999995e-05
Step 3752, training loss: 3.5352933406829834, total_norm: 3.425274610519409, lr: 5.9999999999999995e-05
Step 3753, training loss: 4.1817169189453125, total_norm: 3.965955972671509, lr: 5.9999999999999995e-05
Step 3754, training loss: 4.316647529602051, total_norm: 4.010456085205078, lr: 5.9999999999999995e-05
Step 3755, training loss: 3.292583703994751, total_norm: 3.1490237712860107, lr: 5.9999999999999995e-05
Step 3756, training loss: 3.4032695293426514, total_norm: 3.2683372497558594, lr: 5.9999999999999995e-05
Step 3757, training loss: 4.675649642944336, total_norm: 5.198520183563232, lr: 5.9999999999999995e-05
Step 3758, training loss: 4.256506443023682, total_norm: 3.796769142150879, lr: 5.9999999999999995e-05
Step 3759, training loss: 3.3058459758758545, total_norm: 3.8176181316375732, lr: 5.9999999999999995e-05
Step 3760, training loss: 3.4238529205322266, total_norm: 4.0243144035339355, lr: 5.9999999999999995e-05
Step 3761, training loss: 4.826577186584473, total_norm: 4.236685752868652, lr: 5.9999999999999995e-05
Step 3762, training loss: 4.454588413238525, total_norm: 3.5022854804992676, lr: 5.9999999999999995e-05
Step 3763, training loss: 3.5887205600738525, total_norm: 3.9659228324890137, lr: 5.9999999999999995e-05
Step 3764, training loss: 4.38688850402832, total_norm: 4.128656387329102, lr: 5.9999999999999995e-05
Step 3765, training loss: 3.8593719005584717, total_norm: 3.6690008640289307, lr: 5.9999999999999995e-05
Step 3766, training loss: 3.7239651679992676, total_norm: 4.283712387084961, lr: 5.9999999999999995e-05
Step 3767, training loss: 3.5624711513519287, total_norm: 3.5299954414367676, lr: 5.9999999999999995e-05
Step 3768, training loss: 4.020757675170898, total_norm: 3.5103399753570557, lr: 5.9999999999999995e-05
Step 3769, training loss: 3.803941011428833, total_norm: 3.385072708129883, lr: 5.9999999999999995e-05
Step 3770, training loss: 4.287226676940918, total_norm: 3.815270185470581, lr: 5.9999999999999995e-05
Step 3771, training loss: 4.181591987609863, total_norm: 3.573798418045044, lr: 5.9999999999999995e-05
Step 3772, training loss: 3.8776867389678955, total_norm: 3.1823294162750244, lr: 5.9999999999999995e-05
Step 3773, training loss: 3.437962532043457, total_norm: 3.2173314094543457, lr: 5.9999999999999995e-05
Step 3774, training loss: 4.503325939178467, total_norm: 4.504207134246826, lr: 5.9999999999999995e-05
Step 3775, training loss: 4.58013391494751, total_norm: 3.8627054691314697, lr: 5.9999999999999995e-05
Step 3776, training loss: 4.0711588859558105, total_norm: 3.319925546646118, lr: 5.9999999999999995e-05
Step 3777, training loss: 4.58437967300415, total_norm: 3.747459650039673, lr: 5.9999999999999995e-05
Step 3778, training loss: 4.033078670501709, total_norm: 3.574100971221924, lr: 5.9999999999999995e-05
Step 3779, training loss: 4.592545032501221, total_norm: 3.7382612228393555, lr: 5.9999999999999995e-05
Step 3780, training loss: 4.699830055236816, total_norm: 3.688778877258301, lr: 5.9999999999999995e-05
Step 3781, training loss: 3.8219215869903564, total_norm: 3.5439603328704834, lr: 5.9999999999999995e-05
Step 3782, training loss: 4.394268989562988, total_norm: 3.7858223915100098, lr: 5.9999999999999995e-05
Step 3783, training loss: 4.297503471374512, total_norm: 3.780435085296631, lr: 5.9999999999999995e-05
Step 3784, training loss: 4.167967796325684, total_norm: 3.9576127529144287, lr: 5.9999999999999995e-05
Step 3785, training loss: 4.143271446228027, total_norm: 3.7579410076141357, lr: 5.9999999999999995e-05
Step 3786, training loss: 3.8625733852386475, total_norm: 3.906980514526367, lr: 5.9999999999999995e-05
Step 3787, training loss: 4.118085861206055, total_norm: 4.004103660583496, lr: 5.9999999999999995e-05
Step 3788, training loss: 5.187917709350586, total_norm: 4.70721960067749, lr: 5.9999999999999995e-05
Step 3789, training loss: 4.399599552154541, total_norm: 4.250380992889404, lr: 5.9999999999999995e-05
Step 3790, training loss: 4.293930530548096, total_norm: 6.289975166320801, lr: 5.9999999999999995e-05
Step 3791, training loss: 4.087117671966553, total_norm: 4.475239276885986, lr: 5.9999999999999995e-05
Step 3792, training loss: 4.710072994232178, total_norm: 5.049222946166992, lr: 5.9999999999999995e-05
Step 3793, training loss: 4.450212001800537, total_norm: 4.464967727661133, lr: 5.9999999999999995e-05
Step 3794, training loss: 3.6383776664733887, total_norm: 3.873471975326538, lr: 5.9999999999999995e-05
Step 3795, training loss: 3.420959234237671, total_norm: 3.475144863128662, lr: 5.9999999999999995e-05
Step 3796, training loss: 3.6353094577789307, total_norm: 3.6688146591186523, lr: 5.9999999999999995e-05
Step 3797, training loss: 3.4746789932250977, total_norm: 3.5571999549865723, lr: 5.9999999999999995e-05
Step 3798, training loss: 4.0062642097473145, total_norm: 3.517258644104004, lr: 5.9999999999999995e-05
Step 3799, training loss: 4.265983581542969, total_norm: 3.6855082511901855, lr: 5.9999999999999995e-05
Step 3800, training loss: 4.074330806732178, total_norm: 3.805173635482788, lr: 5.9999999999999995e-05
Step 3800, validation loss: 5.38224458694458
Step 3801, training loss: 4.378074645996094, total_norm: 3.527831554412842, lr: 5.9999999999999995e-05
Step 3802, training loss: 3.8964896202087402, total_norm: 3.212001323699951, lr: 5.9999999999999995e-05
Step 3803, training loss: 3.7620511054992676, total_norm: 3.0447065830230713, lr: 5.9999999999999995e-05
Step 3804, training loss: 4.0343828201293945, total_norm: 3.359778881072998, lr: 5.9999999999999995e-05
Step 3805, training loss: 4.068592071533203, total_norm: 3.5378150939941406, lr: 5.9999999999999995e-05
Step 3806, training loss: 3.695491313934326, total_norm: 3.2665774822235107, lr: 5.9999999999999995e-05
Step 3807, training loss: 3.8739778995513916, total_norm: 3.2542829513549805, lr: 5.9999999999999995e-05
Step 3808, training loss: 3.8269972801208496, total_norm: 3.2516462802886963, lr: 5.9999999999999995e-05
Step 3809, training loss: 4.123698711395264, total_norm: 3.8785548210144043, lr: 5.9999999999999995e-05
Step 3810, training loss: 4.755456447601318, total_norm: 4.403655529022217, lr: 5.9999999999999995e-05
Step 3811, training loss: 3.9848127365112305, total_norm: 3.3894433975219727, lr: 5.9999999999999995e-05
Step 3812, training loss: 4.116411209106445, total_norm: 4.484707355499268, lr: 5.9999999999999995e-05
Step 3813, training loss: 3.6061322689056396, total_norm: 3.6784276962280273, lr: 5.9999999999999995e-05
Step 3814, training loss: 4.107995510101318, total_norm: 3.3823771476745605, lr: 5.9999999999999995e-05
Step 3815, training loss: 3.63818097114563, total_norm: 3.805852174758911, lr: 5.9999999999999995e-05
Step 3816, training loss: 3.9123802185058594, total_norm: 3.275479555130005, lr: 5.9999999999999995e-05
Step 3817, training loss: 4.050380706787109, total_norm: 4.154677867889404, lr: 5.9999999999999995e-05
Step 3818, training loss: 3.9216573238372803, total_norm: 3.344207763671875, lr: 5.9999999999999995e-05
Step 3819, training loss: 4.320728778839111, total_norm: 3.5470376014709473, lr: 5.9999999999999995e-05
Step 3820, training loss: 4.006714344024658, total_norm: 3.428797960281372, lr: 5.9999999999999995e-05
Step 3821, training loss: 4.116054058074951, total_norm: 3.5957391262054443, lr: 5.9999999999999995e-05
Step 3822, training loss: 4.246459007263184, total_norm: 3.680529832839966, lr: 5.9999999999999995e-05
Step 3823, training loss: 4.2134108543396, total_norm: 3.729792833328247, lr: 5.9999999999999995e-05
Step 3824, training loss: 3.916456937789917, total_norm: 4.254510402679443, lr: 5.9999999999999995e-05
Step 3825, training loss: 4.196183681488037, total_norm: 4.184720993041992, lr: 5.9999999999999995e-05
Step 3826, training loss: 3.552927017211914, total_norm: 3.717818260192871, lr: 5.9999999999999995e-05
Step 3827, training loss: 3.6145451068878174, total_norm: 4.512392997741699, lr: 5.9999999999999995e-05
Step 3828, training loss: 3.802759885787964, total_norm: 3.883754014968872, lr: 5.9999999999999995e-05
Step 3829, training loss: 4.0466227531433105, total_norm: 3.5818097591400146, lr: 5.9999999999999995e-05
Step 3830, training loss: 3.24798583984375, total_norm: 3.3935673236846924, lr: 5.9999999999999995e-05
Step 3831, training loss: 3.250903367996216, total_norm: 3.1261420249938965, lr: 5.9999999999999995e-05
Step 3832, training loss: 3.796750783920288, total_norm: 3.8563904762268066, lr: 5.9999999999999995e-05
Step 3833, training loss: 3.568756341934204, total_norm: 3.7256839275360107, lr: 5.9999999999999995e-05
Step 3834, training loss: 3.9853389263153076, total_norm: 3.811117649078369, lr: 5.9999999999999995e-05
Step 3835, training loss: 3.8055319786071777, total_norm: 3.374453544616699, lr: 5.9999999999999995e-05
Step 3836, training loss: 3.613304615020752, total_norm: 3.255678176879883, lr: 5.9999999999999995e-05
Step 3837, training loss: 3.876767873764038, total_norm: 3.2037723064422607, lr: 5.9999999999999995e-05
Step 3838, training loss: 3.4062283039093018, total_norm: 3.38712215423584, lr: 5.9999999999999995e-05
Step 3839, training loss: 4.03762674331665, total_norm: 3.4789936542510986, lr: 5.9999999999999995e-05
Step 3840, training loss: 4.17256498336792, total_norm: 3.718273162841797, lr: 5.9999999999999995e-05
Step 3841, training loss: 3.959118127822876, total_norm: 3.2575910091400146, lr: 5.9999999999999995e-05
Step 3842, training loss: 4.3503851890563965, total_norm: 3.709683418273926, lr: 5.9999999999999995e-05
Step 3843, training loss: 4.420132160186768, total_norm: 4.02617883682251, lr: 5.9999999999999995e-05
Step 3844, training loss: 4.284078121185303, total_norm: 3.5597593784332275, lr: 5.9999999999999995e-05
Step 3845, training loss: 3.920023202896118, total_norm: 3.411572217941284, lr: 5.9999999999999995e-05
Step 3846, training loss: 4.224182605743408, total_norm: 3.428814649581909, lr: 5.9999999999999995e-05
Step 3847, training loss: 4.536611080169678, total_norm: 3.6932482719421387, lr: 5.9999999999999995e-05
Step 3848, training loss: 4.196192741394043, total_norm: 3.549811601638794, lr: 5.9999999999999995e-05
Step 3849, training loss: 3.5857770442962646, total_norm: 3.380272626876831, lr: 5.9999999999999995e-05
Step 3850, training loss: 3.6547656059265137, total_norm: 3.383707284927368, lr: 5.9999999999999995e-05
Step 3850, validation loss: 5.467372894287109
Step 3851, training loss: 4.219456195831299, total_norm: 3.5095484256744385, lr: 5.9999999999999995e-05
Step 3852, training loss: 3.6778717041015625, total_norm: 4.196847438812256, lr: 5.9999999999999995e-05
Step 3853, training loss: 4.066204071044922, total_norm: 4.91939640045166, lr: 5.9999999999999995e-05
Step 3854, training loss: 3.767831563949585, total_norm: 4.508877277374268, lr: 5.9999999999999995e-05
Step 3855, training loss: 3.332918167114258, total_norm: 4.721045970916748, lr: 5.9999999999999995e-05
Step 3856, training loss: 4.476492881774902, total_norm: 4.134233474731445, lr: 5.9999999999999995e-05
Step 3857, training loss: 4.332969665527344, total_norm: 4.251894950866699, lr: 5.9999999999999995e-05
Step 3858, training loss: 4.095556735992432, total_norm: 4.241911888122559, lr: 5.9999999999999995e-05
Step 3859, training loss: 4.468245506286621, total_norm: 5.376704692840576, lr: 5.9999999999999995e-05
Step 3860, training loss: 4.281082630157471, total_norm: 4.471355438232422, lr: 5.9999999999999995e-05
Step 3861, training loss: 3.5720343589782715, total_norm: 6.305169582366943, lr: 5.9999999999999995e-05
Step 3862, training loss: 3.8271408081054688, total_norm: 4.371248722076416, lr: 5.9999999999999995e-05
Step 3863, training loss: 3.679079055786133, total_norm: 3.7499747276306152, lr: 5.9999999999999995e-05
Step 3864, training loss: 3.265835762023926, total_norm: 3.15998911857605, lr: 5.9999999999999995e-05
Step 3865, training loss: 3.593757390975952, total_norm: 3.2970638275146484, lr: 5.9999999999999995e-05
Step 3866, training loss: 4.666390895843506, total_norm: 4.017579078674316, lr: 5.9999999999999995e-05
Step 3867, training loss: 2.8237545490264893, total_norm: 3.3116087913513184, lr: 5.9999999999999995e-05
Step 3868, training loss: 3.63040828704834, total_norm: 3.8322060108184814, lr: 5.9999999999999995e-05
Step 3869, training loss: 3.7406082153320312, total_norm: 3.7635152339935303, lr: 5.9999999999999995e-05
Step 3870, training loss: 3.48063063621521, total_norm: 3.6675045490264893, lr: 5.9999999999999995e-05
Step 3871, training loss: 3.7540552616119385, total_norm: 3.537775993347168, lr: 5.9999999999999995e-05
Step 3872, training loss: 4.3782877922058105, total_norm: 4.332085609436035, lr: 5.9999999999999995e-05
Step 3873, training loss: 4.618698596954346, total_norm: 4.259209156036377, lr: 5.9999999999999995e-05
Step 3874, training loss: 4.291090488433838, total_norm: 4.232594966888428, lr: 5.9999999999999995e-05
Step 3875, training loss: 4.316451549530029, total_norm: 4.966325283050537, lr: 5.9999999999999995e-05
Step 3876, training loss: 3.6848957538604736, total_norm: 5.680435657501221, lr: 5.9999999999999995e-05
Step 3877, training loss: 4.584935188293457, total_norm: 5.364981174468994, lr: 5.9999999999999995e-05
Step 3878, training loss: 4.225775718688965, total_norm: 4.109437465667725, lr: 5.9999999999999995e-05
Step 3879, training loss: 4.324870586395264, total_norm: 3.719263792037964, lr: 5.9999999999999995e-05
Step 3880, training loss: 3.8455827236175537, total_norm: 3.725144147872925, lr: 5.9999999999999995e-05
Step 3881, training loss: 4.009519577026367, total_norm: 3.7284810543060303, lr: 5.9999999999999995e-05
Step 3882, training loss: 4.510966777801514, total_norm: 4.817319869995117, lr: 5.9999999999999995e-05
Step 3883, training loss: 4.413844108581543, total_norm: 6.874040603637695, lr: 5.9999999999999995e-05
Step 3884, training loss: 4.534564018249512, total_norm: 4.649360656738281, lr: 5.9999999999999995e-05
Step 3885, training loss: 4.440291404724121, total_norm: 4.740166664123535, lr: 5.9999999999999995e-05
Step 3886, training loss: 4.469733715057373, total_norm: 3.9125242233276367, lr: 5.9999999999999995e-05
Step 3887, training loss: 4.561250686645508, total_norm: 4.0660176277160645, lr: 5.9999999999999995e-05
Step 3888, training loss: 4.631290435791016, total_norm: 4.29966926574707, lr: 5.9999999999999995e-05
Step 3889, training loss: 4.42673397064209, total_norm: 4.706130504608154, lr: 5.9999999999999995e-05
Step 3890, training loss: 4.285961627960205, total_norm: 6.1793646812438965, lr: 5.9999999999999995e-05
Step 3891, training loss: 4.222259998321533, total_norm: 4.511240005493164, lr: 5.9999999999999995e-05
Step 3892, training loss: 4.477235794067383, total_norm: 5.676175594329834, lr: 5.9999999999999995e-05
Step 3893, training loss: 4.731993198394775, total_norm: 4.032406806945801, lr: 5.9999999999999995e-05
Step 3894, training loss: 4.159882068634033, total_norm: 3.834907293319702, lr: 5.9999999999999995e-05
Step 3895, training loss: 4.226614475250244, total_norm: 4.479320049285889, lr: 5.9999999999999995e-05
Step 3896, training loss: 3.6446123123168945, total_norm: 5.242098808288574, lr: 5.9999999999999995e-05
Step 3897, training loss: 4.170895099639893, total_norm: 5.027011394500732, lr: 5.9999999999999995e-05
Step 3898, training loss: 4.555501461029053, total_norm: 4.701735019683838, lr: 5.9999999999999995e-05
Step 3899, training loss: 4.477520942687988, total_norm: 3.925157308578491, lr: 5.9999999999999995e-05
Step 3900, training loss: 4.969987869262695, total_norm: 4.203736305236816, lr: 5.9999999999999995e-05
Step 3900, validation loss: 5.604065895080566
Step 3901, training loss: 4.225106716156006, total_norm: 4.765435695648193, lr: 5.9999999999999995e-05
Step 3902, training loss: 4.386266231536865, total_norm: 5.458070755004883, lr: 5.9999999999999995e-05
Step 3903, training loss: 4.227462291717529, total_norm: 4.6956658363342285, lr: 5.9999999999999995e-05
Step 3904, training loss: 4.321574687957764, total_norm: 5.394583225250244, lr: 5.9999999999999995e-05
Step 3905, training loss: 4.287992000579834, total_norm: 6.338204383850098, lr: 5.9999999999999995e-05
Step 3906, training loss: 4.590949535369873, total_norm: 4.003562927246094, lr: 5.9999999999999995e-05
Step 3907, training loss: 4.560756206512451, total_norm: 3.6517553329467773, lr: 5.9999999999999995e-05
Step 3908, training loss: 4.161818027496338, total_norm: 4.73084020614624, lr: 5.9999999999999995e-05
Step 3909, training loss: 4.2015838623046875, total_norm: 3.9949779510498047, lr: 5.9999999999999995e-05
Step 3910, training loss: 4.26599645614624, total_norm: 5.822080612182617, lr: 5.9999999999999995e-05
Step 3911, training loss: 3.885164260864258, total_norm: 4.352212905883789, lr: 5.9999999999999995e-05
Step 3912, training loss: 3.900099515914917, total_norm: 4.140768051147461, lr: 5.9999999999999995e-05
Step 3913, training loss: 3.997978687286377, total_norm: 3.9995908737182617, lr: 5.9999999999999995e-05
Step 3914, training loss: 4.194228649139404, total_norm: 4.2211713790893555, lr: 5.9999999999999995e-05
Step 3915, training loss: 4.4483208656311035, total_norm: 5.7550482749938965, lr: 5.9999999999999995e-05
Step 3916, training loss: 4.147543430328369, total_norm: 4.370967388153076, lr: 5.9999999999999995e-05
Step 3917, training loss: 4.4307427406311035, total_norm: 3.991715908050537, lr: 5.9999999999999995e-05
Step 3918, training loss: 4.543224811553955, total_norm: 4.5081377029418945, lr: 5.9999999999999995e-05
Step 3919, training loss: 4.588901519775391, total_norm: 5.30451774597168, lr: 5.9999999999999995e-05
Step 3920, training loss: 4.466522216796875, total_norm: 6.284780502319336, lr: 5.9999999999999995e-05
Step 3921, training loss: 4.687147617340088, total_norm: 5.005679607391357, lr: 5.9999999999999995e-05
Step 3922, training loss: 3.834517478942871, total_norm: 3.457322120666504, lr: 5.9999999999999995e-05
Step 3923, training loss: 4.082816123962402, total_norm: 4.173540115356445, lr: 5.9999999999999995e-05
Step 3924, training loss: 4.67728328704834, total_norm: 4.436115264892578, lr: 5.9999999999999995e-05
Step 3925, training loss: 4.160725116729736, total_norm: 3.661630153656006, lr: 5.9999999999999995e-05
Step 3926, training loss: 4.4301042556762695, total_norm: 4.197953224182129, lr: 5.9999999999999995e-05
Step 3927, training loss: 3.429701805114746, total_norm: 3.685641288757324, lr: 5.9999999999999995e-05
Step 3928, training loss: 4.3719801902771, total_norm: 4.3644304275512695, lr: 5.9999999999999995e-05
Step 3929, training loss: 4.611106872558594, total_norm: 5.406154155731201, lr: 5.9999999999999995e-05
Step 3930, training loss: 4.1319475173950195, total_norm: 4.561710834503174, lr: 5.9999999999999995e-05
Step 3931, training loss: 3.647387981414795, total_norm: 4.546029567718506, lr: 5.9999999999999995e-05
Step 3932, training loss: 4.13297176361084, total_norm: 5.435338497161865, lr: 5.9999999999999995e-05
Step 3933, training loss: 4.173762798309326, total_norm: 4.445169925689697, lr: 5.9999999999999995e-05
Step 3934, training loss: 4.27411413192749, total_norm: 3.626723527908325, lr: 5.9999999999999995e-05
Step 3935, training loss: 4.03510856628418, total_norm: 3.6042935848236084, lr: 5.9999999999999995e-05
Step 3936, training loss: 4.349825859069824, total_norm: 4.1114983558654785, lr: 5.9999999999999995e-05
Step 3937, training loss: 3.8159596920013428, total_norm: 3.869861125946045, lr: 5.9999999999999995e-05
Step 3938, training loss: 3.462523937225342, total_norm: 4.185237407684326, lr: 5.9999999999999995e-05
Step 3939, training loss: 4.319155693054199, total_norm: 4.663758277893066, lr: 5.9999999999999995e-05
Step 3940, training loss: 4.415133476257324, total_norm: 4.506367206573486, lr: 5.9999999999999995e-05
Step 3941, training loss: 3.905653476715088, total_norm: 3.7364816665649414, lr: 5.9999999999999995e-05
Step 3942, training loss: 4.354236125946045, total_norm: 3.923691987991333, lr: 5.9999999999999995e-05
Step 3943, training loss: 3.148998737335205, total_norm: 3.8711180686950684, lr: 5.9999999999999995e-05
Step 3944, training loss: 2.9749107360839844, total_norm: 3.5552926063537598, lr: 5.9999999999999995e-05
Step 3945, training loss: 3.65779447555542, total_norm: 3.8159589767456055, lr: 5.9999999999999995e-05
Step 3946, training loss: 3.0275731086730957, total_norm: 3.389660358428955, lr: 5.9999999999999995e-05
Step 3947, training loss: 4.11425256729126, total_norm: 4.41815710067749, lr: 5.9999999999999995e-05
Step 3948, training loss: 3.7589073181152344, total_norm: 4.022451400756836, lr: 5.9999999999999995e-05
Step 3949, training loss: 3.7385737895965576, total_norm: 3.647439479827881, lr: 5.9999999999999995e-05
Step 3950, training loss: 4.771213054656982, total_norm: 4.664833068847656, lr: 5.9999999999999995e-05
Step 3950, validation loss: 5.571162223815918
Step 3951, training loss: 4.777482509613037, total_norm: 3.7325241565704346, lr: 5.9999999999999995e-05
Step 3952, training loss: 4.193770885467529, total_norm: 4.746064186096191, lr: 5.9999999999999995e-05
Step 3953, training loss: 4.245319843292236, total_norm: 5.817014217376709, lr: 5.9999999999999995e-05
Step 3954, training loss: 4.531827926635742, total_norm: 8.360030174255371, lr: 5.9999999999999995e-05
Step 3955, training loss: 4.010386943817139, total_norm: 10.286876678466797, lr: 5.9999999999999995e-05
Step 3956, training loss: 4.390642166137695, total_norm: 5.415439128875732, lr: 5.9999999999999995e-05
Step 3957, training loss: 4.784893035888672, total_norm: 4.430810451507568, lr: 5.9999999999999995e-05
Step 3958, training loss: 4.092857360839844, total_norm: 4.539745330810547, lr: 5.9999999999999995e-05
Step 3959, training loss: 4.120662212371826, total_norm: 3.871441125869751, lr: 5.9999999999999995e-05
Step 3960, training loss: 4.380259990692139, total_norm: 5.043442249298096, lr: 5.9999999999999995e-05
Step 3961, training loss: 5.012814521789551, total_norm: 4.363458633422852, lr: 5.9999999999999995e-05
Step 3962, training loss: 4.230696201324463, total_norm: 5.624890327453613, lr: 5.9999999999999995e-05
Step 3963, training loss: 4.003671169281006, total_norm: 4.045721054077148, lr: 5.9999999999999995e-05
Step 3964, training loss: 4.577386379241943, total_norm: 4.279480934143066, lr: 5.9999999999999995e-05
Step 3965, training loss: 4.110068321228027, total_norm: 4.008155822753906, lr: 5.9999999999999995e-05
Step 3966, training loss: 4.084883689880371, total_norm: 3.518068790435791, lr: 5.9999999999999995e-05
Step 3967, training loss: 3.9226295948028564, total_norm: 4.084290981292725, lr: 5.9999999999999995e-05
Step 3968, training loss: 5.293671131134033, total_norm: 4.599748134613037, lr: 5.9999999999999995e-05
Step 3969, training loss: 4.607400894165039, total_norm: 4.264009475708008, lr: 5.9999999999999995e-05
Step 3970, training loss: 4.167102813720703, total_norm: 3.537424325942993, lr: 5.9999999999999995e-05
Step 3971, training loss: 4.0183491706848145, total_norm: 3.742309808731079, lr: 5.9999999999999995e-05
Step 3972, training loss: 3.683703660964966, total_norm: 3.4303345680236816, lr: 5.9999999999999995e-05
Step 3973, training loss: 3.881787061691284, total_norm: 3.6219639778137207, lr: 5.9999999999999995e-05
Step 3974, training loss: 4.562562942504883, total_norm: 4.419103145599365, lr: 5.9999999999999995e-05
Step 3975, training loss: 4.179946422576904, total_norm: 3.7020606994628906, lr: 5.9999999999999995e-05
Step 3976, training loss: 3.898045778274536, total_norm: 3.5624704360961914, lr: 5.9999999999999995e-05
Step 3977, training loss: 4.0319061279296875, total_norm: 3.935687780380249, lr: 5.9999999999999995e-05
Step 3978, training loss: 3.6410481929779053, total_norm: 3.293590784072876, lr: 5.9999999999999995e-05
Step 3979, training loss: 3.836062431335449, total_norm: 3.1289453506469727, lr: 5.9999999999999995e-05
Step 3980, training loss: 5.278539657592773, total_norm: 4.56178092956543, lr: 5.9999999999999995e-05
Step 3981, training loss: 4.275727272033691, total_norm: 4.088489055633545, lr: 5.9999999999999995e-05
Step 3982, training loss: 3.720351219177246, total_norm: 4.011297225952148, lr: 5.9999999999999995e-05
Step 3983, training loss: 5.075074195861816, total_norm: 5.237768173217773, lr: 5.9999999999999995e-05
Step 3984, training loss: 3.97688364982605, total_norm: 3.905113935470581, lr: 5.9999999999999995e-05
Step 3985, training loss: 3.7609214782714844, total_norm: 3.9159698486328125, lr: 5.9999999999999995e-05
Step 3986, training loss: 4.167037010192871, total_norm: 4.18879508972168, lr: 5.9999999999999995e-05
Step 3987, training loss: 3.915308713912964, total_norm: 3.4781606197357178, lr: 5.9999999999999995e-05
Step 3988, training loss: 4.472250938415527, total_norm: 3.662588596343994, lr: 5.9999999999999995e-05
Step 3989, training loss: 3.9013757705688477, total_norm: 3.2901923656463623, lr: 5.9999999999999995e-05
Step 3990, training loss: 4.3538618087768555, total_norm: 3.3542354106903076, lr: 5.9999999999999995e-05
Step 3991, training loss: 4.047410011291504, total_norm: 3.83455753326416, lr: 5.9999999999999995e-05
Step 3992, training loss: 4.529001712799072, total_norm: 4.231927871704102, lr: 5.9999999999999995e-05
Step 3993, training loss: 3.944633722305298, total_norm: 3.499021530151367, lr: 5.9999999999999995e-05
Step 3994, training loss: 4.106231689453125, total_norm: 4.1564741134643555, lr: 5.9999999999999995e-05
Step 3995, training loss: 3.7205870151519775, total_norm: 4.433002471923828, lr: 5.9999999999999995e-05
Step 3996, training loss: 4.729437828063965, total_norm: 4.792860507965088, lr: 5.9999999999999995e-05
Step 3997, training loss: 4.572185516357422, total_norm: 4.306496620178223, lr: 5.9999999999999995e-05
Step 3998, training loss: 4.09082555770874, total_norm: 3.663501262664795, lr: 5.9999999999999995e-05
Step 3999, training loss: 4.552269458770752, total_norm: 4.17329740524292, lr: 5.9999999999999995e-05
Step 4000, training loss: 4.16730260848999, total_norm: 4.460384845733643, lr: 5.9999999999999995e-05
Step 4000, validation loss: 5.609886646270752
Step 4001, training loss: 4.343456745147705, total_norm: 4.981183052062988, lr: 5.9999999999999995e-05
Step 4002, training loss: 4.066997051239014, total_norm: 4.794853210449219, lr: 5.9999999999999995e-05
Step 4003, training loss: 3.9559178352355957, total_norm: 4.361028671264648, lr: 5.9999999999999995e-05
Step 4004, training loss: 4.571901798248291, total_norm: 4.567257404327393, lr: 5.9999999999999995e-05
Step 4005, training loss: 3.8980536460876465, total_norm: 4.038799285888672, lr: 5.9999999999999995e-05
Step 4006, training loss: 4.116597652435303, total_norm: 3.701624870300293, lr: 5.9999999999999995e-05
Step 4007, training loss: 3.9145092964172363, total_norm: 3.9979238510131836, lr: 5.9999999999999995e-05
Step 4008, training loss: 3.280205011367798, total_norm: 3.301313877105713, lr: 5.9999999999999995e-05
Step 4009, training loss: 3.7512452602386475, total_norm: 3.8135786056518555, lr: 5.9999999999999995e-05
Step 4010, training loss: 4.120197296142578, total_norm: 3.762098789215088, lr: 5.9999999999999995e-05
Step 4011, training loss: 4.154520034790039, total_norm: 4.387631893157959, lr: 5.9999999999999995e-05
Step 4012, training loss: 3.684778928756714, total_norm: 3.5452916622161865, lr: 5.9999999999999995e-05
Step 4013, training loss: 3.6338231563568115, total_norm: 4.713224411010742, lr: 5.9999999999999995e-05
Step 4014, training loss: 3.650899887084961, total_norm: 3.444413423538208, lr: 5.9999999999999995e-05
Step 4015, training loss: 4.697336673736572, total_norm: 3.893974542617798, lr: 5.9999999999999995e-05
Step 4016, training loss: 3.9439969062805176, total_norm: 3.7812421321868896, lr: 5.9999999999999995e-05
Step 4017, training loss: 3.7146944999694824, total_norm: 4.16888427734375, lr: 5.9999999999999995e-05
Step 4018, training loss: 4.14712381362915, total_norm: 4.107313632965088, lr: 5.9999999999999995e-05
Step 4019, training loss: 3.9985344409942627, total_norm: 5.027254104614258, lr: 5.9999999999999995e-05
Step 4020, training loss: 3.8135874271392822, total_norm: 4.6661810874938965, lr: 5.9999999999999995e-05
Step 4021, training loss: 4.044801712036133, total_norm: 4.044472694396973, lr: 5.9999999999999995e-05
Step 4022, training loss: 4.352386474609375, total_norm: 3.650125741958618, lr: 5.9999999999999995e-05
Step 4023, training loss: 4.531620502471924, total_norm: 6.931166648864746, lr: 5.9999999999999995e-05
Step 4024, training loss: 4.360839366912842, total_norm: 4.898320198059082, lr: 5.9999999999999995e-05
Step 4025, training loss: 4.903283596038818, total_norm: 5.166759014129639, lr: 5.9999999999999995e-05
Step 4026, training loss: 4.098804473876953, total_norm: 4.173786640167236, lr: 5.9999999999999995e-05
Step 4027, training loss: 4.555487155914307, total_norm: 3.9625415802001953, lr: 5.9999999999999995e-05
Step 4028, training loss: 4.08505392074585, total_norm: 4.215427875518799, lr: 5.9999999999999995e-05
Step 4029, training loss: 4.682039737701416, total_norm: 4.291922569274902, lr: 5.9999999999999995e-05
Step 4030, training loss: 3.6212053298950195, total_norm: 4.142611980438232, lr: 5.9999999999999995e-05
Step 4031, training loss: 4.047421932220459, total_norm: 3.7283685207366943, lr: 5.9999999999999995e-05
Step 4032, training loss: 3.886298179626465, total_norm: 3.6192328929901123, lr: 5.9999999999999995e-05
Step 4033, training loss: 4.457467079162598, total_norm: 3.9020020961761475, lr: 5.9999999999999995e-05
Step 4034, training loss: 4.0078301429748535, total_norm: 3.97637677192688, lr: 5.9999999999999995e-05
Step 4035, training loss: 4.303524017333984, total_norm: 5.177109718322754, lr: 5.9999999999999995e-05
Step 4036, training loss: 4.1816325187683105, total_norm: 6.610672473907471, lr: 5.9999999999999995e-05
Step 4037, training loss: 3.818899393081665, total_norm: 5.487390518188477, lr: 5.9999999999999995e-05
Step 4038, training loss: 3.796354293823242, total_norm: 3.938271999359131, lr: 5.9999999999999995e-05
Step 4039, training loss: 3.6704776287078857, total_norm: 3.88242506980896, lr: 5.9999999999999995e-05
Step 4040, training loss: 3.592121124267578, total_norm: 4.732389450073242, lr: 5.9999999999999995e-05
Step 4041, training loss: 3.7936980724334717, total_norm: 3.73703932762146, lr: 5.9999999999999995e-05
Step 4042, training loss: 3.5798847675323486, total_norm: 3.5618083477020264, lr: 5.9999999999999995e-05
Step 4043, training loss: 4.06435489654541, total_norm: 4.523556709289551, lr: 5.9999999999999995e-05
Step 4044, training loss: 4.003640651702881, total_norm: 4.030445575714111, lr: 5.9999999999999995e-05
Step 4045, training loss: 3.902305841445923, total_norm: 3.7385494709014893, lr: 5.9999999999999995e-05
Step 4046, training loss: 4.600839138031006, total_norm: 4.155957221984863, lr: 5.9999999999999995e-05
Step 4047, training loss: 4.198703765869141, total_norm: 3.5046756267547607, lr: 5.9999999999999995e-05
Step 4048, training loss: 4.240846157073975, total_norm: 3.708281993865967, lr: 5.9999999999999995e-05
Step 4049, training loss: 4.569563388824463, total_norm: 4.459651470184326, lr: 5.9999999999999995e-05
Step 4050, training loss: 4.017563819885254, total_norm: 4.298868179321289, lr: 5.9999999999999995e-05
Step 4050, validation loss: 5.614254951477051
Step 4051, training loss: 4.5512847900390625, total_norm: 4.1604132652282715, lr: 5.9999999999999995e-05
Step 4052, training loss: 4.458892345428467, total_norm: 4.178961753845215, lr: 5.9999999999999995e-05
Step 4053, training loss: 4.418251037597656, total_norm: 3.7778055667877197, lr: 5.9999999999999995e-05
Step 4054, training loss: 4.260349273681641, total_norm: 4.817042827606201, lr: 5.9999999999999995e-05
Step 4055, training loss: 4.269692420959473, total_norm: 4.361325740814209, lr: 5.9999999999999995e-05
Step 4056, training loss: 4.417913913726807, total_norm: 3.9571306705474854, lr: 5.9999999999999995e-05
Step 4057, training loss: 3.815887928009033, total_norm: 3.862917184829712, lr: 5.9999999999999995e-05
Step 4058, training loss: 3.4623167514801025, total_norm: 3.4088077545166016, lr: 5.9999999999999995e-05
Step 4059, training loss: 3.711613416671753, total_norm: 3.880958080291748, lr: 5.9999999999999995e-05
Step 4060, training loss: 4.264557361602783, total_norm: 4.527188777923584, lr: 5.9999999999999995e-05
Step 4061, training loss: 4.254609107971191, total_norm: 3.9019527435302734, lr: 5.9999999999999995e-05
Step 4062, training loss: 4.286142349243164, total_norm: 4.403231620788574, lr: 5.9999999999999995e-05
Step 4063, training loss: 4.836431980133057, total_norm: 5.442435264587402, lr: 5.9999999999999995e-05
Step 4064, training loss: 4.310568809509277, total_norm: 4.9304046630859375, lr: 5.9999999999999995e-05
Step 4065, training loss: 3.9816606044769287, total_norm: 5.694808006286621, lr: 5.9999999999999995e-05
Step 4066, training loss: 4.461880207061768, total_norm: 4.170402526855469, lr: 5.9999999999999995e-05
Step 4067, training loss: 4.030022621154785, total_norm: 4.930149078369141, lr: 5.9999999999999995e-05
Step 4068, training loss: 4.229743957519531, total_norm: 5.163440227508545, lr: 5.9999999999999995e-05
Step 4069, training loss: 4.182513236999512, total_norm: 3.8634681701660156, lr: 5.9999999999999995e-05
Step 4070, training loss: 3.682260274887085, total_norm: 3.7122397422790527, lr: 5.9999999999999995e-05
Step 4071, training loss: 3.5894155502319336, total_norm: 3.7368106842041016, lr: 5.9999999999999995e-05
Step 4072, training loss: 2.807185411453247, total_norm: 5.249375343322754, lr: 5.9999999999999995e-05
Step 4073, training loss: 3.1714959144592285, total_norm: 3.9293057918548584, lr: 5.9999999999999995e-05
Step 4074, training loss: 3.344717502593994, total_norm: 3.611798048019409, lr: 5.9999999999999995e-05
Step 4075, training loss: 4.607940673828125, total_norm: 4.535575866699219, lr: 5.9999999999999995e-05
Step 4076, training loss: 4.61653995513916, total_norm: 5.340763092041016, lr: 5.9999999999999995e-05
Step 4077, training loss: 4.039783477783203, total_norm: 5.19990873336792, lr: 5.9999999999999995e-05
Step 4078, training loss: 4.488392353057861, total_norm: 4.90097713470459, lr: 5.9999999999999995e-05
Step 4079, training loss: 4.122861862182617, total_norm: 4.4458327293396, lr: 5.9999999999999995e-05
Step 4080, training loss: 4.023141860961914, total_norm: 4.75492525100708, lr: 5.9999999999999995e-05
Step 4081, training loss: 4.275108814239502, total_norm: 4.23593282699585, lr: 5.9999999999999995e-05
Step 4082, training loss: 3.7848360538482666, total_norm: 3.9664535522460938, lr: 5.9999999999999995e-05
Step 4083, training loss: 3.7647290229797363, total_norm: 4.219779968261719, lr: 5.9999999999999995e-05
Step 4084, training loss: 4.042898654937744, total_norm: 4.6806817054748535, lr: 5.9999999999999995e-05
Step 4085, training loss: 3.8871560096740723, total_norm: 4.176907539367676, lr: 5.9999999999999995e-05
Step 4086, training loss: 4.136106967926025, total_norm: 4.256478309631348, lr: 5.9999999999999995e-05
Step 4087, training loss: 4.063955783843994, total_norm: 4.265551567077637, lr: 5.9999999999999995e-05
Step 4088, training loss: 3.9125075340270996, total_norm: 4.072446823120117, lr: 5.9999999999999995e-05
Step 4089, training loss: 3.813538074493408, total_norm: 3.727308988571167, lr: 5.9999999999999995e-05
Step 4090, training loss: 4.166375160217285, total_norm: 4.067723274230957, lr: 5.9999999999999995e-05
Step 4091, training loss: 3.9427027702331543, total_norm: 3.9776737689971924, lr: 5.9999999999999995e-05
Step 4092, training loss: 3.931554079055786, total_norm: 4.693984508514404, lr: 5.9999999999999995e-05
Step 4093, training loss: 4.1068620681762695, total_norm: 4.576875686645508, lr: 5.9999999999999995e-05
Step 4094, training loss: 3.864626407623291, total_norm: 4.682785511016846, lr: 5.9999999999999995e-05
Step 4095, training loss: 3.825026512145996, total_norm: 4.046176910400391, lr: 5.9999999999999995e-05
Step 4096, training loss: 4.19652795791626, total_norm: 4.508312702178955, lr: 5.9999999999999995e-05
Step 4097, training loss: 4.264370918273926, total_norm: 3.9285335540771484, lr: 5.9999999999999995e-05
Step 4098, training loss: 3.6414918899536133, total_norm: 4.213263034820557, lr: 5.9999999999999995e-05
Step 4099, training loss: 3.6818251609802246, total_norm: 4.567455291748047, lr: 5.9999999999999995e-05
Step 4100, training loss: 3.8393163681030273, total_norm: 4.420999050140381, lr: 5.9999999999999995e-05
Step 4100, validation loss: 5.560852527618408
Step 4101, training loss: 4.182676792144775, total_norm: 4.855041980743408, lr: 5.9999999999999995e-05
Step 4102, training loss: 4.056011199951172, total_norm: 4.202293872833252, lr: 5.9999999999999995e-05
Step 4103, training loss: 4.722209453582764, total_norm: 4.485910415649414, lr: 5.9999999999999995e-05
Step 4104, training loss: 3.9544708728790283, total_norm: 3.6033711433410645, lr: 5.9999999999999995e-05
Step 4105, training loss: 3.6650280952453613, total_norm: 3.4439303874969482, lr: 5.9999999999999995e-05
Step 4106, training loss: 3.3038339614868164, total_norm: 3.4967947006225586, lr: 5.9999999999999995e-05
Step 4107, training loss: 3.4344375133514404, total_norm: 3.523040294647217, lr: 5.9999999999999995e-05
Step 4108, training loss: 3.9146177768707275, total_norm: 4.526327133178711, lr: 5.9999999999999995e-05
Step 4109, training loss: 4.234710216522217, total_norm: 4.755788326263428, lr: 5.9999999999999995e-05
Step 4110, training loss: 4.590009689331055, total_norm: 4.488645076751709, lr: 5.9999999999999995e-05
Step 4111, training loss: 4.2311787605285645, total_norm: 4.905374050140381, lr: 5.9999999999999995e-05
Step 4112, training loss: 5.049557209014893, total_norm: 6.274752616882324, lr: 5.9999999999999995e-05
Step 4113, training loss: 4.3156418800354, total_norm: 4.8715314865112305, lr: 5.9999999999999995e-05
Step 4114, training loss: 4.5383501052856445, total_norm: 6.083014965057373, lr: 5.9999999999999995e-05
Step 4115, training loss: 4.247661590576172, total_norm: 4.345003128051758, lr: 5.9999999999999995e-05
Step 4116, training loss: 4.624573230743408, total_norm: 4.83805513381958, lr: 5.9999999999999995e-05
Step 4117, training loss: 4.476184844970703, total_norm: 4.424922466278076, lr: 5.9999999999999995e-05
Step 4118, training loss: 4.3400044441223145, total_norm: 4.1677374839782715, lr: 5.9999999999999995e-05
Step 4119, training loss: 4.209135055541992, total_norm: 4.188508987426758, lr: 5.9999999999999995e-05
Step 4120, training loss: 4.612109661102295, total_norm: 4.357876300811768, lr: 5.9999999999999995e-05
Step 4121, training loss: 4.476884841918945, total_norm: 3.798607110977173, lr: 5.9999999999999995e-05
Step 4122, training loss: 4.033874988555908, total_norm: 3.8701376914978027, lr: 5.9999999999999995e-05
Step 4123, training loss: 4.0069899559021, total_norm: 4.287240505218506, lr: 5.9999999999999995e-05
Step 4124, training loss: 4.417665958404541, total_norm: 3.8065004348754883, lr: 5.9999999999999995e-05
Step 4125, training loss: 3.756721019744873, total_norm: 3.8184025287628174, lr: 5.9999999999999995e-05
Step 4126, training loss: 4.043796539306641, total_norm: 3.4973204135894775, lr: 5.9999999999999995e-05
Step 4127, training loss: 4.386267185211182, total_norm: 3.568530797958374, lr: 5.9999999999999995e-05
Step 4128, training loss: 4.078030109405518, total_norm: 3.81831693649292, lr: 5.9999999999999995e-05
Step 4129, training loss: 4.2795867919921875, total_norm: 3.877875566482544, lr: 5.9999999999999995e-05
Step 4130, training loss: 4.108620643615723, total_norm: 4.130945682525635, lr: 5.9999999999999995e-05
Step 4131, training loss: 4.351224899291992, total_norm: 4.8016886711120605, lr: 5.9999999999999995e-05
Step 4132, training loss: 4.065377235412598, total_norm: 4.182112693786621, lr: 5.9999999999999995e-05
Step 4133, training loss: 3.760425090789795, total_norm: 3.4917168617248535, lr: 5.9999999999999995e-05
Step 4134, training loss: 3.757844924926758, total_norm: 3.5883219242095947, lr: 5.9999999999999995e-05
Step 4135, training loss: 4.07943868637085, total_norm: 3.791382312774658, lr: 5.9999999999999995e-05
Step 4136, training loss: 3.9782121181488037, total_norm: 3.3985085487365723, lr: 5.9999999999999995e-05
Step 4137, training loss: 4.36997652053833, total_norm: 4.739944934844971, lr: 5.9999999999999995e-05
Step 4138, training loss: 4.557829856872559, total_norm: 4.842676639556885, lr: 5.9999999999999995e-05
Step 4139, training loss: 4.346339702606201, total_norm: 4.0192108154296875, lr: 5.9999999999999995e-05
Step 4140, training loss: 4.440604209899902, total_norm: 3.43825101852417, lr: 5.9999999999999995e-05
Step 4141, training loss: 3.7989485263824463, total_norm: 3.620229482650757, lr: 5.9999999999999995e-05
Step 4142, training loss: 4.696139335632324, total_norm: 3.87821888923645, lr: 5.9999999999999995e-05
Step 4143, training loss: 4.292463302612305, total_norm: 3.5022685527801514, lr: 5.9999999999999995e-05
Step 4144, training loss: 4.078406810760498, total_norm: 3.717780351638794, lr: 5.9999999999999995e-05
Step 4145, training loss: 4.9799346923828125, total_norm: 3.7739920616149902, lr: 5.9999999999999995e-05
Step 4146, training loss: 5.145065784454346, total_norm: 5.539793491363525, lr: 5.9999999999999995e-05
Step 4147, training loss: 4.546339988708496, total_norm: 4.419750690460205, lr: 5.9999999999999995e-05
Step 4148, training loss: 4.714476108551025, total_norm: 3.9873526096343994, lr: 5.9999999999999995e-05
Step 4149, training loss: 4.737346172332764, total_norm: 4.201440811157227, lr: 5.9999999999999995e-05
Step 4150, training loss: 4.989961624145508, total_norm: 4.253981590270996, lr: 5.9999999999999995e-05
Step 4150, validation loss: 5.341119289398193
Step 4151, training loss: 4.600374221801758, total_norm: 5.1872076988220215, lr: 5.9999999999999995e-05
Step 4152, training loss: 4.102016448974609, total_norm: 4.311654567718506, lr: 5.9999999999999995e-05
Step 4153, training loss: 4.174227237701416, total_norm: 4.073800563812256, lr: 5.9999999999999995e-05
Step 4154, training loss: 4.5904130935668945, total_norm: 4.731592178344727, lr: 5.9999999999999995e-05
Step 4155, training loss: 4.5936102867126465, total_norm: 4.67121696472168, lr: 5.9999999999999995e-05
Step 4156, training loss: 4.784215450286865, total_norm: 5.621391773223877, lr: 5.9999999999999995e-05
Step 4157, training loss: 4.107875823974609, total_norm: 3.605597734451294, lr: 5.9999999999999995e-05
Step 4158, training loss: 4.885288715362549, total_norm: 4.994552135467529, lr: 5.9999999999999995e-05
Step 4159, training loss: 4.704495429992676, total_norm: 3.8419249057769775, lr: 5.9999999999999995e-05
Step 4160, training loss: 4.214730739593506, total_norm: 4.286288738250732, lr: 5.9999999999999995e-05
Step 4161, training loss: 3.8354880809783936, total_norm: 6.249863147735596, lr: 5.9999999999999995e-05
Step 4162, training loss: 4.643554210662842, total_norm: 5.855815410614014, lr: 5.9999999999999995e-05
Step 4163, training loss: 3.813624858856201, total_norm: 4.43446159362793, lr: 5.9999999999999995e-05
Step 4164, training loss: 3.7912254333496094, total_norm: 3.548773765563965, lr: 5.9999999999999995e-05
Step 4165, training loss: 4.502912998199463, total_norm: 3.6633124351501465, lr: 5.9999999999999995e-05
Step 4166, training loss: 4.08066463470459, total_norm: 3.902536630630493, lr: 5.9999999999999995e-05
Step 4167, training loss: 4.056088924407959, total_norm: 4.105993747711182, lr: 5.9999999999999995e-05
Step 4168, training loss: 4.195834636688232, total_norm: 4.470730781555176, lr: 5.9999999999999995e-05
Step 4169, training loss: 4.729926109313965, total_norm: 6.025140285491943, lr: 5.9999999999999995e-05
Step 4170, training loss: 3.801140546798706, total_norm: 4.911902904510498, lr: 5.9999999999999995e-05
Step 4171, training loss: 4.021108150482178, total_norm: 4.529906272888184, lr: 5.9999999999999995e-05
Step 4172, training loss: 3.9998555183410645, total_norm: 4.051724433898926, lr: 5.9999999999999995e-05
Step 4173, training loss: 4.0406599044799805, total_norm: 4.001373767852783, lr: 5.9999999999999995e-05
Step 4174, training loss: 4.879636287689209, total_norm: 4.965014457702637, lr: 5.9999999999999995e-05
Step 4175, training loss: 3.997950553894043, total_norm: 4.266454219818115, lr: 5.9999999999999995e-05
Step 4176, training loss: 4.440914630889893, total_norm: 4.763722896575928, lr: 5.9999999999999995e-05
Step 4177, training loss: 4.138764381408691, total_norm: 4.443878650665283, lr: 5.9999999999999995e-05
Step 4178, training loss: 4.197319984436035, total_norm: 3.8408327102661133, lr: 5.9999999999999995e-05
Step 4179, training loss: 4.508615493774414, total_norm: 3.9210262298583984, lr: 5.9999999999999995e-05
Step 4180, training loss: 4.472783088684082, total_norm: 4.361933708190918, lr: 5.9999999999999995e-05
Step 4181, training loss: 3.9300451278686523, total_norm: 4.269287109375, lr: 5.9999999999999995e-05
Step 4182, training loss: 4.8056640625, total_norm: 5.6673102378845215, lr: 5.9999999999999995e-05
Step 4183, training loss: 5.046558856964111, total_norm: 5.895318984985352, lr: 5.9999999999999995e-05
Step 4184, training loss: 4.93477725982666, total_norm: 4.365695953369141, lr: 5.9999999999999995e-05
Step 4185, training loss: 4.204195499420166, total_norm: 4.150318145751953, lr: 5.9999999999999995e-05
Step 4186, training loss: 3.7795486450195312, total_norm: 4.457126140594482, lr: 5.9999999999999995e-05
Step 4187, training loss: 4.407318592071533, total_norm: 5.32977294921875, lr: 5.9999999999999995e-05
Step 4188, training loss: 4.006566524505615, total_norm: 5.251756191253662, lr: 5.9999999999999995e-05
Step 4189, training loss: 4.148151397705078, total_norm: 5.419663429260254, lr: 5.9999999999999995e-05
Step 4190, training loss: 4.614765167236328, total_norm: 7.086955547332764, lr: 5.9999999999999995e-05
Step 4191, training loss: 4.729386329650879, total_norm: 5.760383129119873, lr: 5.9999999999999995e-05
Step 4192, training loss: 4.187461853027344, total_norm: 4.434179782867432, lr: 5.9999999999999995e-05
Step 4193, training loss: 4.238485813140869, total_norm: 4.6043782234191895, lr: 5.9999999999999995e-05
Step 4194, training loss: 4.165898323059082, total_norm: 8.524870872497559, lr: 5.9999999999999995e-05
Step 4195, training loss: 4.4004974365234375, total_norm: 9.890625, lr: 5.9999999999999995e-05
Step 4196, training loss: 4.492767333984375, total_norm: 5.807009696960449, lr: 5.9999999999999995e-05
Step 4197, training loss: 4.6350603103637695, total_norm: 6.874521732330322, lr: 5.9999999999999995e-05
Step 4198, training loss: 4.879518032073975, total_norm: 6.16529655456543, lr: 5.9999999999999995e-05
Step 4199, training loss: 4.100781440734863, total_norm: 4.09796142578125, lr: 5.9999999999999995e-05
Step 4200, training loss: 4.491401195526123, total_norm: 3.761277914047241, lr: 5.9999999999999995e-05
Step 4200, validation loss: 5.187883377075195
Step 4201, training loss: 4.127381801605225, total_norm: 4.826445579528809, lr: 5.9999999999999995e-05
Step 4202, training loss: 4.1103129386901855, total_norm: 7.018564224243164, lr: 5.9999999999999995e-05
Step 4203, training loss: 3.9375925064086914, total_norm: 7.6309003829956055, lr: 5.9999999999999995e-05
Step 4204, training loss: 4.161799907684326, total_norm: 8.071798324584961, lr: 5.9999999999999995e-05
Step 4205, training loss: 3.3507964611053467, total_norm: 3.9139976501464844, lr: 5.9999999999999995e-05
Step 4206, training loss: 4.040489673614502, total_norm: 5.108166217803955, lr: 5.9999999999999995e-05
Step 4207, training loss: 4.007019996643066, total_norm: 4.814346790313721, lr: 5.9999999999999995e-05
Step 4208, training loss: 3.944714307785034, total_norm: 4.175274848937988, lr: 5.9999999999999995e-05
Step 4209, training loss: 3.837590217590332, total_norm: 5.00125789642334, lr: 5.9999999999999995e-05
Step 4210, training loss: 3.435473918914795, total_norm: 4.132986068725586, lr: 5.9999999999999995e-05
Step 4211, training loss: 3.9920899868011475, total_norm: 4.537635326385498, lr: 5.9999999999999995e-05
Step 4212, training loss: 4.261966705322266, total_norm: 4.8950982093811035, lr: 5.9999999999999995e-05
Step 4213, training loss: 4.079257011413574, total_norm: 4.730746746063232, lr: 5.9999999999999995e-05
Step 4214, training loss: 3.741570472717285, total_norm: 6.788210868835449, lr: 5.9999999999999995e-05
Step 4215, training loss: 4.323332786560059, total_norm: 4.0018768310546875, lr: 5.9999999999999995e-05
Step 4216, training loss: 3.850979804992676, total_norm: 3.4772531986236572, lr: 5.9999999999999995e-05
Step 4217, training loss: 3.8003346920013428, total_norm: 3.429760694503784, lr: 5.9999999999999995e-05
Step 4218, training loss: 3.8531739711761475, total_norm: 3.6127030849456787, lr: 5.9999999999999995e-05
Step 4219, training loss: 4.244811058044434, total_norm: 3.6428000926971436, lr: 5.9999999999999995e-05
Step 4220, training loss: 4.557183265686035, total_norm: 4.058011054992676, lr: 5.9999999999999995e-05
Step 4221, training loss: 3.710507869720459, total_norm: 4.8147172927856445, lr: 5.9999999999999995e-05
Step 4222, training loss: 3.935293674468994, total_norm: 3.833916425704956, lr: 5.9999999999999995e-05
Step 4223, training loss: 4.082591533660889, total_norm: 4.226657867431641, lr: 5.9999999999999995e-05
Step 4224, training loss: 4.429902076721191, total_norm: 4.721902847290039, lr: 5.9999999999999995e-05
Step 4225, training loss: 4.519395351409912, total_norm: 4.039815902709961, lr: 5.9999999999999995e-05
Step 4226, training loss: 4.178881645202637, total_norm: 3.7459146976470947, lr: 5.9999999999999995e-05
Step 4227, training loss: 4.126811504364014, total_norm: 3.4578325748443604, lr: 5.9999999999999995e-05
Step 4228, training loss: 4.677794456481934, total_norm: 4.581526279449463, lr: 5.9999999999999995e-05
Step 4229, training loss: 4.190821170806885, total_norm: 4.869040012359619, lr: 5.9999999999999995e-05
Step 4230, training loss: 3.8615615367889404, total_norm: 4.91659688949585, lr: 5.9999999999999995e-05
Step 4231, training loss: 4.200588226318359, total_norm: 4.706663608551025, lr: 5.9999999999999995e-05
Step 4232, training loss: 4.4440484046936035, total_norm: 3.8524081707000732, lr: 5.9999999999999995e-05
Step 4233, training loss: 4.064682960510254, total_norm: 4.479526519775391, lr: 5.9999999999999995e-05
Step 4234, training loss: 3.912062168121338, total_norm: 4.571870803833008, lr: 5.9999999999999995e-05
Step 4235, training loss: 3.6263768672943115, total_norm: 4.195667266845703, lr: 5.9999999999999995e-05
Step 4236, training loss: 4.356082916259766, total_norm: 3.7534596920013428, lr: 5.9999999999999995e-05
Step 4237, training loss: 3.9778826236724854, total_norm: 3.8190250396728516, lr: 5.9999999999999995e-05
Step 4238, training loss: 4.065225124359131, total_norm: 3.600548028945923, lr: 5.9999999999999995e-05
Step 4239, training loss: 3.650754690170288, total_norm: 3.226349353790283, lr: 5.9999999999999995e-05
Step 4240, training loss: 4.145763397216797, total_norm: 3.187628746032715, lr: 5.9999999999999995e-05
Step 4241, training loss: 4.024159908294678, total_norm: 3.636754274368286, lr: 5.9999999999999995e-05
Step 4242, training loss: 4.275003910064697, total_norm: 3.769866943359375, lr: 5.9999999999999995e-05
Step 4243, training loss: 3.573822259902954, total_norm: 4.055905342102051, lr: 5.9999999999999995e-05
Step 4244, training loss: 3.901216745376587, total_norm: 3.7704687118530273, lr: 5.9999999999999995e-05
Step 4245, training loss: 3.7393243312835693, total_norm: 3.767531633377075, lr: 5.9999999999999995e-05
Step 4246, training loss: 4.98459005355835, total_norm: 4.720925331115723, lr: 5.9999999999999995e-05
Step 4247, training loss: 4.2250261306762695, total_norm: 4.530557632446289, lr: 5.9999999999999995e-05
Step 4248, training loss: 3.8216171264648438, total_norm: 4.389764308929443, lr: 5.9999999999999995e-05
Step 4249, training loss: 3.831144094467163, total_norm: 5.085334300994873, lr: 5.9999999999999995e-05
Step 4250, training loss: 4.418366432189941, total_norm: 4.744742393493652, lr: 5.9999999999999995e-05
Step 4250, validation loss: 5.367575168609619
Step 4251, training loss: 4.061078071594238, total_norm: 4.105237007141113, lr: 5.9999999999999995e-05
Step 4252, training loss: 4.457309246063232, total_norm: 4.757534980773926, lr: 5.9999999999999995e-05
Step 4253, training loss: 3.933257579803467, total_norm: 3.639232873916626, lr: 5.9999999999999995e-05
Step 4254, training loss: 4.556453704833984, total_norm: 4.327766418457031, lr: 5.9999999999999995e-05
Step 4255, training loss: 3.7639198303222656, total_norm: 3.8529281616210938, lr: 5.9999999999999995e-05
Step 4256, training loss: 4.222113132476807, total_norm: 4.264952182769775, lr: 5.9999999999999995e-05
Step 4257, training loss: 3.989370584487915, total_norm: 3.578986406326294, lr: 5.9999999999999995e-05
Step 4258, training loss: 3.4280803203582764, total_norm: 4.293556213378906, lr: 5.9999999999999995e-05
Step 4259, training loss: 4.342722415924072, total_norm: 4.097784996032715, lr: 5.9999999999999995e-05
Step 4260, training loss: 3.2068307399749756, total_norm: 3.795870780944824, lr: 5.9999999999999995e-05
Step 4261, training loss: 4.150223731994629, total_norm: 3.7677481174468994, lr: 5.9999999999999995e-05
Step 4262, training loss: 4.5109639167785645, total_norm: 3.8551411628723145, lr: 5.9999999999999995e-05
Step 4263, training loss: 3.4333200454711914, total_norm: 3.3521015644073486, lr: 5.9999999999999995e-05
Step 4264, training loss: 3.09437894821167, total_norm: 3.5432190895080566, lr: 5.9999999999999995e-05
Step 4265, training loss: 3.6231393814086914, total_norm: 3.87908935546875, lr: 5.9999999999999995e-05
Step 4266, training loss: 4.022866249084473, total_norm: 4.774345874786377, lr: 5.9999999999999995e-05
Step 4267, training loss: 4.455461025238037, total_norm: 4.546051025390625, lr: 5.9999999999999995e-05
Step 4268, training loss: 3.64245867729187, total_norm: 4.265561580657959, lr: 5.9999999999999995e-05
Step 4269, training loss: 3.1993815898895264, total_norm: 3.457551956176758, lr: 5.9999999999999995e-05
Step 4270, training loss: 3.3683621883392334, total_norm: 3.4992990493774414, lr: 5.9999999999999995e-05
Step 4271, training loss: 3.2237696647644043, total_norm: 3.6325607299804688, lr: 5.9999999999999995e-05
Step 4272, training loss: 3.9535505771636963, total_norm: 3.98514723777771, lr: 5.9999999999999995e-05
Step 4273, training loss: 3.764139413833618, total_norm: 3.665628433227539, lr: 5.9999999999999995e-05
Step 4274, training loss: 3.3448660373687744, total_norm: 3.6585137844085693, lr: 5.9999999999999995e-05
Step 4275, training loss: 4.253355503082275, total_norm: 4.076033115386963, lr: 5.9999999999999995e-05
Step 4276, training loss: 3.9764082431793213, total_norm: 4.281740188598633, lr: 5.9999999999999995e-05
Step 4277, training loss: 3.555345058441162, total_norm: 3.670419454574585, lr: 5.9999999999999995e-05
Step 4278, training loss: 3.4626147747039795, total_norm: 3.3007521629333496, lr: 5.9999999999999995e-05
Step 4279, training loss: 3.606623888015747, total_norm: 3.397589921951294, lr: 5.9999999999999995e-05
Step 4280, training loss: 3.411431074142456, total_norm: 3.442713737487793, lr: 5.9999999999999995e-05
Step 4281, training loss: 4.099241256713867, total_norm: 4.384591579437256, lr: 5.9999999999999995e-05
Step 4282, training loss: 4.217684745788574, total_norm: 4.568366050720215, lr: 5.9999999999999995e-05
Step 4283, training loss: 3.1863110065460205, total_norm: 3.5140769481658936, lr: 5.9999999999999995e-05
Step 4284, training loss: 3.2853920459747314, total_norm: 3.746678352355957, lr: 5.9999999999999995e-05
Step 4285, training loss: 4.4971842765808105, total_norm: 4.959120273590088, lr: 5.9999999999999995e-05
Step 4286, training loss: 4.120267868041992, total_norm: 3.8117620944976807, lr: 5.9999999999999995e-05
Step 4287, training loss: 3.197948932647705, total_norm: 4.477227210998535, lr: 5.9999999999999995e-05
Step 4288, training loss: 3.3308372497558594, total_norm: 4.538475513458252, lr: 5.9999999999999995e-05
Step 4289, training loss: 4.720130920410156, total_norm: 5.201076030731201, lr: 5.9999999999999995e-05
Step 4290, training loss: 4.378113269805908, total_norm: 4.669592380523682, lr: 5.9999999999999995e-05
Step 4291, training loss: 3.4788007736206055, total_norm: 4.453408241271973, lr: 5.9999999999999995e-05
Step 4292, training loss: 4.277339935302734, total_norm: 4.57256555557251, lr: 5.9999999999999995e-05
Step 4293, training loss: 3.735133409500122, total_norm: 4.1658430099487305, lr: 5.9999999999999995e-05
Step 4294, training loss: 3.5698671340942383, total_norm: 3.8920412063598633, lr: 5.9999999999999995e-05
Step 4295, training loss: 3.410576343536377, total_norm: 3.5206642150878906, lr: 5.9999999999999995e-05
Step 4296, training loss: 3.865389823913574, total_norm: 3.6207377910614014, lr: 5.9999999999999995e-05
Step 4297, training loss: 3.6533854007720947, total_norm: 3.7433485984802246, lr: 5.9999999999999995e-05
Step 4298, training loss: 4.166196823120117, total_norm: 4.317666053771973, lr: 5.9999999999999995e-05
Step 4299, training loss: 4.030243396759033, total_norm: 3.772916555404663, lr: 5.9999999999999995e-05
Step 4300, training loss: 3.722867965698242, total_norm: 3.3608558177948, lr: 5.9999999999999995e-05
Step 4300, validation loss: 5.3937859535217285
Step 4301, training loss: 3.2978782653808594, total_norm: 3.449213981628418, lr: 5.9999999999999995e-05
Step 4302, training loss: 4.361105918884277, total_norm: 4.820488452911377, lr: 5.9999999999999995e-05
Step 4303, training loss: 4.416749954223633, total_norm: 4.306526184082031, lr: 5.9999999999999995e-05
Step 4304, training loss: 3.937899351119995, total_norm: 3.7079780101776123, lr: 5.9999999999999995e-05
Step 4305, training loss: 4.397576808929443, total_norm: 3.964463710784912, lr: 5.9999999999999995e-05
Step 4306, training loss: 3.886538505554199, total_norm: 3.692265748977661, lr: 5.9999999999999995e-05
Step 4307, training loss: 4.445249557495117, total_norm: 4.157234191894531, lr: 5.9999999999999995e-05
Step 4308, training loss: 4.535953998565674, total_norm: 4.294587135314941, lr: 5.9999999999999995e-05
Step 4309, training loss: 3.703789234161377, total_norm: 3.9234490394592285, lr: 5.9999999999999995e-05
Step 4310, training loss: 4.261632919311523, total_norm: 3.9468369483947754, lr: 5.9999999999999995e-05
Step 4311, training loss: 4.130503177642822, total_norm: 3.8737633228302, lr: 5.9999999999999995e-05
Step 4312, training loss: 4.021633625030518, total_norm: 4.288428783416748, lr: 5.9999999999999995e-05
Step 4313, training loss: 3.981887102127075, total_norm: 3.6362650394439697, lr: 5.9999999999999995e-05
Step 4314, training loss: 3.71972393989563, total_norm: 3.6940386295318604, lr: 5.9999999999999995e-05
Step 4315, training loss: 3.9523870944976807, total_norm: 3.8349132537841797, lr: 5.9999999999999995e-05
Step 4316, training loss: 4.980471134185791, total_norm: 4.431516647338867, lr: 5.9999999999999995e-05
Step 4317, training loss: 4.227102756500244, total_norm: 4.3692755699157715, lr: 5.9999999999999995e-05
Step 4318, training loss: 4.087670803070068, total_norm: 4.886839389801025, lr: 5.9999999999999995e-05
Step 4319, training loss: 3.8757944107055664, total_norm: 3.8981716632843018, lr: 5.9999999999999995e-05
Step 4320, training loss: 4.528499126434326, total_norm: 4.589569091796875, lr: 5.9999999999999995e-05
Step 4321, training loss: 4.287537097930908, total_norm: 4.327348232269287, lr: 5.9999999999999995e-05
Step 4322, training loss: 3.4809954166412354, total_norm: 3.792522668838501, lr: 5.9999999999999995e-05
Step 4323, training loss: 3.230808734893799, total_norm: 3.2958126068115234, lr: 5.9999999999999995e-05
Step 4324, training loss: 3.4394724369049072, total_norm: 3.260899782180786, lr: 5.9999999999999995e-05
Step 4325, training loss: 3.313955307006836, total_norm: 3.2805593013763428, lr: 5.9999999999999995e-05
Step 4326, training loss: 3.8352701663970947, total_norm: 3.805938720703125, lr: 5.9999999999999995e-05
Step 4327, training loss: 4.086911201477051, total_norm: 3.741579532623291, lr: 5.9999999999999995e-05
Step 4328, training loss: 3.8993401527404785, total_norm: 4.053172588348389, lr: 5.9999999999999995e-05
Step 4329, training loss: 4.20457124710083, total_norm: 3.9289019107818604, lr: 5.9999999999999995e-05
Step 4330, training loss: 3.763618230819702, total_norm: 3.4780044555664062, lr: 5.9999999999999995e-05
Step 4331, training loss: 3.608248233795166, total_norm: 3.445068597793579, lr: 5.9999999999999995e-05
Step 4332, training loss: 3.884707450866699, total_norm: 3.9594345092773438, lr: 5.9999999999999995e-05
Step 4333, training loss: 3.9607064723968506, total_norm: 4.467617988586426, lr: 5.9999999999999995e-05
Step 4334, training loss: 3.539562702178955, total_norm: 3.5992026329040527, lr: 5.9999999999999995e-05
Step 4335, training loss: 3.7496464252471924, total_norm: 3.939366340637207, lr: 5.9999999999999995e-05
Step 4336, training loss: 3.717072010040283, total_norm: 3.5231192111968994, lr: 5.9999999999999995e-05
Step 4337, training loss: 3.9921278953552246, total_norm: 4.1331353187561035, lr: 5.9999999999999995e-05
Step 4338, training loss: 4.633277893066406, total_norm: 5.237147331237793, lr: 5.9999999999999995e-05
Step 4339, training loss: 3.840689182281494, total_norm: 4.049042224884033, lr: 5.9999999999999995e-05
Step 4340, training loss: 3.977707862854004, total_norm: 4.729741096496582, lr: 5.9999999999999995e-05
Step 4341, training loss: 3.4698455333709717, total_norm: 3.95115327835083, lr: 5.9999999999999995e-05
Step 4342, training loss: 3.994080066680908, total_norm: 3.879620313644409, lr: 5.9999999999999995e-05
Step 4343, training loss: 3.5216522216796875, total_norm: 3.7719874382019043, lr: 5.9999999999999995e-05
Step 4344, training loss: 3.7680726051330566, total_norm: 3.482316017150879, lr: 5.9999999999999995e-05
Step 4345, training loss: 3.8853070735931396, total_norm: 4.11028528213501, lr: 5.9999999999999995e-05
Step 4346, training loss: 3.7409470081329346, total_norm: 3.7234816551208496, lr: 5.9999999999999995e-05
Step 4347, training loss: 4.172618389129639, total_norm: 4.098050594329834, lr: 5.9999999999999995e-05
Step 4348, training loss: 3.8983206748962402, total_norm: 4.896409511566162, lr: 5.9999999999999995e-05
Step 4349, training loss: 3.9991137981414795, total_norm: 3.990147113800049, lr: 5.9999999999999995e-05
Step 4350, training loss: 4.113641738891602, total_norm: 4.166769504547119, lr: 5.9999999999999995e-05
Step 4350, validation loss: 5.447642803192139
Step 4351, training loss: 4.076709270477295, total_norm: 3.9945173263549805, lr: 5.9999999999999995e-05
Step 4352, training loss: 3.7330613136291504, total_norm: 4.314225673675537, lr: 5.9999999999999995e-05
Step 4353, training loss: 4.021237850189209, total_norm: 3.989147663116455, lr: 5.9999999999999995e-05
Step 4354, training loss: 3.4195516109466553, total_norm: 4.139501571655273, lr: 5.9999999999999995e-05
Step 4355, training loss: 3.5065972805023193, total_norm: 4.075376987457275, lr: 5.9999999999999995e-05
Step 4356, training loss: 3.6768627166748047, total_norm: 4.470425605773926, lr: 5.9999999999999995e-05
Step 4357, training loss: 3.9066426753997803, total_norm: 4.379765510559082, lr: 5.9999999999999995e-05
Step 4358, training loss: 3.116884708404541, total_norm: 3.8547637462615967, lr: 5.9999999999999995e-05
Step 4359, training loss: 3.125290632247925, total_norm: 3.36973237991333, lr: 5.9999999999999995e-05
Step 4360, training loss: 3.664473295211792, total_norm: 3.834160327911377, lr: 5.9999999999999995e-05
Step 4361, training loss: 3.4410221576690674, total_norm: 4.00784969329834, lr: 5.9999999999999995e-05
Step 4362, training loss: 3.843179941177368, total_norm: 4.129186153411865, lr: 5.9999999999999995e-05
Step 4363, training loss: 3.66257381439209, total_norm: 3.849097490310669, lr: 5.9999999999999995e-05
Step 4364, training loss: 3.487069845199585, total_norm: 4.571094989776611, lr: 5.9999999999999995e-05
Step 4365, training loss: 3.7109732627868652, total_norm: 3.5775580406188965, lr: 5.9999999999999995e-05
Step 4366, training loss: 3.2725296020507812, total_norm: 3.518099069595337, lr: 5.9999999999999995e-05
Step 4367, training loss: 3.8639094829559326, total_norm: 3.572279930114746, lr: 5.9999999999999995e-05
Step 4368, training loss: 3.9912638664245605, total_norm: 3.447410821914673, lr: 5.9999999999999995e-05
Step 4369, training loss: 3.7923367023468018, total_norm: 3.5332205295562744, lr: 5.9999999999999995e-05
Step 4370, training loss: 4.178504943847656, total_norm: 4.098232746124268, lr: 5.9999999999999995e-05
Step 4371, training loss: 4.263205528259277, total_norm: 4.483241558074951, lr: 5.9999999999999995e-05
Step 4372, training loss: 4.137878894805908, total_norm: 3.9354968070983887, lr: 5.9999999999999995e-05
Step 4373, training loss: 3.73816180229187, total_norm: 3.636721611022949, lr: 5.9999999999999995e-05
Step 4374, training loss: 4.063304901123047, total_norm: 3.7100327014923096, lr: 5.9999999999999995e-05
Step 4375, training loss: 4.395773887634277, total_norm: 3.93472957611084, lr: 5.9999999999999995e-05
Step 4376, training loss: 4.054600715637207, total_norm: 4.102802276611328, lr: 5.9999999999999995e-05
Step 4377, training loss: 3.4214870929718018, total_norm: 3.3843729496002197, lr: 5.9999999999999995e-05
Step 4378, training loss: 3.4956552982330322, total_norm: 3.877673387527466, lr: 5.9999999999999995e-05
Step 4379, training loss: 4.069584846496582, total_norm: 3.950704336166382, lr: 5.9999999999999995e-05
Step 4380, training loss: 3.5448246002197266, total_norm: 3.790015697479248, lr: 5.9999999999999995e-05
Step 4381, training loss: 3.9086763858795166, total_norm: 4.262282848358154, lr: 5.9999999999999995e-05
Step 4382, training loss: 3.610386371612549, total_norm: 3.814760208129883, lr: 5.9999999999999995e-05
Step 4383, training loss: 3.208662509918213, total_norm: 4.321938991546631, lr: 5.9999999999999995e-05
Step 4384, training loss: 4.28680944442749, total_norm: 4.239602565765381, lr: 5.9999999999999995e-05
Step 4385, training loss: 4.23031759262085, total_norm: 6.621108055114746, lr: 5.9999999999999995e-05
Step 4386, training loss: 3.9735772609710693, total_norm: 5.685763359069824, lr: 5.9999999999999995e-05
Step 4387, training loss: 4.358529090881348, total_norm: 5.303666114807129, lr: 5.9999999999999995e-05
Step 4388, training loss: 4.132286548614502, total_norm: 4.705664157867432, lr: 5.9999999999999995e-05
Step 4389, training loss: 3.4712417125701904, total_norm: 4.433914661407471, lr: 5.9999999999999995e-05
Step 4390, training loss: 3.7042391300201416, total_norm: 4.821187973022461, lr: 5.9999999999999995e-05
Step 4391, training loss: 3.5431509017944336, total_norm: 4.219027042388916, lr: 5.9999999999999995e-05
Step 4392, training loss: 3.137131452560425, total_norm: 3.446321964263916, lr: 5.9999999999999995e-05
Step 4393, training loss: 3.464413642883301, total_norm: 3.881916046142578, lr: 5.9999999999999995e-05
Step 4394, training loss: 4.541367530822754, total_norm: 4.60498046875, lr: 5.9999999999999995e-05
Step 4395, training loss: 2.732820749282837, total_norm: 4.673441410064697, lr: 5.9999999999999995e-05
Step 4396, training loss: 3.548118829727173, total_norm: 4.300066947937012, lr: 5.9999999999999995e-05
Step 4397, training loss: 3.6112210750579834, total_norm: 3.8728814125061035, lr: 5.9999999999999995e-05
Step 4398, training loss: 3.3248751163482666, total_norm: 4.072127342224121, lr: 5.9999999999999995e-05
Step 4399, training loss: 3.5923409461975098, total_norm: 4.3057074546813965, lr: 5.9999999999999995e-05
Step 4400, training loss: 4.243747234344482, total_norm: 5.261782169342041, lr: 5.9999999999999995e-05
Step 4400, validation loss: 5.59252405166626
Step 4401, training loss: 4.4518914222717285, total_norm: 4.898958206176758, lr: 5.9999999999999995e-05
Step 4402, training loss: 4.151111125946045, total_norm: 5.314398765563965, lr: 5.9999999999999995e-05
Step 4403, training loss: 4.204011917114258, total_norm: 5.108521461486816, lr: 5.9999999999999995e-05
Step 4404, training loss: 3.6119048595428467, total_norm: 6.591215133666992, lr: 5.9999999999999995e-05
Step 4405, training loss: 4.512329578399658, total_norm: 8.856486320495605, lr: 5.9999999999999995e-05
Step 4406, training loss: 4.080286026000977, total_norm: 5.150893211364746, lr: 5.9999999999999995e-05
Step 4407, training loss: 4.180569648742676, total_norm: 4.677692890167236, lr: 5.9999999999999995e-05
Step 4408, training loss: 3.715454339981079, total_norm: 3.914246082305908, lr: 5.9999999999999995e-05
Step 4409, training loss: 3.8604118824005127, total_norm: 4.0080060958862305, lr: 5.9999999999999995e-05
Step 4410, training loss: 4.404147148132324, total_norm: 5.282922267913818, lr: 5.9999999999999995e-05
Step 4411, training loss: 4.308290481567383, total_norm: 7.280052661895752, lr: 5.9999999999999995e-05
Step 4412, training loss: 4.394676208496094, total_norm: 6.207132816314697, lr: 5.9999999999999995e-05
Step 4413, training loss: 4.288430213928223, total_norm: 5.845726490020752, lr: 5.9999999999999995e-05
Step 4414, training loss: 4.311548233032227, total_norm: 5.274367332458496, lr: 5.9999999999999995e-05
Step 4415, training loss: 4.363039970397949, total_norm: 4.062341213226318, lr: 5.9999999999999995e-05
Step 4416, training loss: 4.453660488128662, total_norm: 4.300767421722412, lr: 5.9999999999999995e-05
Step 4417, training loss: 4.265141010284424, total_norm: 4.768557548522949, lr: 5.9999999999999995e-05
Step 4418, training loss: 4.172966003417969, total_norm: 6.152875900268555, lr: 5.9999999999999995e-05
Step 4419, training loss: 4.107552528381348, total_norm: 5.394035816192627, lr: 5.9999999999999995e-05
Step 4420, training loss: 4.44193172454834, total_norm: 6.979371547698975, lr: 5.9999999999999995e-05
Step 4421, training loss: 4.573660373687744, total_norm: 5.199689865112305, lr: 5.9999999999999995e-05
Step 4422, training loss: 4.022456645965576, total_norm: 4.630128383636475, lr: 5.9999999999999995e-05
Step 4423, training loss: 4.093255519866943, total_norm: 3.9421770572662354, lr: 5.9999999999999995e-05
Step 4424, training loss: 3.5163164138793945, total_norm: 3.7928318977355957, lr: 5.9999999999999995e-05
Step 4425, training loss: 4.021284103393555, total_norm: 3.9691758155822754, lr: 5.9999999999999995e-05
Step 4426, training loss: 4.434566497802734, total_norm: 4.513153553009033, lr: 5.9999999999999995e-05
Step 4427, training loss: 4.327295303344727, total_norm: 4.291785717010498, lr: 5.9999999999999995e-05
Step 4428, training loss: 4.867884635925293, total_norm: 6.162569522857666, lr: 5.9999999999999995e-05
Step 4429, training loss: 4.074642181396484, total_norm: 3.9940476417541504, lr: 5.9999999999999995e-05
Step 4430, training loss: 4.277907848358154, total_norm: 4.406987190246582, lr: 5.9999999999999995e-05
Step 4431, training loss: 4.06768274307251, total_norm: 4.376657009124756, lr: 5.9999999999999995e-05
Step 4432, training loss: 4.156589031219482, total_norm: 4.314653396606445, lr: 5.9999999999999995e-05
Step 4433, training loss: 4.111842632293701, total_norm: 5.403270244598389, lr: 5.9999999999999995e-05
Step 4434, training loss: 4.373566150665283, total_norm: 4.933636665344238, lr: 5.9999999999999995e-05
Step 4435, training loss: 4.3808913230896, total_norm: 5.1723127365112305, lr: 5.9999999999999995e-05
Step 4436, training loss: 4.047936916351318, total_norm: 5.330011367797852, lr: 5.9999999999999995e-05
Step 4437, training loss: 4.036268711090088, total_norm: 3.827317953109741, lr: 5.9999999999999995e-05
Step 4438, training loss: 4.135730266571045, total_norm: 4.930293560028076, lr: 5.9999999999999995e-05
Step 4439, training loss: 3.734842538833618, total_norm: 4.201660633087158, lr: 5.9999999999999995e-05
Step 4440, training loss: 3.7383854389190674, total_norm: 4.798612594604492, lr: 5.9999999999999995e-05
Step 4441, training loss: 3.8369498252868652, total_norm: 4.114717960357666, lr: 5.9999999999999995e-05
Step 4442, training loss: 4.023646354675293, total_norm: 4.654078483581543, lr: 5.9999999999999995e-05
Step 4443, training loss: 4.282471179962158, total_norm: 4.082941055297852, lr: 5.9999999999999995e-05
Step 4444, training loss: 4.0386962890625, total_norm: 4.571944236755371, lr: 5.9999999999999995e-05
Step 4445, training loss: 4.236014366149902, total_norm: 4.088773727416992, lr: 5.9999999999999995e-05
Step 4446, training loss: 4.326871395111084, total_norm: 4.063848495483398, lr: 5.9999999999999995e-05
Step 4447, training loss: 4.400045394897461, total_norm: 4.469895839691162, lr: 5.9999999999999995e-05
Step 4448, training loss: 4.2790303230285645, total_norm: 4.736876010894775, lr: 5.9999999999999995e-05
Step 4449, training loss: 4.512391090393066, total_norm: 4.857132434844971, lr: 5.9999999999999995e-05
Step 4450, training loss: 3.688333749771118, total_norm: 4.0874528884887695, lr: 5.9999999999999995e-05
Step 4450, validation loss: 5.581341743469238
Step 4451, training loss: 3.9413225650787354, total_norm: 4.805211067199707, lr: 5.9999999999999995e-05
Step 4452, training loss: 4.494832992553711, total_norm: 4.291945934295654, lr: 5.9999999999999995e-05
Step 4453, training loss: 3.963435411453247, total_norm: 3.8408708572387695, lr: 5.9999999999999995e-05
Step 4454, training loss: 4.229093074798584, total_norm: 3.9624228477478027, lr: 5.9999999999999995e-05
Step 4455, training loss: 3.2718617916107178, total_norm: 3.6823925971984863, lr: 5.9999999999999995e-05
Step 4456, training loss: 4.201084136962891, total_norm: 3.9208908081054688, lr: 5.9999999999999995e-05
Step 4457, training loss: 4.4187421798706055, total_norm: 4.382804870605469, lr: 5.9999999999999995e-05
Step 4458, training loss: 3.986633539199829, total_norm: 4.627506256103516, lr: 5.9999999999999995e-05
Step 4459, training loss: 3.533778190612793, total_norm: 5.061216354370117, lr: 5.9999999999999995e-05
Step 4460, training loss: 4.0063958168029785, total_norm: 4.690502643585205, lr: 5.9999999999999995e-05
Step 4461, training loss: 4.047789573669434, total_norm: 5.4011359214782715, lr: 5.9999999999999995e-05
Step 4462, training loss: 4.116381645202637, total_norm: 3.916292667388916, lr: 5.9999999999999995e-05
Step 4463, training loss: 3.9075167179107666, total_norm: 4.088502407073975, lr: 5.9999999999999995e-05
Step 4464, training loss: 4.154392242431641, total_norm: 4.06920862197876, lr: 5.9999999999999995e-05
Step 4465, training loss: 3.6744296550750732, total_norm: 3.9695122241973877, lr: 5.9999999999999995e-05
Step 4466, training loss: 3.3338353633880615, total_norm: 4.198048114776611, lr: 5.9999999999999995e-05
Step 4467, training loss: 4.202242851257324, total_norm: 5.225698947906494, lr: 5.9999999999999995e-05
Step 4468, training loss: 4.2709879875183105, total_norm: 4.578103542327881, lr: 5.9999999999999995e-05
Step 4469, training loss: 3.752826452255249, total_norm: 4.075120449066162, lr: 5.9999999999999995e-05
Step 4470, training loss: 4.172424793243408, total_norm: 4.422847747802734, lr: 5.9999999999999995e-05
Step 4471, training loss: 3.0211524963378906, total_norm: 4.077126979827881, lr: 5.9999999999999995e-05
Step 4472, training loss: 2.841399908065796, total_norm: 3.6310653686523438, lr: 5.9999999999999995e-05
Step 4473, training loss: 3.5018317699432373, total_norm: 3.691657304763794, lr: 5.9999999999999995e-05
Step 4474, training loss: 2.8737261295318604, total_norm: 3.55587100982666, lr: 5.9999999999999995e-05
Step 4475, training loss: 3.9780776500701904, total_norm: 5.518383026123047, lr: 5.9999999999999995e-05
Step 4476, training loss: 3.6070945262908936, total_norm: 4.955451965332031, lr: 5.9999999999999995e-05
Step 4477, training loss: 3.5724730491638184, total_norm: 3.859919548034668, lr: 5.9999999999999995e-05
Step 4478, training loss: 4.584291934967041, total_norm: 4.665258884429932, lr: 5.9999999999999995e-05
Step 4479, training loss: 4.587035655975342, total_norm: 4.625894069671631, lr: 5.9999999999999995e-05
Step 4480, training loss: 4.026174545288086, total_norm: 4.617621421813965, lr: 5.9999999999999995e-05
Step 4481, training loss: 4.117068767547607, total_norm: 5.306765556335449, lr: 5.9999999999999995e-05
Step 4482, training loss: 4.3469696044921875, total_norm: 6.657355785369873, lr: 5.9999999999999995e-05
Step 4483, training loss: 3.828435182571411, total_norm: 7.471365451812744, lr: 5.9999999999999995e-05
Step 4484, training loss: 4.2262797355651855, total_norm: 7.694703102111816, lr: 5.9999999999999995e-05
Step 4485, training loss: 4.626952648162842, total_norm: 6.6979780197143555, lr: 5.9999999999999995e-05
Step 4486, training loss: 3.9149091243743896, total_norm: 5.460027694702148, lr: 5.9999999999999995e-05
Step 4487, training loss: 3.94498348236084, total_norm: 4.704091548919678, lr: 5.9999999999999995e-05
Step 4488, training loss: 4.226286888122559, total_norm: 5.291563510894775, lr: 5.9999999999999995e-05
Step 4489, training loss: 4.852344989776611, total_norm: 5.07985258102417, lr: 5.9999999999999995e-05
Step 4490, training loss: 4.137286186218262, total_norm: 6.460536956787109, lr: 5.9999999999999995e-05
Step 4491, training loss: 3.863339424133301, total_norm: 4.530915260314941, lr: 5.9999999999999995e-05
Step 4492, training loss: 4.407164573669434, total_norm: 4.644925117492676, lr: 5.9999999999999995e-05
Step 4493, training loss: 3.923326015472412, total_norm: 3.886401891708374, lr: 5.9999999999999995e-05
Step 4494, training loss: 3.94797420501709, total_norm: 4.050522327423096, lr: 5.9999999999999995e-05
Step 4495, training loss: 3.7792394161224365, total_norm: 4.164376258850098, lr: 5.9999999999999995e-05
Step 4496, training loss: 5.121127605438232, total_norm: 5.233636856079102, lr: 5.9999999999999995e-05
Step 4497, training loss: 4.453169345855713, total_norm: 4.551643371582031, lr: 5.9999999999999995e-05
Step 4498, training loss: 4.029486179351807, total_norm: 4.085766315460205, lr: 5.9999999999999995e-05
Step 4499, training loss: 3.885875701904297, total_norm: 4.118152618408203, lr: 5.9999999999999995e-05
Step 4500, training loss: 3.5257339477539062, total_norm: 3.806607484817505, lr: 5.9999999999999995e-05
Step 4500, validation loss: 5.560540199279785
Step 4501, training loss: 3.711428642272949, total_norm: 4.1254119873046875, lr: 5.9999999999999995e-05
Step 4502, training loss: 4.410451889038086, total_norm: 5.020314693450928, lr: 5.9999999999999995e-05
Step 4503, training loss: 4.018780708312988, total_norm: 4.390308856964111, lr: 5.9999999999999995e-05
Step 4504, training loss: 3.7654449939727783, total_norm: 4.058629035949707, lr: 5.9999999999999995e-05
Step 4505, training loss: 3.892531633377075, total_norm: 4.336184978485107, lr: 5.9999999999999995e-05
Step 4506, training loss: 3.521920680999756, total_norm: 3.8869986534118652, lr: 5.9999999999999995e-05
Step 4507, training loss: 3.674924373626709, total_norm: 3.731872797012329, lr: 5.9999999999999995e-05
Step 4508, training loss: 5.16488790512085, total_norm: 4.938273906707764, lr: 5.9999999999999995e-05
Step 4509, training loss: 4.129556655883789, total_norm: 4.450275421142578, lr: 5.9999999999999995e-05
Step 4510, training loss: 3.5523693561553955, total_norm: 3.9497079849243164, lr: 5.9999999999999995e-05
Step 4511, training loss: 4.908744812011719, total_norm: 5.056644439697266, lr: 5.9999999999999995e-05
Step 4512, training loss: 3.7875545024871826, total_norm: 4.052359104156494, lr: 5.9999999999999995e-05
Step 4513, training loss: 3.5937843322753906, total_norm: 4.010854721069336, lr: 5.9999999999999995e-05
Step 4514, training loss: 4.0123162269592285, total_norm: 4.4405670166015625, lr: 5.9999999999999995e-05
Step 4515, training loss: 3.754690408706665, total_norm: 3.692379951477051, lr: 5.9999999999999995e-05
Step 4516, training loss: 4.323575019836426, total_norm: 4.231764316558838, lr: 5.9999999999999995e-05
Step 4517, training loss: 3.7738404273986816, total_norm: 3.9652698040008545, lr: 5.9999999999999995e-05
Step 4518, training loss: 4.188314437866211, total_norm: 3.841953754425049, lr: 5.9999999999999995e-05
Step 4519, training loss: 3.8946714401245117, total_norm: 4.06565523147583, lr: 5.9999999999999995e-05
Step 4520, training loss: 4.355814456939697, total_norm: 4.38546895980835, lr: 5.9999999999999995e-05
Step 4521, training loss: 3.769958019256592, total_norm: 3.6395037174224854, lr: 5.9999999999999995e-05
Step 4522, training loss: 3.9184036254882812, total_norm: 3.9282000064849854, lr: 5.9999999999999995e-05
Step 4523, training loss: 3.572937250137329, total_norm: 4.125245094299316, lr: 5.9999999999999995e-05
Step 4524, training loss: 4.565417289733887, total_norm: 5.17140531539917, lr: 5.9999999999999995e-05
Step 4525, training loss: 4.4455952644348145, total_norm: 4.703233242034912, lr: 5.9999999999999995e-05
Step 4526, training loss: 3.887941360473633, total_norm: 3.886287212371826, lr: 5.9999999999999995e-05
Step 4527, training loss: 4.393701553344727, total_norm: 4.464388370513916, lr: 5.9999999999999995e-05
Step 4528, training loss: 4.0255818367004395, total_norm: 4.377679824829102, lr: 5.9999999999999995e-05
Step 4529, training loss: 4.216870307922363, total_norm: 4.4298095703125, lr: 5.9999999999999995e-05
Step 4530, training loss: 3.9385969638824463, total_norm: 4.541740417480469, lr: 5.9999999999999995e-05
Step 4531, training loss: 3.8253536224365234, total_norm: 4.914128303527832, lr: 5.9999999999999995e-05
Step 4532, training loss: 4.4170427322387695, total_norm: 4.740959644317627, lr: 5.9999999999999995e-05
Step 4533, training loss: 3.7626166343688965, total_norm: 4.564761161804199, lr: 5.9999999999999995e-05
Step 4534, training loss: 3.9789977073669434, total_norm: 4.578615188598633, lr: 5.9999999999999995e-05
Step 4535, training loss: 3.78031063079834, total_norm: 4.149203777313232, lr: 5.9999999999999995e-05
Step 4536, training loss: 3.1455934047698975, total_norm: 3.750246524810791, lr: 5.9999999999999995e-05
Step 4537, training loss: 3.6009676456451416, total_norm: 3.7757761478424072, lr: 5.9999999999999995e-05
Step 4538, training loss: 3.9663543701171875, total_norm: 4.260682582855225, lr: 5.9999999999999995e-05
Step 4539, training loss: 4.029790878295898, total_norm: 4.680459976196289, lr: 5.9999999999999995e-05
Step 4540, training loss: 3.5307769775390625, total_norm: 3.7101855278015137, lr: 5.9999999999999995e-05
Step 4541, training loss: 3.5045528411865234, total_norm: 5.104675769805908, lr: 5.9999999999999995e-05
Step 4542, training loss: 3.5422494411468506, total_norm: 4.140026092529297, lr: 5.9999999999999995e-05
Step 4543, training loss: 4.550583362579346, total_norm: 5.176478385925293, lr: 5.9999999999999995e-05
Step 4544, training loss: 3.810887575149536, total_norm: 4.342493057250977, lr: 5.9999999999999995e-05
Step 4545, training loss: 3.573561906814575, total_norm: 3.5021204948425293, lr: 5.9999999999999995e-05
Step 4546, training loss: 3.99275279045105, total_norm: 4.023521900177002, lr: 5.9999999999999995e-05
Step 4547, training loss: 3.889387607574463, total_norm: 5.756370544433594, lr: 5.9999999999999995e-05
Step 4548, training loss: 3.677340030670166, total_norm: 5.24790620803833, lr: 5.9999999999999995e-05
Step 4549, training loss: 3.894719123840332, total_norm: 4.550222873687744, lr: 5.9999999999999995e-05
Step 4550, training loss: 4.196841239929199, total_norm: 4.6180739402771, lr: 5.9999999999999995e-05
Step 4550, validation loss: 5.574879169464111
Step 4551, training loss: 4.465074062347412, total_norm: 6.83018159866333, lr: 5.9999999999999995e-05
Step 4552, training loss: 4.226373195648193, total_norm: 4.7038092613220215, lr: 5.9999999999999995e-05
Step 4553, training loss: 4.745301723480225, total_norm: 5.1452860832214355, lr: 5.9999999999999995e-05
Step 4554, training loss: 3.9557888507843018, total_norm: 4.45912504196167, lr: 5.9999999999999995e-05
Step 4555, training loss: 4.396646022796631, total_norm: 4.8967790603637695, lr: 5.9999999999999995e-05
Step 4556, training loss: 3.9490017890930176, total_norm: 4.598228454589844, lr: 5.9999999999999995e-05
Step 4557, training loss: 4.577329158782959, total_norm: 5.426729202270508, lr: 5.9999999999999995e-05
Step 4558, training loss: 3.5224926471710205, total_norm: 5.4183268547058105, lr: 5.9999999999999995e-05
Step 4559, training loss: 3.982966661453247, total_norm: 5.490224838256836, lr: 5.9999999999999995e-05
Step 4560, training loss: 3.829589366912842, total_norm: 5.776516914367676, lr: 5.9999999999999995e-05
Step 4561, training loss: 4.334540367126465, total_norm: 5.319462299346924, lr: 5.9999999999999995e-05
Step 4562, training loss: 3.8588500022888184, total_norm: 3.9581799507141113, lr: 5.9999999999999995e-05
Step 4563, training loss: 4.107991695404053, total_norm: 4.73817777633667, lr: 5.9999999999999995e-05
Step 4564, training loss: 3.9264447689056396, total_norm: 5.078009605407715, lr: 5.9999999999999995e-05
Step 4565, training loss: 3.63527774810791, total_norm: 4.838528156280518, lr: 5.9999999999999995e-05
Step 4566, training loss: 3.6223273277282715, total_norm: 4.515120029449463, lr: 5.9999999999999995e-05
Step 4567, training loss: 3.5072853565216064, total_norm: 5.099148750305176, lr: 5.9999999999999995e-05
Step 4568, training loss: 3.442673683166504, total_norm: 4.745212554931641, lr: 5.9999999999999995e-05
Step 4569, training loss: 3.644822120666504, total_norm: 3.9496653079986572, lr: 5.9999999999999995e-05
Step 4570, training loss: 3.429814577102661, total_norm: 3.687983989715576, lr: 5.9999999999999995e-05
Step 4571, training loss: 3.939274311065674, total_norm: 4.843110084533691, lr: 5.9999999999999995e-05
Step 4572, training loss: 3.8383610248565674, total_norm: 4.469926834106445, lr: 5.9999999999999995e-05
Step 4573, training loss: 3.750868320465088, total_norm: 4.163262844085693, lr: 5.9999999999999995e-05
Step 4574, training loss: 4.389620780944824, total_norm: 4.323214054107666, lr: 5.9999999999999995e-05
Step 4575, training loss: 4.027512073516846, total_norm: 3.9795732498168945, lr: 5.9999999999999995e-05
Step 4576, training loss: 4.090399265289307, total_norm: 4.239986896514893, lr: 5.9999999999999995e-05
Step 4577, training loss: 4.423361778259277, total_norm: 4.777283668518066, lr: 5.9999999999999995e-05
Step 4578, training loss: 3.902333974838257, total_norm: 4.98368501663208, lr: 5.9999999999999995e-05
Step 4579, training loss: 4.384877681732178, total_norm: 4.573006629943848, lr: 5.9999999999999995e-05
Step 4580, training loss: 4.296756744384766, total_norm: 5.252325057983398, lr: 5.9999999999999995e-05
Step 4581, training loss: 4.23702335357666, total_norm: 5.173973560333252, lr: 5.9999999999999995e-05
Step 4582, training loss: 4.118491172790527, total_norm: 4.617612838745117, lr: 5.9999999999999995e-05
Step 4583, training loss: 4.068511962890625, total_norm: 4.205256462097168, lr: 5.9999999999999995e-05
Step 4584, training loss: 4.288976669311523, total_norm: 5.018580436706543, lr: 5.9999999999999995e-05
Step 4585, training loss: 3.664245367050171, total_norm: 4.689352512359619, lr: 5.9999999999999995e-05
Step 4586, training loss: 3.318509817123413, total_norm: 4.270364761352539, lr: 5.9999999999999995e-05
Step 4587, training loss: 3.571500539779663, total_norm: 4.65313720703125, lr: 5.9999999999999995e-05
Step 4588, training loss: 4.142978668212891, total_norm: 4.509106636047363, lr: 5.9999999999999995e-05
Step 4589, training loss: 4.092350006103516, total_norm: 3.8151865005493164, lr: 5.9999999999999995e-05
Step 4590, training loss: 4.1303863525390625, total_norm: 4.5495924949646, lr: 5.9999999999999995e-05
Step 4591, training loss: 4.706295013427734, total_norm: 5.490006923675537, lr: 5.9999999999999995e-05
Step 4592, training loss: 4.173715591430664, total_norm: 4.747408866882324, lr: 5.9999999999999995e-05
Step 4593, training loss: 3.884326219558716, total_norm: 6.060546875, lr: 5.9999999999999995e-05
Step 4594, training loss: 4.339379787445068, total_norm: 5.822876930236816, lr: 5.9999999999999995e-05
Step 4595, training loss: 3.921590566635132, total_norm: 4.315062999725342, lr: 5.9999999999999995e-05
Step 4596, training loss: 4.083746433258057, total_norm: 4.385406494140625, lr: 5.9999999999999995e-05
Step 4597, training loss: 4.0352020263671875, total_norm: 4.237768173217773, lr: 5.9999999999999995e-05
Step 4598, training loss: 3.5574724674224854, total_norm: 4.875428199768066, lr: 5.9999999999999995e-05
Step 4599, training loss: 3.4131810665130615, total_norm: 4.3195414543151855, lr: 5.9999999999999995e-05
Step 4600, training loss: 2.5990278720855713, total_norm: 4.78074836730957, lr: 5.9999999999999995e-05
Step 4600, validation loss: 5.5770111083984375
Step 4601, training loss: 2.963402509689331, total_norm: 3.967088222503662, lr: 5.9999999999999995e-05
Step 4602, training loss: 3.2011659145355225, total_norm: 3.7475428581237793, lr: 5.9999999999999995e-05
Step 4603, training loss: 4.477747440338135, total_norm: 4.560290813446045, lr: 5.9999999999999995e-05
Step 4604, training loss: 4.516066551208496, total_norm: 4.713927745819092, lr: 5.9999999999999995e-05
Step 4605, training loss: 3.905735492706299, total_norm: 4.835229873657227, lr: 5.9999999999999995e-05
Step 4606, training loss: 4.397740364074707, total_norm: 4.957344055175781, lr: 5.9999999999999995e-05
Step 4607, training loss: 3.9507956504821777, total_norm: 4.691723823547363, lr: 5.9999999999999995e-05
Step 4608, training loss: 3.8743762969970703, total_norm: 4.721884250640869, lr: 5.9999999999999995e-05
Step 4609, training loss: 4.112859725952148, total_norm: 5.03165864944458, lr: 5.9999999999999995e-05
Step 4610, training loss: 3.6382944583892822, total_norm: 4.713275909423828, lr: 5.9999999999999995e-05
Step 4611, training loss: 3.6204099655151367, total_norm: 3.9818735122680664, lr: 5.9999999999999995e-05
Step 4612, training loss: 3.9411063194274902, total_norm: 5.252871513366699, lr: 5.9999999999999995e-05
Step 4613, training loss: 3.7920079231262207, total_norm: 4.6245317459106445, lr: 5.9999999999999995e-05
Step 4614, training loss: 4.010518550872803, total_norm: 4.615624904632568, lr: 5.9999999999999995e-05
Step 4615, training loss: 3.9593851566314697, total_norm: 5.230732440948486, lr: 5.9999999999999995e-05
Step 4616, training loss: 3.7395172119140625, total_norm: 4.156190872192383, lr: 5.9999999999999995e-05
Step 4617, training loss: 3.665130376815796, total_norm: 4.383424282073975, lr: 5.9999999999999995e-05
Step 4618, training loss: 4.026089668273926, total_norm: 3.779130220413208, lr: 5.9999999999999995e-05
Step 4619, training loss: 3.8036932945251465, total_norm: 3.8224477767944336, lr: 5.9999999999999995e-05
Step 4620, training loss: 3.784264326095581, total_norm: 4.307914733886719, lr: 5.9999999999999995e-05
Step 4621, training loss: 3.9619243144989014, total_norm: 4.575747013092041, lr: 5.9999999999999995e-05
Step 4622, training loss: 3.7521395683288574, total_norm: 4.667191505432129, lr: 5.9999999999999995e-05
Step 4623, training loss: 3.7283012866973877, total_norm: 4.837601661682129, lr: 5.9999999999999995e-05
Step 4624, training loss: 4.043903350830078, total_norm: 6.458599090576172, lr: 5.9999999999999995e-05
Step 4625, training loss: 4.085578918457031, total_norm: 5.43074893951416, lr: 5.9999999999999995e-05
Step 4626, training loss: 3.4943649768829346, total_norm: 3.8697597980499268, lr: 5.9999999999999995e-05
Step 4627, training loss: 3.55308198928833, total_norm: 4.532995700836182, lr: 5.9999999999999995e-05
Step 4628, training loss: 3.729775905609131, total_norm: 4.65317440032959, lr: 5.9999999999999995e-05
Step 4629, training loss: 4.067709922790527, total_norm: 5.6096601486206055, lr: 5.9999999999999995e-05
Step 4630, training loss: 3.927440881729126, total_norm: 5.191346168518066, lr: 5.9999999999999995e-05
Step 4631, training loss: 4.59184455871582, total_norm: 5.219402313232422, lr: 5.9999999999999995e-05
Step 4632, training loss: 3.828530788421631, total_norm: 4.202613353729248, lr: 5.9999999999999995e-05
Step 4633, training loss: 3.5570900440216064, total_norm: 3.8485279083251953, lr: 5.9999999999999995e-05
Step 4634, training loss: 3.1649672985076904, total_norm: 3.7143590450286865, lr: 5.9999999999999995e-05
Step 4635, training loss: 3.290980577468872, total_norm: 3.8763186931610107, lr: 5.9999999999999995e-05
Step 4636, training loss: 3.7954132556915283, total_norm: 5.062121391296387, lr: 5.9999999999999995e-05
Step 4637, training loss: 4.126451015472412, total_norm: 5.607153415679932, lr: 5.9999999999999995e-05
Step 4638, training loss: 4.473250389099121, total_norm: 6.0295538902282715, lr: 5.9999999999999995e-05
Step 4639, training loss: 4.097825050354004, total_norm: 6.162038803100586, lr: 5.9999999999999995e-05
Step 4640, training loss: 4.914546012878418, total_norm: 6.422950267791748, lr: 5.9999999999999995e-05
Step 4641, training loss: 4.175606727600098, total_norm: 5.670260429382324, lr: 5.9999999999999995e-05
Step 4642, training loss: 4.361703872680664, total_norm: 6.007976531982422, lr: 5.9999999999999995e-05
Step 4643, training loss: 4.082257270812988, total_norm: 4.74760627746582, lr: 5.9999999999999995e-05
Step 4644, training loss: 4.437533855438232, total_norm: 5.518764972686768, lr: 5.9999999999999995e-05
Step 4645, training loss: 4.334986209869385, total_norm: 5.824560165405273, lr: 5.9999999999999995e-05
Step 4646, training loss: 4.179078102111816, total_norm: 5.319214344024658, lr: 5.9999999999999995e-05
Step 4647, training loss: 4.06481409072876, total_norm: 5.047420501708984, lr: 5.9999999999999995e-05
Step 4648, training loss: 4.459975242614746, total_norm: 5.142172813415527, lr: 5.9999999999999995e-05
Step 4649, training loss: 4.338504791259766, total_norm: 4.490630626678467, lr: 5.9999999999999995e-05
Step 4650, training loss: 3.884798526763916, total_norm: 4.337543487548828, lr: 5.9999999999999995e-05
Step 4650, validation loss: 5.455472469329834
Step 4651, training loss: 3.843421220779419, total_norm: 4.040089130401611, lr: 5.9999999999999995e-05
Step 4652, training loss: 4.253649711608887, total_norm: 3.8836846351623535, lr: 5.9999999999999995e-05
Step 4653, training loss: 3.6524229049682617, total_norm: 4.367628574371338, lr: 5.9999999999999995e-05
Step 4654, training loss: 3.93351674079895, total_norm: 4.199755668640137, lr: 5.9999999999999995e-05
Step 4655, training loss: 4.254508972167969, total_norm: 4.420032501220703, lr: 5.9999999999999995e-05
Step 4656, training loss: 3.948632001876831, total_norm: 4.539555549621582, lr: 5.9999999999999995e-05
Step 4657, training loss: 4.093653678894043, total_norm: 3.723276138305664, lr: 5.9999999999999995e-05
Step 4658, training loss: 3.9996867179870605, total_norm: 4.956552982330322, lr: 5.9999999999999995e-05
Step 4659, training loss: 4.247750282287598, total_norm: 4.8525614738464355, lr: 5.9999999999999995e-05
Step 4660, training loss: 3.9379332065582275, total_norm: 3.95192551612854, lr: 5.9999999999999995e-05
Step 4661, training loss: 3.618541717529297, total_norm: 3.9765775203704834, lr: 5.9999999999999995e-05
Step 4662, training loss: 3.5978150367736816, total_norm: 3.932448387145996, lr: 5.9999999999999995e-05
Step 4663, training loss: 3.9598121643066406, total_norm: 4.517942905426025, lr: 5.9999999999999995e-05
Step 4664, training loss: 3.822258234024048, total_norm: 3.830084800720215, lr: 5.9999999999999995e-05
Step 4665, training loss: 4.243980407714844, total_norm: 4.642404079437256, lr: 5.9999999999999995e-05
Step 4666, training loss: 4.372249126434326, total_norm: 4.351548194885254, lr: 5.9999999999999995e-05
Step 4667, training loss: 4.195487022399902, total_norm: 3.8598971366882324, lr: 5.9999999999999995e-05
Step 4668, training loss: 4.2735185623168945, total_norm: 3.7148525714874268, lr: 5.9999999999999995e-05
Step 4669, training loss: 3.6280059814453125, total_norm: 4.181708812713623, lr: 5.9999999999999995e-05
Step 4670, training loss: 4.527218341827393, total_norm: 4.823076248168945, lr: 5.9999999999999995e-05
Step 4671, training loss: 4.094808101654053, total_norm: 3.847367525100708, lr: 5.9999999999999995e-05
Step 4672, training loss: 3.910085439682007, total_norm: 3.7562901973724365, lr: 5.9999999999999995e-05
Step 4673, training loss: 4.79603385925293, total_norm: 4.2459821701049805, lr: 5.9999999999999995e-05
Step 4674, training loss: 4.974613666534424, total_norm: 4.976210117340088, lr: 5.9999999999999995e-05
Step 4675, training loss: 4.3966593742370605, total_norm: 4.9729413986206055, lr: 5.9999999999999995e-05
Step 4676, training loss: 4.5690155029296875, total_norm: 5.154208660125732, lr: 5.9999999999999995e-05
Step 4677, training loss: 4.5829691886901855, total_norm: 5.416010856628418, lr: 5.9999999999999995e-05
Step 4678, training loss: 4.803092002868652, total_norm: 5.321038722991943, lr: 5.9999999999999995e-05
Step 4679, training loss: 4.423193454742432, total_norm: 4.722776889801025, lr: 5.9999999999999995e-05
Step 4680, training loss: 3.948208808898926, total_norm: 4.465079307556152, lr: 5.9999999999999995e-05
Step 4681, training loss: 3.9838147163391113, total_norm: 4.482989311218262, lr: 5.9999999999999995e-05
Step 4682, training loss: 4.4120001792907715, total_norm: 4.703352928161621, lr: 5.9999999999999995e-05
Step 4683, training loss: 4.447759628295898, total_norm: 4.909932613372803, lr: 5.9999999999999995e-05
Step 4684, training loss: 4.672014236450195, total_norm: 5.40994930267334, lr: 5.9999999999999995e-05
Step 4685, training loss: 3.925795316696167, total_norm: 4.115253925323486, lr: 5.9999999999999995e-05
Step 4686, training loss: 4.734458923339844, total_norm: 5.405843257904053, lr: 5.9999999999999995e-05
Step 4687, training loss: 4.4726176261901855, total_norm: 4.180013179779053, lr: 5.9999999999999995e-05
Step 4688, training loss: 4.012180805206299, total_norm: 4.144929885864258, lr: 5.9999999999999995e-05
Step 4689, training loss: 3.6496267318725586, total_norm: 4.82151460647583, lr: 5.9999999999999995e-05
Step 4690, training loss: 4.510378360748291, total_norm: 5.4971604347229, lr: 5.9999999999999995e-05
Step 4691, training loss: 3.7488760948181152, total_norm: 5.466826915740967, lr: 5.9999999999999995e-05
Step 4692, training loss: 3.668728828430176, total_norm: 4.535885334014893, lr: 5.9999999999999995e-05
Step 4693, training loss: 4.367880821228027, total_norm: 4.2829508781433105, lr: 5.9999999999999995e-05
Step 4694, training loss: 3.9779467582702637, total_norm: 4.511967658996582, lr: 5.9999999999999995e-05
Step 4695, training loss: 3.905630111694336, total_norm: 3.9257612228393555, lr: 5.9999999999999995e-05
Step 4696, training loss: 4.061789035797119, total_norm: 4.422953128814697, lr: 5.9999999999999995e-05
Step 4697, training loss: 4.582028388977051, total_norm: 5.204105854034424, lr: 5.9999999999999995e-05
Step 4698, training loss: 3.667391300201416, total_norm: 4.879847049713135, lr: 5.9999999999999995e-05
Step 4699, training loss: 3.887911558151245, total_norm: 4.862329959869385, lr: 5.9999999999999995e-05
Step 4700, training loss: 3.8491930961608887, total_norm: 4.397061824798584, lr: 5.9999999999999995e-05
Step 4700, validation loss: 5.315197467803955
Step 4701, training loss: 3.8805365562438965, total_norm: 4.4198689460754395, lr: 5.9999999999999995e-05
Step 4702, training loss: 4.764071464538574, total_norm: 5.565971374511719, lr: 5.9999999999999995e-05
Step 4703, training loss: 3.8583405017852783, total_norm: 4.614502429962158, lr: 5.9999999999999995e-05
Step 4704, training loss: 4.301716327667236, total_norm: 4.862759590148926, lr: 5.9999999999999995e-05
Step 4705, training loss: 3.979370355606079, total_norm: 4.404653549194336, lr: 5.9999999999999995e-05
Step 4706, training loss: 4.008464336395264, total_norm: 4.294203758239746, lr: 5.9999999999999995e-05
Step 4707, training loss: 4.334149360656738, total_norm: 4.202788352966309, lr: 5.9999999999999995e-05
Step 4708, training loss: 4.347987651824951, total_norm: 4.492539405822754, lr: 5.9999999999999995e-05
Step 4709, training loss: 3.813047409057617, total_norm: 4.533323764801025, lr: 5.9999999999999995e-05
Step 4710, training loss: 4.680925369262695, total_norm: 5.87821626663208, lr: 5.9999999999999995e-05
Step 4711, training loss: 4.910937309265137, total_norm: 5.893400192260742, lr: 5.9999999999999995e-05
Step 4712, training loss: 4.76866340637207, total_norm: 4.905019760131836, lr: 5.9999999999999995e-05
Step 4713, training loss: 4.080084800720215, total_norm: 4.743906021118164, lr: 5.9999999999999995e-05
Step 4714, training loss: 3.6202940940856934, total_norm: 4.702109336853027, lr: 5.9999999999999995e-05
Step 4715, training loss: 4.263950347900391, total_norm: 4.48565149307251, lr: 5.9999999999999995e-05
Step 4716, training loss: 3.860093355178833, total_norm: 4.404188632965088, lr: 5.9999999999999995e-05
Step 4717, training loss: 4.0296711921691895, total_norm: 4.728489398956299, lr: 5.9999999999999995e-05
Step 4718, training loss: 4.499619960784912, total_norm: 6.5411200523376465, lr: 5.9999999999999995e-05
Step 4719, training loss: 4.5818963050842285, total_norm: 6.326968669891357, lr: 5.9999999999999995e-05
Step 4720, training loss: 4.046347141265869, total_norm: 5.523076057434082, lr: 5.9999999999999995e-05
Step 4721, training loss: 4.079547882080078, total_norm: 5.222592830657959, lr: 5.9999999999999995e-05
Step 4722, training loss: 4.034887313842773, total_norm: 7.394810676574707, lr: 5.9999999999999995e-05
Step 4723, training loss: 4.2218337059021, total_norm: 7.738028049468994, lr: 5.9999999999999995e-05
Step 4724, training loss: 4.386905193328857, total_norm: 5.345938682556152, lr: 5.9999999999999995e-05
Step 4725, training loss: 4.544459342956543, total_norm: 7.539577007293701, lr: 5.9999999999999995e-05
Step 4726, training loss: 4.789126873016357, total_norm: 6.82180643081665, lr: 5.9999999999999995e-05
Step 4727, training loss: 3.9777915477752686, total_norm: 5.980279922485352, lr: 5.9999999999999995e-05
Step 4728, training loss: 4.307766437530518, total_norm: 4.652009010314941, lr: 5.9999999999999995e-05
Step 4729, training loss: 3.963953733444214, total_norm: 3.8103792667388916, lr: 5.9999999999999995e-05
Step 4730, training loss: 3.9401357173919678, total_norm: 4.965185642242432, lr: 5.9999999999999995e-05
Step 4731, training loss: 3.7964301109313965, total_norm: 5.687761306762695, lr: 5.9999999999999995e-05
Step 4732, training loss: 4.056641578674316, total_norm: 5.923032283782959, lr: 5.9999999999999995e-05
Step 4733, training loss: 3.196298599243164, total_norm: 4.218253135681152, lr: 5.9999999999999995e-05
Step 4734, training loss: 3.885787010192871, total_norm: 5.121812343597412, lr: 5.9999999999999995e-05
Step 4735, training loss: 3.828033447265625, total_norm: 5.415975570678711, lr: 5.9999999999999995e-05
Step 4736, training loss: 3.822664737701416, total_norm: 5.084238529205322, lr: 5.9999999999999995e-05
Step 4737, training loss: 3.718308925628662, total_norm: 5.265442848205566, lr: 5.9999999999999995e-05
Step 4738, training loss: 3.2896432876586914, total_norm: 4.036566734313965, lr: 5.9999999999999995e-05
Step 4739, training loss: 3.840794324874878, total_norm: 4.127684593200684, lr: 5.9999999999999995e-05
Step 4740, training loss: 4.139791965484619, total_norm: 4.8382391929626465, lr: 5.9999999999999995e-05
Step 4741, training loss: 3.925339698791504, total_norm: 4.692307472229004, lr: 5.9999999999999995e-05
Step 4742, training loss: 3.6522176265716553, total_norm: 6.267505645751953, lr: 5.9999999999999995e-05
Step 4743, training loss: 4.174862384796143, total_norm: 4.088982105255127, lr: 5.9999999999999995e-05
Step 4744, training loss: 3.7150750160217285, total_norm: 4.183500289916992, lr: 5.9999999999999995e-05
Step 4745, training loss: 3.642763376235962, total_norm: 3.9170825481414795, lr: 5.9999999999999995e-05
Step 4746, training loss: 3.674666404724121, total_norm: 3.8846590518951416, lr: 5.9999999999999995e-05
Step 4747, training loss: 4.087170600891113, total_norm: 4.243479251861572, lr: 5.9999999999999995e-05
Step 4748, training loss: 4.333284854888916, total_norm: 4.385091781616211, lr: 5.9999999999999995e-05
Step 4749, training loss: 3.539970636367798, total_norm: 3.9196999073028564, lr: 5.9999999999999995e-05
Step 4750, training loss: 3.7545883655548096, total_norm: 3.8382489681243896, lr: 5.9999999999999995e-05
Step 4750, validation loss: 5.108172416687012
Step 4751, training loss: 3.910892963409424, total_norm: 3.969172716140747, lr: 5.9999999999999995e-05
Step 4752, training loss: 4.254933834075928, total_norm: 5.092466831207275, lr: 5.9999999999999995e-05
Step 4753, training loss: 4.3689188957214355, total_norm: 4.749025344848633, lr: 5.9999999999999995e-05
Step 4754, training loss: 3.9589576721191406, total_norm: 4.100530624389648, lr: 5.9999999999999995e-05
Step 4755, training loss: 3.9176290035247803, total_norm: 4.104166507720947, lr: 5.9999999999999995e-05
Step 4756, training loss: 4.483604431152344, total_norm: 4.63116979598999, lr: 5.9999999999999995e-05
Step 4757, training loss: 3.9442648887634277, total_norm: 4.323520183563232, lr: 5.9999999999999995e-05
Step 4758, training loss: 3.710838794708252, total_norm: 5.001245498657227, lr: 5.9999999999999995e-05
Step 4759, training loss: 4.035398960113525, total_norm: 5.398533821105957, lr: 5.9999999999999995e-05
Step 4760, training loss: 4.252435207366943, total_norm: 4.894241809844971, lr: 5.9999999999999995e-05
Step 4761, training loss: 3.8686020374298096, total_norm: 4.729600429534912, lr: 5.9999999999999995e-05
Step 4762, training loss: 3.748335838317871, total_norm: 5.065545558929443, lr: 5.9999999999999995e-05
Step 4763, training loss: 3.4483907222747803, total_norm: 4.172296047210693, lr: 5.9999999999999995e-05
Step 4764, training loss: 4.169017314910889, total_norm: 4.043165683746338, lr: 5.9999999999999995e-05
Step 4765, training loss: 3.7998270988464355, total_norm: 4.022974491119385, lr: 5.9999999999999995e-05
Step 4766, training loss: 3.8820953369140625, total_norm: 4.193453788757324, lr: 5.9999999999999995e-05
Step 4767, training loss: 3.47796893119812, total_norm: 3.6864447593688965, lr: 5.9999999999999995e-05
Step 4768, training loss: 3.9662373065948486, total_norm: 3.9952852725982666, lr: 5.9999999999999995e-05
Step 4769, training loss: 3.8677642345428467, total_norm: 4.4361090660095215, lr: 5.9999999999999995e-05
Step 4770, training loss: 4.1220502853393555, total_norm: 3.9762823581695557, lr: 5.9999999999999995e-05
Step 4771, training loss: 3.412557363510132, total_norm: 4.011871814727783, lr: 5.9999999999999995e-05
Step 4772, training loss: 3.7636289596557617, total_norm: 3.972651958465576, lr: 5.9999999999999995e-05
Step 4773, training loss: 3.614684820175171, total_norm: 4.533447265625, lr: 5.9999999999999995e-05
Step 4774, training loss: 4.889195442199707, total_norm: 6.728085517883301, lr: 5.9999999999999995e-05
Step 4775, training loss: 4.070430278778076, total_norm: 4.565743923187256, lr: 5.9999999999999995e-05
Step 4776, training loss: 3.6598243713378906, total_norm: 4.1025238037109375, lr: 5.9999999999999995e-05
Step 4777, training loss: 3.6752724647521973, total_norm: 4.890843391418457, lr: 5.9999999999999995e-05
Step 4778, training loss: 4.262353897094727, total_norm: 5.2451043128967285, lr: 5.9999999999999995e-05
Step 4779, training loss: 3.9021871089935303, total_norm: 4.482712268829346, lr: 5.9999999999999995e-05
Step 4780, training loss: 4.32790994644165, total_norm: 5.45608377456665, lr: 5.9999999999999995e-05
Step 4781, training loss: 3.7949562072753906, total_norm: 4.64426326751709, lr: 5.9999999999999995e-05
Step 4782, training loss: 4.412271976470947, total_norm: 5.85907506942749, lr: 5.9999999999999995e-05
Step 4783, training loss: 3.6159720420837402, total_norm: 4.335414886474609, lr: 5.9999999999999995e-05
Step 4784, training loss: 4.059929847717285, total_norm: 4.580501556396484, lr: 5.9999999999999995e-05
Step 4785, training loss: 3.8481051921844482, total_norm: 4.019479751586914, lr: 5.9999999999999995e-05
Step 4786, training loss: 3.31643009185791, total_norm: 4.129152774810791, lr: 5.9999999999999995e-05
Step 4787, training loss: 4.189455032348633, total_norm: 4.711777210235596, lr: 5.9999999999999995e-05
Step 4788, training loss: 3.092170000076294, total_norm: 4.147226333618164, lr: 5.9999999999999995e-05
Step 4789, training loss: 4.0069122314453125, total_norm: 4.749641418457031, lr: 5.9999999999999995e-05
Step 4790, training loss: 4.361370086669922, total_norm: 5.014328479766846, lr: 5.9999999999999995e-05
Step 4791, training loss: 3.2597925662994385, total_norm: 3.7323455810546875, lr: 5.9999999999999995e-05
Step 4792, training loss: 2.9826409816741943, total_norm: 3.9225571155548096, lr: 5.9999999999999995e-05
Step 4793, training loss: 3.4827380180358887, total_norm: 3.8442633152008057, lr: 5.9999999999999995e-05
Step 4794, training loss: 3.899724245071411, total_norm: 4.526884078979492, lr: 5.9999999999999995e-05
Step 4795, training loss: 4.336030960083008, total_norm: 4.799440860748291, lr: 5.9999999999999995e-05
Step 4796, training loss: 3.5392403602600098, total_norm: 4.809010028839111, lr: 5.9999999999999995e-05
Step 4797, training loss: 3.1104156970977783, total_norm: 4.584901809692383, lr: 5.9999999999999995e-05
Step 4798, training loss: 3.2427728176116943, total_norm: 4.509870529174805, lr: 5.9999999999999995e-05
Step 4799, training loss: 3.0809314250946045, total_norm: 3.795576810836792, lr: 5.9999999999999995e-05
Step 4800, training loss: 3.811232328414917, total_norm: 4.280744552612305, lr: 5.9999999999999995e-05
Step 4800, validation loss: 5.37139892578125
Step 4801, training loss: 3.614010810852051, total_norm: 4.154993534088135, lr: 5.9999999999999995e-05
Step 4802, training loss: 3.222447633743286, total_norm: 3.579437494277954, lr: 5.9999999999999995e-05
Step 4803, training loss: 4.124574661254883, total_norm: 4.3782548904418945, lr: 5.9999999999999995e-05
Step 4804, training loss: 3.8459513187408447, total_norm: 4.806976795196533, lr: 5.9999999999999995e-05
Step 4805, training loss: 3.447713851928711, total_norm: 4.934756755828857, lr: 5.9999999999999995e-05
Step 4806, training loss: 3.346907138824463, total_norm: 4.459228038787842, lr: 5.9999999999999995e-05
Step 4807, training loss: 3.4820611476898193, total_norm: 4.158666610717773, lr: 5.9999999999999995e-05
Step 4808, training loss: 3.268301010131836, total_norm: 3.916682243347168, lr: 5.9999999999999995e-05
Step 4809, training loss: 3.9726297855377197, total_norm: 4.422945022583008, lr: 5.9999999999999995e-05
Step 4810, training loss: 4.062125205993652, total_norm: 4.257070064544678, lr: 5.9999999999999995e-05
Step 4811, training loss: 3.049981117248535, total_norm: 3.580702781677246, lr: 5.9999999999999995e-05
Step 4812, training loss: 3.1708507537841797, total_norm: 4.090468406677246, lr: 5.9999999999999995e-05
Step 4813, training loss: 4.374171257019043, total_norm: 6.0360636711120605, lr: 5.9999999999999995e-05
Step 4814, training loss: 4.012592315673828, total_norm: 4.790805816650391, lr: 5.9999999999999995e-05
Step 4815, training loss: 3.1659505367279053, total_norm: 9.22584342956543, lr: 5.9999999999999995e-05
Step 4816, training loss: 3.193610191345215, total_norm: 5.266005992889404, lr: 5.9999999999999995e-05
Step 4817, training loss: 4.535138130187988, total_norm: 4.710371017456055, lr: 5.9999999999999995e-05
Step 4818, training loss: 4.229122638702393, total_norm: 4.848898410797119, lr: 5.9999999999999995e-05
Step 4819, training loss: 3.3734726905822754, total_norm: 4.463325023651123, lr: 5.9999999999999995e-05
Step 4820, training loss: 4.191516876220703, total_norm: 5.036242485046387, lr: 5.9999999999999995e-05
Step 4821, training loss: 3.610316038131714, total_norm: 5.569888591766357, lr: 5.9999999999999995e-05
Step 4822, training loss: 3.4650187492370605, total_norm: 5.919267177581787, lr: 5.9999999999999995e-05
Step 4823, training loss: 3.2988264560699463, total_norm: 4.916258811950684, lr: 5.9999999999999995e-05
Step 4824, training loss: 3.734074831008911, total_norm: 4.4322509765625, lr: 5.9999999999999995e-05
Step 4825, training loss: 3.4688937664031982, total_norm: 3.8105664253234863, lr: 5.9999999999999995e-05
Step 4826, training loss: 4.01942253112793, total_norm: 4.394023895263672, lr: 5.9999999999999995e-05
Step 4827, training loss: 3.851017713546753, total_norm: 4.3051676750183105, lr: 5.9999999999999995e-05
Step 4828, training loss: 3.5493147373199463, total_norm: 3.903550148010254, lr: 5.9999999999999995e-05
Step 4829, training loss: 3.169297456741333, total_norm: 4.090944290161133, lr: 5.9999999999999995e-05
Step 4830, training loss: 4.229762554168701, total_norm: 5.2485198974609375, lr: 5.9999999999999995e-05
Step 4831, training loss: 4.26520299911499, total_norm: 5.0016350746154785, lr: 5.9999999999999995e-05
Step 4832, training loss: 3.803534746170044, total_norm: 4.351969242095947, lr: 5.9999999999999995e-05
Step 4833, training loss: 4.2280120849609375, total_norm: 4.529033184051514, lr: 5.9999999999999995e-05
Step 4834, training loss: 3.727487087249756, total_norm: 4.208180904388428, lr: 5.9999999999999995e-05
Step 4835, training loss: 4.254943370819092, total_norm: 4.6010823249816895, lr: 5.9999999999999995e-05
Step 4836, training loss: 4.3733906745910645, total_norm: 4.692195415496826, lr: 5.9999999999999995e-05
Step 4837, training loss: 3.5208959579467773, total_norm: 4.109576225280762, lr: 5.9999999999999995e-05
Step 4838, training loss: 4.112936973571777, total_norm: 4.788305282592773, lr: 5.9999999999999995e-05
Step 4839, training loss: 3.9636688232421875, total_norm: 4.317959785461426, lr: 5.9999999999999995e-05
Step 4840, training loss: 3.86417818069458, total_norm: 4.610018730163574, lr: 5.9999999999999995e-05
Step 4841, training loss: 3.7880876064300537, total_norm: 4.255418300628662, lr: 5.9999999999999995e-05
Step 4842, training loss: 3.5503742694854736, total_norm: 4.001275539398193, lr: 5.9999999999999995e-05
Step 4843, training loss: 3.757075786590576, total_norm: 4.231391429901123, lr: 5.9999999999999995e-05
Step 4844, training loss: 4.813789367675781, total_norm: 6.0483598709106445, lr: 5.9999999999999995e-05
Step 4845, training loss: 4.061817646026611, total_norm: 4.511657238006592, lr: 5.9999999999999995e-05
Step 4846, training loss: 3.894788980484009, total_norm: 4.7824177742004395, lr: 5.9999999999999995e-05
Step 4847, training loss: 3.682476282119751, total_norm: 3.982954978942871, lr: 5.9999999999999995e-05
Step 4848, training loss: 4.317553520202637, total_norm: 4.2167582511901855, lr: 5.9999999999999995e-05
Step 4849, training loss: 4.0576701164245605, total_norm: 4.218606948852539, lr: 5.9999999999999995e-05
Step 4850, training loss: 3.311607599258423, total_norm: 3.9493680000305176, lr: 5.9999999999999995e-05
Step 4850, validation loss: 5.333406925201416
Step 4851, training loss: 3.0625758171081543, total_norm: 3.8461573123931885, lr: 5.9999999999999995e-05
Step 4852, training loss: 3.2198572158813477, total_norm: 4.0793328285217285, lr: 5.9999999999999995e-05
Step 4853, training loss: 3.1162843704223633, total_norm: 3.6037437915802, lr: 5.9999999999999995e-05
Step 4854, training loss: 3.6510066986083984, total_norm: 4.361985683441162, lr: 5.9999999999999995e-05
Step 4855, training loss: 3.9024465084075928, total_norm: 3.902186393737793, lr: 5.9999999999999995e-05
Step 4856, training loss: 3.752592086791992, total_norm: 4.629749774932861, lr: 5.9999999999999995e-05
Step 4857, training loss: 4.041753768920898, total_norm: 4.522385120391846, lr: 5.9999999999999995e-05
Step 4858, training loss: 3.6116864681243896, total_norm: 3.7918848991394043, lr: 5.9999999999999995e-05
Step 4859, training loss: 3.4696691036224365, total_norm: 3.905160427093506, lr: 5.9999999999999995e-05
Step 4860, training loss: 3.73458194732666, total_norm: 4.292120933532715, lr: 5.9999999999999995e-05
Step 4861, training loss: 3.8597779273986816, total_norm: 5.056641578674316, lr: 5.9999999999999995e-05
Step 4862, training loss: 3.392397165298462, total_norm: 3.7838079929351807, lr: 5.9999999999999995e-05
Step 4863, training loss: 3.6121621131896973, total_norm: 4.365900993347168, lr: 5.9999999999999995e-05
Step 4864, training loss: 3.5388453006744385, total_norm: 3.7790653705596924, lr: 5.9999999999999995e-05
Step 4865, training loss: 3.8185324668884277, total_norm: 4.320321559906006, lr: 5.9999999999999995e-05
Step 4866, training loss: 4.4736456871032715, total_norm: 4.857727527618408, lr: 5.9999999999999995e-05
Step 4867, training loss: 3.6881704330444336, total_norm: 4.401709079742432, lr: 5.9999999999999995e-05
Step 4868, training loss: 3.8427319526672363, total_norm: 5.414380073547363, lr: 5.9999999999999995e-05
Step 4869, training loss: 3.339021682739258, total_norm: 4.89810848236084, lr: 5.9999999999999995e-05
Step 4870, training loss: 3.9031572341918945, total_norm: 5.569976806640625, lr: 5.9999999999999995e-05
Step 4871, training loss: 3.3979365825653076, total_norm: 4.7486677169799805, lr: 5.9999999999999995e-05
Step 4872, training loss: 3.6141538619995117, total_norm: 4.272314071655273, lr: 5.9999999999999995e-05
Step 4873, training loss: 3.744243860244751, total_norm: 4.7380547523498535, lr: 5.9999999999999995e-05
Step 4874, training loss: 3.585118532180786, total_norm: 4.189512252807617, lr: 5.9999999999999995e-05
Step 4875, training loss: 4.025193214416504, total_norm: 4.590808391571045, lr: 5.9999999999999995e-05
Step 4876, training loss: 3.806659460067749, total_norm: 5.079259395599365, lr: 5.9999999999999995e-05
Step 4877, training loss: 3.8868284225463867, total_norm: 5.247031211853027, lr: 5.9999999999999995e-05
Step 4878, training loss: 4.010516166687012, total_norm: 5.581044673919678, lr: 5.9999999999999995e-05
Step 4879, training loss: 3.982541084289551, total_norm: 5.516261577606201, lr: 5.9999999999999995e-05
Step 4880, training loss: 3.580827474594116, total_norm: 4.850095272064209, lr: 5.9999999999999995e-05
Step 4881, training loss: 3.8541359901428223, total_norm: 4.311925411224365, lr: 5.9999999999999995e-05
Step 4882, training loss: 3.301140546798706, total_norm: 4.658527374267578, lr: 5.9999999999999995e-05
Step 4883, training loss: 3.355316400527954, total_norm: 4.314802646636963, lr: 5.9999999999999995e-05
Step 4884, training loss: 3.5217843055725098, total_norm: 4.570159912109375, lr: 5.9999999999999995e-05
Step 4885, training loss: 3.7350080013275146, total_norm: 4.547223091125488, lr: 5.9999999999999995e-05
Step 4886, training loss: 2.9770193099975586, total_norm: 3.9439358711242676, lr: 5.9999999999999995e-05
Step 4887, training loss: 2.9697012901306152, total_norm: 3.6491549015045166, lr: 5.9999999999999995e-05
Step 4888, training loss: 3.50689435005188, total_norm: 4.721804618835449, lr: 5.9999999999999995e-05
Step 4889, training loss: 3.283806085586548, total_norm: 4.255838394165039, lr: 5.9999999999999995e-05
Step 4890, training loss: 3.697275161743164, total_norm: 4.446625709533691, lr: 5.9999999999999995e-05
Step 4891, training loss: 3.491365432739258, total_norm: 4.463592052459717, lr: 5.9999999999999995e-05
Step 4892, training loss: 3.3653383255004883, total_norm: 4.495907783508301, lr: 5.9999999999999995e-05
Step 4893, training loss: 3.5633065700531006, total_norm: 4.918148040771484, lr: 5.9999999999999995e-05
Step 4894, training loss: 3.093757390975952, total_norm: 4.253137111663818, lr: 5.9999999999999995e-05
Step 4895, training loss: 3.703564405441284, total_norm: 4.81941556930542, lr: 5.9999999999999995e-05
Step 4896, training loss: 3.8240787982940674, total_norm: 4.901639461517334, lr: 5.9999999999999995e-05
Step 4897, training loss: 3.5920419692993164, total_norm: 3.8682851791381836, lr: 5.9999999999999995e-05
Step 4898, training loss: 3.992177724838257, total_norm: 4.4383320808410645, lr: 5.9999999999999995e-05
Step 4899, training loss: 4.098186016082764, total_norm: 4.974099636077881, lr: 5.9999999999999995e-05
Step 4900, training loss: 3.961068868637085, total_norm: 5.518533706665039, lr: 5.9999999999999995e-05
Step 4900, validation loss: 5.602617263793945
Step 4901, training loss: 3.5847771167755127, total_norm: 5.301211357116699, lr: 5.9999999999999995e-05
Step 4902, training loss: 3.9214062690734863, total_norm: 5.264215469360352, lr: 5.9999999999999995e-05
Step 4903, training loss: 4.220767974853516, total_norm: 4.217557907104492, lr: 5.9999999999999995e-05
Step 4904, training loss: 3.913637399673462, total_norm: 4.443354606628418, lr: 5.9999999999999995e-05
Step 4905, training loss: 3.2549796104431152, total_norm: 4.0823588371276855, lr: 5.9999999999999995e-05
Step 4906, training loss: 3.3252620697021484, total_norm: 4.860848426818848, lr: 5.9999999999999995e-05
Step 4907, training loss: 3.885096788406372, total_norm: 4.678496360778809, lr: 5.9999999999999995e-05
Step 4908, training loss: 3.400911569595337, total_norm: 4.35209846496582, lr: 5.9999999999999995e-05
Step 4909, training loss: 3.757725954055786, total_norm: 4.914478778839111, lr: 5.9999999999999995e-05
Step 4910, training loss: 3.4369094371795654, total_norm: 4.329479694366455, lr: 5.9999999999999995e-05
Step 4911, training loss: 3.0752174854278564, total_norm: 4.242947101593018, lr: 5.9999999999999995e-05
Step 4912, training loss: 4.0812668800354, total_norm: 4.636242866516113, lr: 5.9999999999999995e-05
Step 4913, training loss: 4.0929083824157715, total_norm: 5.377668380737305, lr: 5.9999999999999995e-05
Step 4914, training loss: 3.8282337188720703, total_norm: 5.464859485626221, lr: 5.9999999999999995e-05
Step 4915, training loss: 4.224313259124756, total_norm: 5.836215496063232, lr: 5.9999999999999995e-05
Step 4916, training loss: 3.9808430671691895, total_norm: 5.538618564605713, lr: 5.9999999999999995e-05
Step 4917, training loss: 3.331653118133545, total_norm: 4.567215442657471, lr: 5.9999999999999995e-05
Step 4918, training loss: 3.5683977603912354, total_norm: 5.25067663192749, lr: 5.9999999999999995e-05
Step 4919, training loss: 3.4010910987854004, total_norm: 4.8542561531066895, lr: 5.9999999999999995e-05
Step 4920, training loss: 2.9885852336883545, total_norm: 3.9792816638946533, lr: 5.9999999999999995e-05
Step 4921, training loss: 3.2882282733917236, total_norm: 3.948744058609009, lr: 5.9999999999999995e-05
Step 4922, training loss: 4.353453159332275, total_norm: 5.017995834350586, lr: 5.9999999999999995e-05
Step 4923, training loss: 2.618088722229004, total_norm: 4.024718761444092, lr: 5.9999999999999995e-05
Step 4924, training loss: 3.4656190872192383, total_norm: 4.81807279586792, lr: 5.9999999999999995e-05
Step 4925, training loss: 3.491641044616699, total_norm: 4.9343671798706055, lr: 5.9999999999999995e-05
Step 4926, training loss: 3.2304418087005615, total_norm: 4.9392409324646, lr: 5.9999999999999995e-05
Step 4927, training loss: 3.5153005123138428, total_norm: 4.765317916870117, lr: 5.9999999999999995e-05
Step 4928, training loss: 4.169092178344727, total_norm: 5.506381988525391, lr: 5.9999999999999995e-05
Step 4929, training loss: 4.34050178527832, total_norm: 6.877758026123047, lr: 5.9999999999999995e-05
Step 4930, training loss: 4.0439653396606445, total_norm: 6.298263072967529, lr: 5.9999999999999995e-05
Step 4931, training loss: 4.084466934204102, total_norm: 6.489744663238525, lr: 5.9999999999999995e-05
Step 4932, training loss: 3.5029890537261963, total_norm: 7.832140922546387, lr: 5.9999999999999995e-05
Step 4933, training loss: 4.435194969177246, total_norm: 9.044660568237305, lr: 5.9999999999999995e-05
Step 4934, training loss: 3.9648613929748535, total_norm: 6.940089702606201, lr: 5.9999999999999995e-05
Step 4935, training loss: 4.054389476776123, total_norm: 6.124760627746582, lr: 5.9999999999999995e-05
Step 4936, training loss: 3.5770041942596436, total_norm: 5.430513381958008, lr: 5.9999999999999995e-05
Step 4937, training loss: 3.7106029987335205, total_norm: 5.295059680938721, lr: 5.9999999999999995e-05
Step 4938, training loss: 4.272208213806152, total_norm: 5.251295566558838, lr: 5.9999999999999995e-05
Step 4939, training loss: 4.127147197723389, total_norm: 5.392600059509277, lr: 5.9999999999999995e-05
Step 4940, training loss: 4.255617141723633, total_norm: 5.532614231109619, lr: 5.9999999999999995e-05
Step 4941, training loss: 4.14833927154541, total_norm: 6.5903778076171875, lr: 5.9999999999999995e-05
Step 4942, training loss: 4.20702600479126, total_norm: 6.161754131317139, lr: 5.9999999999999995e-05
Step 4943, training loss: 4.210796356201172, total_norm: 5.760859966278076, lr: 5.9999999999999995e-05
Step 4944, training loss: 4.314040184020996, total_norm: 5.224572658538818, lr: 5.9999999999999995e-05
Step 4945, training loss: 4.090576171875, total_norm: 5.128096580505371, lr: 5.9999999999999995e-05
Step 4946, training loss: 3.953636407852173, total_norm: 5.066264629364014, lr: 5.9999999999999995e-05
Step 4947, training loss: 3.9036312103271484, total_norm: 4.295435428619385, lr: 5.9999999999999995e-05
Step 4948, training loss: 4.270411491394043, total_norm: 5.798135280609131, lr: 5.9999999999999995e-05
Step 4949, training loss: 4.385189533233643, total_norm: 5.443792819976807, lr: 5.9999999999999995e-05
Step 4950, training loss: 3.8387293815612793, total_norm: 4.962884902954102, lr: 5.9999999999999995e-05
Step 4950, validation loss: 5.54659366607666
Step 4951, training loss: 3.909843921661377, total_norm: 4.589311599731445, lr: 5.9999999999999995e-05
Step 4952, training loss: 3.344818115234375, total_norm: 4.149113178253174, lr: 5.9999999999999995e-05
Step 4953, training loss: 3.834606170654297, total_norm: 4.076186656951904, lr: 5.9999999999999995e-05
Step 4954, training loss: 4.236271381378174, total_norm: 4.338090896606445, lr: 5.9999999999999995e-05
Step 4955, training loss: 4.1637420654296875, total_norm: 4.159884929656982, lr: 5.9999999999999995e-05
Step 4956, training loss: 4.71887731552124, total_norm: 5.163410663604736, lr: 5.9999999999999995e-05
Step 4957, training loss: 3.866858959197998, total_norm: 4.192260265350342, lr: 5.9999999999999995e-05
Step 4958, training loss: 4.08609676361084, total_norm: 4.382801055908203, lr: 5.9999999999999995e-05
Step 4959, training loss: 3.8969922065734863, total_norm: 4.418752193450928, lr: 5.9999999999999995e-05
Step 4960, training loss: 3.9609344005584717, total_norm: 4.765503883361816, lr: 5.9999999999999995e-05
Step 4961, training loss: 3.8986105918884277, total_norm: 5.051872253417969, lr: 5.9999999999999995e-05
Step 4962, training loss: 4.17696475982666, total_norm: 4.641361713409424, lr: 5.9999999999999995e-05
Step 4963, training loss: 4.19979190826416, total_norm: 5.173700332641602, lr: 5.9999999999999995e-05
Step 4964, training loss: 3.8668277263641357, total_norm: 5.258171081542969, lr: 5.9999999999999995e-05
Step 4965, training loss: 3.8691964149475098, total_norm: 4.411148548126221, lr: 5.9999999999999995e-05
Step 4966, training loss: 3.97275710105896, total_norm: 6.142814636230469, lr: 5.9999999999999995e-05
Step 4967, training loss: 3.580291986465454, total_norm: 4.64499568939209, lr: 5.9999999999999995e-05
Step 4968, training loss: 3.6044318675994873, total_norm: 5.40155553817749, lr: 5.9999999999999995e-05
Step 4969, training loss: 3.6710026264190674, total_norm: 4.614070415496826, lr: 5.9999999999999995e-05
Step 4970, training loss: 3.8670921325683594, total_norm: 5.84401273727417, lr: 5.9999999999999995e-05
Step 4971, training loss: 4.096661567687988, total_norm: 5.2465620040893555, lr: 5.9999999999999995e-05
Step 4972, training loss: 3.9076783657073975, total_norm: 5.2984185218811035, lr: 5.9999999999999995e-05
Step 4973, training loss: 4.081112384796143, total_norm: 4.890185832977295, lr: 5.9999999999999995e-05
Step 4974, training loss: 4.142714500427246, total_norm: 4.512198448181152, lr: 5.9999999999999995e-05
Step 4975, training loss: 4.224604606628418, total_norm: 5.026692867279053, lr: 5.9999999999999995e-05
Step 4976, training loss: 4.098574161529541, total_norm: 5.87593412399292, lr: 5.9999999999999995e-05
Step 4977, training loss: 4.339849948883057, total_norm: 4.7225518226623535, lr: 5.9999999999999995e-05
Step 4978, training loss: 3.501206636428833, total_norm: 4.40022087097168, lr: 5.9999999999999995e-05
Step 4979, training loss: 3.7672336101531982, total_norm: 4.580237865447998, lr: 5.9999999999999995e-05
Step 4980, training loss: 4.304838180541992, total_norm: 5.071979999542236, lr: 5.9999999999999995e-05
Step 4981, training loss: 3.7817726135253906, total_norm: 4.580097675323486, lr: 5.9999999999999995e-05
Step 4982, training loss: 4.0739569664001465, total_norm: 4.821488380432129, lr: 5.9999999999999995e-05
Step 4983, training loss: 3.103595495223999, total_norm: 3.860459566116333, lr: 5.9999999999999995e-05
Step 4984, training loss: 4.000560283660889, total_norm: 4.254482746124268, lr: 5.9999999999999995e-05
Step 4985, training loss: 4.2243757247924805, total_norm: 4.75635290145874, lr: 5.9999999999999995e-05
Step 4986, training loss: 3.7942862510681152, total_norm: 4.063450813293457, lr: 5.9999999999999995e-05
Step 4987, training loss: 3.362654685974121, total_norm: 4.542139053344727, lr: 5.9999999999999995e-05
Step 4988, training loss: 3.7889938354492188, total_norm: 4.744476318359375, lr: 5.9999999999999995e-05
Step 4989, training loss: 3.9017443656921387, total_norm: 4.903064727783203, lr: 5.9999999999999995e-05
Step 4990, training loss: 3.9405405521392822, total_norm: 4.438429832458496, lr: 5.9999999999999995e-05
Step 4991, training loss: 3.75541090965271, total_norm: 4.71273136138916, lr: 5.9999999999999995e-05
Step 4992, training loss: 3.9721431732177734, total_norm: 4.457755088806152, lr: 5.9999999999999995e-05
Step 4993, training loss: 3.509775400161743, total_norm: 4.184286594390869, lr: 5.9999999999999995e-05
Step 4994, training loss: 3.166668653488159, total_norm: 3.7843267917633057, lr: 5.9999999999999995e-05
Step 4995, training loss: 4.046398162841797, total_norm: 4.874778747558594, lr: 5.9999999999999995e-05
Step 4996, training loss: 4.07465124130249, total_norm: 4.94113302230835, lr: 5.9999999999999995e-05
Step 4997, training loss: 3.5923123359680176, total_norm: 4.111359119415283, lr: 5.9999999999999995e-05
Step 4998, training loss: 3.9631848335266113, total_norm: 5.157415866851807, lr: 5.9999999999999995e-05
Step 4999, training loss: 2.8779401779174805, total_norm: 4.86130428314209, lr: 5.9999999999999995e-05
Testing loss: 5.865208148956299
